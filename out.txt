DEBUG:ostruct:_main: Starting ostruct CLI with log file: /Users/yaniv/.ostruct/logs/ostruct.log
DEBUG:ostruct:_main: Initialized security manager with base dir: /Users/yaniv/Documents/code/openai-structured
DEBUG:ostruct:_main: [_main] Creating template context from arguments
DEBUG:openai_structured.cli.template_validation:validate_template_placeholders: === validate_template_placeholders called ===
DEBUG:openai_structured.cli.template_validation:validate_template_placeholders: Args: task_template=Analyze the following code files for potential issues. For each file, provide a detailed review focusing on security, code quality, performance, and documentation.

For each file, identify:
1. Any security vulnerabilities or risks
2. Code quality issues and potential improvements
3. Performance concerns and optimization opportunities
4. Documentation and maintainability issues

Provide your review in a structured format with:
- A list of specific issues found in each file
- The severity and category of each issue
- Line numbers where applicable
- Clear recommendations for fixing each issue
- A summary assessment for each file
- An overall summary of the entire codebase

Files to review:

{% for file in code %}
## File: {{ file.path }}

{{ file.content }}

{% endfor %}

, template_context={'code': 'FileInfoList'}
DEBUG:openai_structured.cli.template_validation:validate_template_placeholders: Available variables: {'code'}
DEBUG:openai_structured.cli.template_validation:validate_template_placeholders: Found loop variables: {'file'}
DEBUG:openai_structured.cli.template_validation:validate_template_placeholders: Found undeclared variables: {'code'}
DEBUG:openai_structured.cli.template_validation:validate_template_placeholders: Before create_validation_context - available_vars type: <class 'set'>, value: {'code'}
DEBUG:openai_structured.cli.template_validation:validate_template_placeholders: Before create_validation_context - template_context type: <class 'dict'>, value: {'code': FileInfoList(['src/openai_structured/client.py', 'src/openai_structured/__init__.py', 'src/openai_structured/buffer.py', 'src/openai_structured/model_version.py', 'src/openai_structured/errors.py', 'src/openai_structured/cli/file_list.py', 'src/openai_structured/cli/template_schema.py', 'src/openai_structured/cli/security_types.py', 'src/openai_structured/cli/template_io.py', 'src/openai_structured/cli/template_validation.py', 'src/openai_structured/cli/template_utils.py', 'src/openai_structured/cli/template_extensions.py', 'src/openai_structured/cli/security.py', 'src/openai_structured/cli/template_rendering.py', 'src/openai_structured/cli/__init__.py', 'src/openai_structured/cli/template_filters.py', 'src/openai_structured/cli/cli.py', 'src/openai_structured/cli/utils.py', 'src/openai_structured/cli/file_utils.py', 'src/openai_structured/cli/errors.py', 'src/openai_structured/cli/progress.py', 'src/openai_structured/cli/template_env.py', 'src/openai_structured/cli/path_utils.py', 'src/openai_structured/cli/file_info.py'])}
DEBUG:openai_structured.cli.template_validation:validate_template_placeholders: Creating validation context with template_context: {'code': FileInfoList(['src/openai_structured/client.py', 'src/openai_structured/__init__.py', 'src/openai_structured/buffer.py', 'src/openai_structured/model_version.py', 'src/openai_structured/errors.py', 'src/openai_structured/cli/file_list.py', 'src/openai_structured/cli/template_schema.py', 'src/openai_structured/cli/security_types.py', 'src/openai_structured/cli/template_io.py', 'src/openai_structured/cli/template_validation.py', 'src/openai_structured/cli/template_utils.py', 'src/openai_structured/cli/template_extensions.py', 'src/openai_structured/cli/security.py', 'src/openai_structured/cli/template_rendering.py', 'src/openai_structured/cli/__init__.py', 'src/openai_structured/cli/template_filters.py', 'src/openai_structured/cli/cli.py', 'src/openai_structured/cli/utils.py', 'src/openai_structured/cli/file_utils.py', 'src/openai_structured/cli/errors.py', 'src/openai_structured/cli/progress.py', 'src/openai_structured/cli/template_env.py', 'src/openai_structured/cli/path_utils.py', 'src/openai_structured/cli/file_info.py'])}
DEBUG:openai_structured.cli.template_schema:__init__: Created ValidationProxy for code with valid_attrs=None, nested_attrs=None, allow_nested=True
DEBUG:ostruct:_main: Total tokens in prompt: 52,405
INFO:ostruct:_main: *** DRY RUN MODE - No API call will be made ***

INFO:ostruct:_main: System Prompt:
You are an expert code reviewer with deep knowledge of software development best practices, security, and performance optimization. Your task is to review code files and provide structured feedback.

Focus on these key areas:
1. Security Vulnerabilities
   - Injection flaws
   - Authentication issues
   - Data exposure
   - Security misconfigurations
   - Known vulnerable dependencies

2. Code Quality
   - Clean code principles
   - SOLID principles
   - Design patterns
   - Code organization
   - Naming conventions
   - Error handling

3. Performance
   - Algorithmic efficiency
   - Resource usage
   - Memory management
   - Concurrency issues
   - Database query optimization

4. Documentation & Maintainability
   - Code comments
   - Function/class documentation
   - API documentation
   - Architecture documentation
   - Test coverage

For each issue found:
- Assign appropriate severity (critical, major, minor, suggestion)
- Categorize the issue (security, performance, style, documentation)
- Provide clear description of the problem
- Include specific line numbers where applicable
- Offer actionable recommendations for improvement

Be thorough but practical in your recommendations. Focus on significant issues that impact code quality, security, or performance. Avoid nitpicking minor stylistic preferences unless they significantly impact readability.

INFO:ostruct:_main: User Prompt:
Analyze the following code files for potential issues. For each file, provide a detailed review focusing on security, code quality, performance, and documentation.

For each file, identify:
1. Any security vulnerabilities or risks
2. Code quality issues and potential improvements
3. Performance concerns and optimization opportunities
4. Documentation and maintainability issues

Provide your review in a structured format with:
- A list of specific issues found in each file
- The severity and category of each issue
- Line numbers where applicable
- Clear recommendations for fixing each issue
- A summary assessment for each file
- An overall summary of the entire codebase

Files to review:



"""Client for making structured OpenAI API calls.

This module provides functions for making structured calls to OpenAI's API,
ensuring responses conform to specified Pydantic models.

Constants:
    DEFAULT_TEMPERATURE (float): Default temperature for sampling (0.0)
    DEFAULT_TIMEOUT (float): Default timeout in seconds for API calls (60.0)

For production use, it's recommended to implement retry and rate limiting logic:

Example with retries:
    from tenacity import retry, stop_after_attempt, wait_exponential

    @retry(stop=stop_after_attempt(3), wait=wait_exponential())
    def my_resilient_call():
        result = openai_structured_call(
            client=client,
            model="gpt-4o-2024-08-06",
            output_schema=MySchema,
            user_prompt="...",
            system_prompt="..."
        )
        return result

Example with rate limiting:
    from asyncio_throttle import Throttler

    async with Throttler(rate_limit=100, period=60):
        result = await async_openai_structured_call(...)

See documentation for more examples and best practices.
"""

# Define public API
__all__ = [
    # Main functions
    "openai_structured_call",
    "async_openai_structured_call",
    "openai_structured_stream",
    "async_openai_structured_stream",
    # Configuration
    "StreamConfig",
    "DEFAULT_TEMPERATURE",
    "DEFAULT_TIMEOUT",
    # Type hints
    "LogCallback",
    # Exceptions
    "StreamInterruptedError",
    "StreamParseError",
    "StreamBufferError",
    "BufferOverflowError",
    "ModelNotSupportedError",
    "InvalidResponseFormatError",
    "EmptyResponseError",
    "JSONParseError",
    "ValidationError",
]

# Standard library imports
import asyncio
import json
import logging
import re
from functools import wraps
from typing import (
    Any,
    AsyncGenerator,
    Callable,
    Generator,
    List,
    NoReturn,
    Optional,
    Set,
    Type,
    TypeVar,
    Union,
    cast,
)

# Third-party imports
import aiohttp
from openai import (
    APIConnectionError,
    APITimeoutError,
    AsyncOpenAI,
    OpenAI,
    OpenAIError,
    RateLimitError,
)
from openai.types.chat import ChatCompletionMessageParam
from openai.types.chat.completion_create_params import ResponseFormat
from pydantic import BaseModel, ValidationError
from typing_extensions import ParamSpec

# Local imports
from .buffer import StreamBuffer, StreamConfig
from .errors import (
    BufferOverflowError,
    EmptyResponseError,
    InvalidResponseFormatError,
    JSONParseError,
    ModelNotSupportedError,
    OpenAIClientError,
    StreamBufferError,
    StreamInterruptedError,
)
from .model_version import ModelVersion

# Type variables and aliases
ClientT = TypeVar("ClientT", bound=Union[OpenAI, AsyncOpenAI])
LogCallback = Callable[[int, str, dict[str, Any]], None]
P = ParamSpec("P")
R = TypeVar("R")

# Constants
DEFAULT_TEMPERATURE = 0.2
DEFAULT_TIMEOUT = 60.0  # Default timeout in seconds

# Format: "{base_model}-{YYYY}-{MM}-{DD}"
# Examples:
#   - "gpt-4o-2024-08-06"      -> ("gpt-4o", "2024-08-06")
#   - "gpt-4-turbo-2024-08-06" -> ("gpt-4-turbo", "2024-08-06")
#
# Pattern components:
# ^           Start of string
# ([\w-]+?)   Group 1: Base model name
#   [\w-]     Allow word chars (a-z, A-Z, 0-9, _) and hyphens
#   +?        One or more chars (non-greedy)
# -           Literal hyphen separator
# (...)       Group 2: Date in YYYY-MM-DD format
#   \d{4}     Exactly 4 digits for year
#   -         Literal hyphen
#   \d{2}     Exactly 2 digits for month
#   -         Literal hyphen
#   \d{2}     Exactly 2 digits for day
# $           End of string
MODEL_VERSION_PATTERN = re.compile(r"^([\w-]+?)-(\d{4}-\d{2}-\d{2})$")

# Model token limits (based on OpenAI specifications as of 2024):
# Models can be specified using either aliases or dated versions:
# Aliases:
# - gpt-4o: 128K context window, 16K max output tokens (minimum version: 2024-08-06)
# - gpt-4o-mini: 128K context window, 16K max output tokens (minimum version: 2024-07-18)
# - o1: 200K context window, 100K max output tokens (minimum version: 2024-12-17)
#
# When using aliases (e.g., "gpt-4o"), OpenAI will automatically use the latest
# compatible version. We validate that the model meets our minimum version
# requirements, but the actual version resolution is handled by OpenAI.
#
# Dated versions (recommended for production):
# - gpt-4o-2024-08-06: 128K context window, 16K max output tokens
# - gpt-4o-mini-2024-07-18: 128K context window, 16K max output tokens
# - o1-2024-12-17: 200K context window, 100K max output tokens
#
# Note: These limits may change as OpenAI updates their models

# Model version mapping - maps aliases to minimum supported versions
OPENAI_API_SUPPORTED_MODELS = {
    "gpt-4o": ModelVersion(2024, 8, 6),  # Minimum supported version
    "gpt-4o-mini": ModelVersion(2024, 7, 18),  # Minimum supported version
    "o1": ModelVersion(2024, 12, 17),  # Minimum supported version
}


def validate_parameters(func: Callable[P, R]) -> Callable[P, R]:
    """Decorator to validate common parameters."""

    @wraps(func)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        # Get temperature with proper type casting
        temp_val = kwargs.get("temperature", DEFAULT_TEMPERATURE)
        temperature = float(
            temp_val
            if isinstance(temp_val, (int, float, str))
            else DEFAULT_TEMPERATURE
        )
        if not 0 <= temperature <= 2:
            raise OpenAIClientError("Temperature must be between 0 and 2")

        # Get top_p with proper type casting
        top_p_val = kwargs.get("top_p", 1.0)
        top_p = float(
            top_p_val if isinstance(top_p_val, (int, float, str)) else 1.0
        )
        if not 0 <= top_p <= 1:
            raise OpenAIClientError("Top-p must be between 0 and 1")

        # Get frequency and presence penalties with proper type casting
        for param in ["frequency_penalty", "presence_penalty"]:
            val = kwargs.get(param, 0.0)
            value = float(val if isinstance(val, (int, float, str)) else 0.0)
            if not -2 <= value <= 2:
                raise OpenAIClientError(f"{param} must be between -2 and 2")

        return func(*args, **kwargs)

    return wrapper


class StreamParseError(StreamInterruptedError):
    """Raised when stream content cannot be parsed after multiple attempts."""

    def __init__(self, message: str, attempts: int, last_error: Exception):
        super().__init__(
            f"{message} after {attempts} attempts. Last error: {last_error}"
        )
        self.attempts = attempts
        self.last_error = last_error


def supports_structured_output(model_name: str) -> bool:
    """Check if a model supports structured output.

    This function validates whether a given model name supports structured output,
    handling both aliases and dated versions. For dated versions, it ensures they meet
    minimum version requirements.

    The function supports two types of model names:
    1. Aliases (e.g., "gpt-4o"): These are automatically resolved to the latest
       compatible version by OpenAI. We validate that the alias is supported.

    2. Dated versions (e.g., "gpt-4o-2024-08-06"): These specify an exact version.
       We validate that:
       a) The base model (e.g., "gpt-4o") is supported
       b) The date meets or exceeds our minimum version requirement

    If using a dated version newer than our minimum (e.g., "gpt-4o-2024-09-01"),
    it will be accepted as long as the base model is supported and the date is
    greater than or equal to our minimum version.

    Args:
        model_name: The model name to validate. Can be either:
                   - an alias (e.g., "gpt-4o")
                   - dated version (e.g., "gpt-4o-2024-08-06")
                   - newer version (e.g., "gpt-4o-2024-09-01")

    Returns:
        bool: True if the model supports structured output

    Examples:
        >>> supports_structured_output("gpt-4o")  # Alias
        True
        >>> supports_structured_output("gpt-4o-2024-08-06")  # Minimum version
        True
        >>> supports_structured_output("gpt-4o-2024-09-01")  # Newer version
        True
        >>> supports_structured_output("gpt-3.5-turbo")  # Unsupported model
        False
        >>> supports_structured_output("gpt-4o-2024-07-01")  # Too old
        False
    """
    # Check for exact matches (aliases)
    if model_name in OPENAI_API_SUPPORTED_MODELS:
        return True

    # Try to parse as a dated version
    match = MODEL_VERSION_PATTERN.match(model_name)
    if not match:
        return False

    base_model, version_str = match.groups()

    # Check if the base model has a minimum version requirement
    if base_model in OPENAI_API_SUPPORTED_MODELS:
        try:
            version = ModelVersion.from_string(version_str)
            min_version = OPENAI_API_SUPPORTED_MODELS[base_model]
            return version >= min_version
        except ValueError:
            return False

    return False


def _validate_client_type(client: Any, expected_type: Type[ClientT]) -> None:
    """Validate that the client is of the expected type."""
    if not isinstance(client, expected_type):
        raise TypeError(
            f"Expected client of type {expected_type.__name__}, got {type(client).__name__}"
        )


def _validate_request(
    model: str,
    client: Union[OpenAI, AsyncOpenAI],
    expected_type: Type[ClientT],
) -> None:
    """Validate the request parameters."""
    _validate_client_type(client, expected_type)
    if not supports_structured_output(model):
        raise ModelNotSupportedError(
            f"Model {model} does not support structured output"
        )


def _create_chat_messages(
    system_prompt: str,
    user_prompt: str,
) -> List[ChatCompletionMessageParam]:
    """Create chat messages for the OpenAI API."""
    return [
        cast(
            ChatCompletionMessageParam,
            {"role": "system", "content": system_prompt},
        ),
        cast(
            ChatCompletionMessageParam,
            {"role": "user", "content": user_prompt},
        ),
    ]


def _create_request_params(
    model: str,
    messages: List[ChatCompletionMessageParam],
    schema: dict[str, Any],
    temperature: float,
    max_tokens: Optional[int],
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
    stream: bool,
    timeout: Optional[float] = None,
) -> dict[str, Any]:
    """Create request parameters for the API call."""
    params: dict[str, Any] = {
        "model": model,
        "messages": messages,
        "response_format": cast(
            ResponseFormat,
            {
                "type": "json_schema",
                "json_schema": {
                    "name": "structured_output",
                    "schema": schema,
                },
            },
        ),
        "temperature": temperature,
        "max_tokens": max_tokens,
        "top_p": top_p,
        "frequency_penalty": frequency_penalty,
        "presence_penalty": presence_penalty,
        "stream": stream,
    }

    if timeout is not None:
        params["timeout"] = timeout

    return params


def _get_schema(model_class: Type[BaseModel]) -> dict[str, Any]:
    """Get JSON schema for a model class."""
    return model_class.model_json_schema()


def _parse_json_response(
    content: Optional[str],
    output_schema: Type[BaseModel],
    response_id: Optional[str] = None,
) -> BaseModel:
    """Parse and validate JSON response."""
    if not content:
        raise EmptyResponseError("OpenAI API returned empty response")

    try:
        return output_schema.model_validate_json(content)
    except ValidationError as e:
        raise InvalidResponseFormatError(
            f"Response validation failed: {e}\n"
            f"Received content (first 200 chars): {content[:200]}",
            response_id=response_id,
        ) from e
    except json.JSONDecodeError as e:
        error_pos = e.pos
        context = content[
            max(0, error_pos - 50) : min(len(content), error_pos + 50)
        ]
        if error_pos > 50:
            context = "..." + context
        if error_pos + 50 < len(content):
            context = context + "..."

        raise JSONParseError(
            f"Invalid JSON at position {error_pos}: {e.msg}\n"
            f"Context: {context}\n"
            f"Full response (first 200 chars): {content[:200]}",
            response_id=response_id,
        ) from e


def _redact_string(text: str) -> str:
    """Redact sensitive patterns from a string."""
    if not text:
        return text
    # Redact OpenAI API keys (sk-...)
    return re.sub(r"sk-[a-zA-Z0-9]{32,}", "[REDACTED-API-KEY]", text)


def _redact_sensitive_data(data: dict[str, Any]) -> dict[str, Any]:
    """Redact sensitive data from logging output."""
    if not data:
        return data

    redacted = data.copy()
    sensitive_keys = {
        "api_key",
        "authorization",
        "key",
        "secret",
        "password",
        "token",
        "access_token",
        "refresh_token",
    }

    for key in redacted:
        value = redacted[key]
        # Redact any key containing sensitive terms
        if any(sensitive in key.lower() for sensitive in sensitive_keys):
            redacted[key] = "[REDACTED]"
        # Redact Authorization headers
        elif key.lower() == "headers" and isinstance(value, dict):
            headers = value.copy()
            for header in headers:
                if header.lower() in {"authorization", "x-api-key"}:
                    headers[header] = "[REDACTED]"
            redacted[key] = headers
        # Redact sensitive patterns in strings
        elif isinstance(value, str):
            redacted[key] = _redact_string(value)
        # Handle error messages and nested structures
        elif key in {"error", "error_message"} and isinstance(value, str):
            redacted[key] = _redact_string(value)

    return redacted


def _log(
    on_log: Optional[LogCallback],
    level: int,
    message: str,
    data: Optional[dict[str, Any]] = None,
) -> None:
    """Utility function for consistent logging with sensitive data redaction."""
    if on_log:
        safe_data = _redact_sensitive_data(data or {})
        on_log(level, message, safe_data)


def _handle_api_error(
    error: Exception, on_log: Optional[LogCallback] = None
) -> NoReturn:
    """Handle OpenAI API errors with enhanced logging."""
    if on_log:
        # First log basic error info
        _log(
            on_log,
            logging.ERROR,
            "OpenAI API error",
            {
                "error": _redact_string(str(error)),
                "error_type": type(error).__name__,
            },
        )

        # Then log detailed error data if it's an OpenAI error
        if isinstance(error, OpenAIError):
            error_data: dict[str, Any] = {
                "error_type": error.__class__.__name__,
                "error_message": _redact_string(str(error)),
                "status_code": getattr(error, "status_code", None),
                "request_id": getattr(error, "request_id", None),
                "should_retry": isinstance(
                    error,
                    (APIConnectionError, APITimeoutError, RateLimitError),
                ),
                "retry_after": getattr(error, "retry_after", None),
            }
            _log(on_log, logging.ERROR, "OpenAI API error details", error_data)

    # Re-raise OpenAI errors and our own errors as is, wrap others in OpenAIClientError
    if isinstance(
        error, (OpenAIError, InvalidResponseFormatError, JSONParseError)
    ):
        raise error
    raise OpenAIClientError(
        f"Unexpected error during API call: {error}"
    ) from error


def _log_request_start(
    on_log: Optional[LogCallback],
    model: str,
    temperature: float,
    max_tokens: Optional[int],
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
    streaming: bool,
    output_schema: Type[BaseModel],
) -> None:
    """Log the start of an API request."""
    _log(
        on_log,
        logging.DEBUG,
        "Starting OpenAI request",
        {
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "frequency_penalty": frequency_penalty,
            "presence_penalty": presence_penalty,
            "streaming": streaming,
            "output_schema": output_schema.__name__,
        },
    )


def _prepare_request(
    model: str,
    client: Union[OpenAI, AsyncOpenAI],
    expected_type: Type[ClientT],
    output_schema: Type[BaseModel],
    system_prompt: str,
    user_prompt: str,
    temperature: float,
    max_tokens: Optional[int],
    top_p: float,
    frequency_penalty: float,
    presence_penalty: float,
    stream: bool,
    timeout: Optional[float] = None,
) -> dict[str, Any]:
    """Prepare common request parameters and validate inputs."""
    _validate_request(model, client, expected_type)
    messages = _create_chat_messages(system_prompt, user_prompt)
    schema = _get_schema(output_schema)

    return _create_request_params(
        model=model,
        messages=messages,
        schema=schema,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        frequency_penalty=frequency_penalty,
        presence_penalty=presence_penalty,
        stream=stream,
        timeout=timeout,
    )


def _handle_stream_error(
    e: Exception, on_log: Optional[LogCallback]
) -> NoReturn:
    """Handle stream-specific errors consistently."""
    # Handle known error types first
    if isinstance(e, StreamBufferError):
        raise

    if isinstance(e, StreamParseError):
        _log(
            on_log,
            logging.ERROR,
            "Stream parsing failed",
            {"attempts": e.attempts, "error": str(e.last_error)},
        )
        raise

    if isinstance(
        e,
        (
            aiohttp.ClientError,
            aiohttp.ServerTimeoutError,
            asyncio.TimeoutError,
            ConnectionError,
        ),
    ):
        _log(
            on_log,
            logging.ERROR,
            "Stream interrupted",
            {"error": str(e), "error_type": type(e).__name__},
        )
        raise StreamInterruptedError(
            f"Stream was interrupted: {e}. "
            "Check your network connection and API status."
        ) from e

    # Handle OpenAI errors directly
    if isinstance(e, OpenAIError):
        raise

    # Convert unknown exceptions to StreamInterruptedError
    raise StreamInterruptedError(f"Stream was interrupted: {e}") from e


def _process_stream_chunk(
    chunk: Any,
    buffer: StreamBuffer,
    output_schema: Type[BaseModel],
    on_log: Optional[LogCallback] = None,
) -> Optional[BaseModel]:
    """Process a single stream chunk with error handling."""
    if not chunk.choices:
        return None

    delta = chunk.choices[0].delta
    if not delta.content:
        return None

    try:
        buffer.write(delta.content)

        # Only log significant size changes
        if buffer.should_log_size():
            _log(
                on_log,
                logging.DEBUG,
                "Buffer size increased significantly",
                {"size_bytes": buffer.total_bytes},
            )

        current_content = buffer.getvalue()
        try:
            model_instance = output_schema.model_validate_json(current_content)
            # Only log successful parse
            _log(on_log, logging.DEBUG, "Successfully parsed complete object")
            buffer.reset()
            return model_instance
        except (ValidationError, json.JSONDecodeError):
            # Only attempt cleanup if buffer is getting large
            if buffer.total_bytes > buffer.config.cleanup_threshold:
                buffer.cleanup()
    except (BufferOverflowError, StreamParseError) as e:
        # Only log critical errors
        _log(
            on_log,
            logging.ERROR,
            "Critical stream buffer error",
            {"error": str(e), "bytes": buffer.total_bytes},
        )
        raise

    return None


@validate_parameters
def openai_structured_call(
    client: OpenAI,
    model: str,
    output_schema: Type[BaseModel],
    user_prompt: str,
    system_prompt: str,
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: Optional[int] = None,
    top_p: float = 1.0,
    frequency_penalty: float = 0.0,
    presence_penalty: float = 0.0,
    on_log: Optional[LogCallback] = None,
    timeout: Optional[float] = DEFAULT_TIMEOUT,
) -> BaseModel:
    """Make a structured call to the OpenAI API.

    Args:
        client: OpenAI client instance
        model: Model name (e.g., "gpt-4o-2024-08-06")
        output_schema: Pydantic model class for response validation
        user_prompt: User's request to process
        system_prompt: System instructions for the model
        temperature: Sampling temperature (0-2)
        max_tokens: Maximum tokens in response
        top_p: Top-p sampling parameter (0-1)
        frequency_penalty: Frequency penalty (-2 to 2)
        presence_penalty: Presence penalty (-2 to 2)
        on_log: Optional logging callback
        timeout: Request timeout in seconds (default: 60s)

    Returns:
        Validated instance of output_schema

    Raises:
        TimeoutError: If the request exceeds the timeout
        OpenAIClientError: For client-side errors
        APIConnectionError: For network-related errors
        InvalidResponseFormatError: If response fails validation
    """
    try:
        _log_request_start(
            on_log,
            model,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
            False,
            output_schema,
        )

        params = _prepare_request(
            model,
            client,
            OpenAI,
            output_schema,
            system_prompt,
            user_prompt,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
            False,
            timeout,
        )

        response = client.chat.completions.create(**params)

        if not response.choices or not response.choices[0].message.content:
            raise EmptyResponseError("OpenAI API returned an empty response.")

        _log(
            on_log,
            logging.DEBUG,
            "Request completed",
            {"response_id": response.id},
        )

        return _parse_json_response(
            response.choices[0].message.content,
            output_schema,
            response_id=response.id,
        )

    except Exception as e:
        _handle_api_error(e, on_log)


@validate_parameters
async def async_openai_structured_call(
    client: AsyncOpenAI,
    model: str,
    output_schema: Type[BaseModel],
    user_prompt: str,
    system_prompt: str,
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: Optional[int] = None,
    top_p: float = 1.0,
    frequency_penalty: float = 0.0,
    presence_penalty: float = 0.0,
    on_log: Optional[LogCallback] = None,
    timeout: Optional[float] = DEFAULT_TIMEOUT,
) -> BaseModel:
    """Make an async structured call to the OpenAI API.

    This is the async version of openai_structured_call. It requires an AsyncOpenAI client
    and should be used in async contexts.

    Args:
        client: AsyncOpenAI client instance
        model: Model name (e.g., "gpt-4o-2024-08-06")
        output_schema: Pydantic model class for response validation
        user_prompt: User's request to process
        system_prompt: System instructions for the model
        temperature: Sampling temperature (0-2)
        max_tokens: Maximum tokens in response
        top_p: Top-p sampling parameter (0-1)
        frequency_penalty: Frequency penalty (-2 to 2)
        presence_penalty: Presence penalty (-2 to 2)
        on_log: Optional logging callback
        timeout: Request timeout in seconds (default: 60s)

    Returns:
        Validated instance of output_schema

    Raises:
        asyncio.TimeoutError: If the request exceeds the timeout
        OpenAIClientError: For client-side errors
        APIConnectionError: For network-related errors
        InvalidResponseFormatError: If response fails validation
    """
    try:
        _log_request_start(
            on_log,
            model,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
            False,
            output_schema,
        )

        params = _prepare_request(
            model,
            client,
            AsyncOpenAI,
            output_schema,
            system_prompt,
            user_prompt,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
            False,
            timeout,
        )

        response = await client.chat.completions.create(**params)

        if not response.choices or not response.choices[0].message.content:
            raise EmptyResponseError("OpenAI API returned an empty response.")

        _log(
            on_log,
            logging.DEBUG,
            "Request completed",
            {"response_id": response.id},
        )

        return _parse_json_response(
            response.choices[0].message.content,
            output_schema,
            response_id=response.id,
        )

    except Exception as e:
        _handle_api_error(e, on_log)


@validate_parameters
async def async_openai_structured_stream(
    client: AsyncOpenAI,
    model: str,
    output_schema: Type[BaseModel],
    system_prompt: str,
    user_prompt: str,
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: Optional[int] = None,
    top_p: float = 1.0,
    frequency_penalty: float = 0.0,
    presence_penalty: float = 0.0,
    on_log: Optional[LogCallback] = None,
    stream_config: Optional[StreamConfig] = None,
    timeout: Optional[float] = None,
) -> AsyncGenerator[BaseModel, None]:
    """Stream structured output from OpenAI's API asynchronously.

    This is the asynchronous version of openai_structured_stream. It provides
    the same functionality but can be used in async contexts for better
    performance and resource utilization.

    Args:
        client: AsyncOpenAI client instance
        model: Model name (e.g., "gpt-4o-2024-08-06")
        output_schema: Pydantic model class defining the expected output structure
        system_prompt: System message to guide the model's behavior
        user_prompt: User message containing the actual request
        temperature: Controls randomness (0.0-2.0, default: 0.7)
        max_tokens: Maximum tokens to generate (optional)
        top_p: Nucleus sampling parameter (optional)
        frequency_penalty: Frequency penalty parameter (optional)
        presence_penalty: Presence penalty parameter (optional)
        on_log: Optional callback for logging events
        stream_config: Optional configuration for stream buffer management
        timeout: Optional timeout in seconds for the API request

    Returns:
        AsyncGenerator yielding structured objects matching output_schema

    Raises:
        ModelNotSupportedError: If model doesn't support structured output
        StreamInterruptedError: If stream is interrupted (e.g., network issues)
        StreamParseError: If response can't be parsed into schema
        StreamBufferError: If buffer management fails
        BufferOverflowError: If buffer size exceeds limit
        APIError: For OpenAI API errors
        APITimeoutError: If request exceeds timeout
        ValidationError: If response doesn't match schema

    Examples:
        >>> from pydantic import BaseModel
        >>> from openai import AsyncOpenAI
        >>> import asyncio
        >>>
        >>> class StoryChapter(BaseModel):
        ...     title: str
        ...     content: str
        ...
        >>> async def main():
        ...     client = AsyncOpenAI()
        ...     prompt = "Write a story about a magical forest"
        ...
        ...     # Basic streaming
        ...     async for chapter in async_openai_structured_stream(
        ...         client=client,
        ...         model="gpt-4o-2024-08-06",
        ...         output_schema=StoryChapter,
        ...         system_prompt="You are a creative writer",
        ...         user_prompt=prompt
        ...     ):
        ...         print(f"Chapter: {chapter.title}")
        ...         print(chapter.content)
        ...
        ...     # With custom buffer config and timeout
        ...     config = StreamConfig(
        ...         max_buffer_size=2 * 1024 * 1024,  # 2MB
        ...         cleanup_threshold=1024 * 1024,     # 1MB
        ...         chunk_size=4096                    # 4KB
        ...     )
        ...     async for chapter in async_openai_structured_stream(
        ...         client=client,
        ...         model="gpt-4o-2024-08-06",
        ...         output_schema=StoryChapter,
        ...         system_prompt="You are a creative writer",
        ...         user_prompt=prompt,
        ...         stream_config=config,
        ...         timeout=30.0
        ...     ):
        ...         print(f"Chapter: {chapter.title}")
        ...         print(chapter.content)
        ...
        >>> asyncio.run(main())
    """
    buffer = None
    stream = None

    try:
        _log_request_start(
            on_log,
            model,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
            True,
            output_schema,
        )

        params = _prepare_request(
            model,
            client,
            AsyncOpenAI,
            output_schema,
            system_prompt,
            user_prompt,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
            True,
            timeout,
        )

        buffer = StreamBuffer(config=stream_config or StreamConfig())

        _log(on_log, logging.DEBUG, "Creating streaming completion")

        stream = await client.chat.completions.create(**params)

        _log(on_log, logging.DEBUG, "Stream created")

        async for chunk in stream:
            result = _process_stream_chunk(
                chunk, buffer, output_schema, on_log
            )
            if result is not None:
                yield result

    except Exception as e:
        _handle_stream_error(e, on_log)

    finally:
        if buffer:
            buffer.close()
        if stream and hasattr(stream, "close"):
            try:
                await stream.close()
            except Exception as e:
                _log(
                    on_log,
                    logging.WARNING,
                    "Error closing stream",
                    {"error": str(e)},
                )


def validate_template(template: str, available_vars: Set[str]) -> None:
    """Validate that a template string only uses available variables.

    Args:
        template: Template string with {var} placeholders
        available_vars: Set of variable names that can be used

    Raises:
        ValueError: If template uses undefined variables or has invalid syntax
    """
    # Find all {var} placeholders
    placeholders = set(re.findall(r"{([^}]+)}", template))

    # Check for undefined variables
    undefined = placeholders - available_vars
    if undefined:
        raise ValueError(
            f"Template uses undefined variables: {', '.join(sorted(undefined))}"
        )

    # Validate template syntax by trying to format with dummy values
    dummy_values = {var: "" for var in available_vars}
    try:
        template.format(**dummy_values)
    except ValueError as e:
        raise ValueError(f"Invalid template syntax: {e}")


@validate_parameters
def openai_structured_stream(
    client: OpenAI,
    model: str,
    output_schema: Type[BaseModel],
    system_prompt: str,
    user_prompt: str,
    temperature: float = DEFAULT_TEMPERATURE,
    max_tokens: Optional[int] = None,
    top_p: float = 1.0,
    frequency_penalty: float = 0.0,
    presence_penalty: float = 0.0,
    on_log: Optional[LogCallback] = None,
    stream_config: Optional[StreamConfig] = None,
    timeout: Optional[float] = None,
) -> Generator[BaseModel, None, None]:
    """Stream structured output from OpenAI's API.

    This function streams responses from OpenAI's API, parsing each chunk into
    a structured output that matches the provided schema. It's useful for
    handling long responses or when you want to process results incrementally.

    Args:
        client: OpenAI client instance
        model: Model name (e.g., "gpt-4o-2024-08-06")
        output_schema: Pydantic model class defining the expected output structure
        system_prompt: System message to guide the model's behavior
        user_prompt: User message containing the actual request
        temperature: Controls randomness (0.0-2.0, default: 0.7)
        max_tokens: Maximum tokens to generate (optional)
        top_p: Nucleus sampling parameter (optional)
        frequency_penalty: Frequency penalty parameter (optional)
        presence_penalty: Presence penalty parameter (optional)
        on_log: Optional callback for logging events
        stream_config: Optional configuration for stream buffer management
        timeout: Optional timeout in seconds for the API request

    Returns:
        Generator yielding structured objects matching output_schema

    Raises:
        ModelNotSupportedError: If model doesn't support structured output
        StreamInterruptedError: If stream is interrupted (e.g., network issues)
        StreamParseError: If response can't be parsed into schema
        StreamBufferError: If buffer management fails
        BufferOverflowError: If buffer size exceeds limit
        APIError: For OpenAI API errors
        APITimeoutError: If request exceeds timeout
        ValidationError: If response doesn't match schema

    Examples:
        >>> from pydantic import BaseModel
        >>> from openai import OpenAI
        >>>
        >>> class StoryChapter(BaseModel):
        ...     title: str
        ...     content: str
        ...
        >>> client = OpenAI()
        >>> prompt = "Write a story about a magical forest"
        >>>
        >>> # Basic streaming
        >>> for chapter in openai_structured_stream(
        ...     client=client,
        ...     model="gpt-4o-2024-08-06",
        ...     output_schema=StoryChapter,
        ...     system_prompt="You are a creative writer",
        ...     user_prompt=prompt
        ... ):
        ...     print(f"Chapter: {chapter.title}")
        ...     print(chapter.content)
        ...
        >>> # With custom buffer config and timeout
        >>> config = StreamConfig(
        ...     max_buffer_size=2 * 1024 * 1024,  # 2MB
        ...     cleanup_threshold=1024 * 1024,     # 1MB
        ...     chunk_size=4096                    # 4KB
        ... )
        >>> for chapter in openai_structured_stream(
        ...     client=client,
        ...     model="gpt-4o-2024-08-06",
        ...     output_schema=StoryChapter,
        ...     system_prompt="You are a creative writer",
        ...     user_prompt=prompt,
        ...     stream_config=config,
        ...     timeout=30.0
        ... ):
        ...     print(f"Chapter: {chapter.title}")
        ...     print(chapter.content)
    """
    buffer = None
    stream = None

    try:
        _log_request_start(
            on_log,
            model,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
            True,
            output_schema,
        )

        params = _prepare_request(
            model,
            client,
            OpenAI,
            output_schema,
            system_prompt,
            user_prompt,
            temperature,
            max_tokens,
            top_p,
            frequency_penalty,
            presence_penalty,
            True,
            timeout,
        )

        buffer = StreamBuffer(config=stream_config or StreamConfig())

        _log(on_log, logging.DEBUG, "Creating streaming completion")

        stream = client.chat.completions.create(**params)

        _log(on_log, logging.DEBUG, "Stream created")

        for chunk in stream:
            result = _process_stream_chunk(
                chunk, buffer, output_schema, on_log
            )
            if result is not None:
                yield result

    except Exception as e:
        _handle_stream_error(e, on_log)

    finally:
        if buffer:
            buffer.close()
        if stream and hasattr(stream, "close"):
            try:
                stream.close()
            except Exception as e:
                _log(
                    on_log,
                    logging.WARNING,
                    "Error closing stream",
                    {"error": str(e)},
                )




# src/openai_structured/__init__.py
"""
openai-structured
=================

A Python library for structured output from OpenAI's API.

:noindex:
"""
from importlib.metadata import version

try:
    __version__ = version("openai-structured")
except Exception:
    __version__ = "unknown"

from .cli import (
    ExitCode,
    ProgressContext,
    create_dynamic_model,
    estimate_tokens_for_chat,
    get_context_window_limit,
    get_default_token_limit,
    main,
    supports_structured_output,
    validate_template_placeholders,
    validate_token_limits,
)
from .client import (
    StreamConfig,
    async_openai_structured_call,
    async_openai_structured_stream,
    openai_structured_call,
    openai_structured_stream,
)
from .errors import (
    APIResponseError,
    BufferOverflowError,
    EmptyResponseError,
    InvalidResponseFormatError,
    JSONParseError,
    ModelNotSupportedError,
    ModelVersionError,
    OpenAIClientError,
    StreamBufferError,
    StreamInterruptedError,
    StreamParseError,
)
from .model_version import ModelVersion, parse_model_version

__all__ = [
    # Main functions
    "openai_structured_call",
    "openai_structured_stream",
    "async_openai_structured_call",
    "async_openai_structured_stream",
    "supports_structured_output",
    # Configuration
    "StreamConfig",
    # Type hints
    "ModelVersion",
    # Utility functions
    "parse_model_version",
    # CLI
    "ExitCode",
    "main",
    "create_dynamic_model",
    "validate_template_placeholders",
    "estimate_tokens_for_chat",
    "get_context_window_limit",
    "get_default_token_limit",
    "validate_token_limits",
    "ProgressContext",
    # Exceptions
    "OpenAIClientError",
    "ModelNotSupportedError",
    "ModelVersionError",
    "APIResponseError",
    "InvalidResponseFormatError",
    "EmptyResponseError",
    "JSONParseError",
    "StreamInterruptedError",
    "StreamBufferError",
    "StreamParseError",
    "BufferOverflowError",
]




import json
import logging
import re
from dataclasses import dataclass
from io import BytesIO, StringIO
from typing import Any, Callable, Dict, Optional, Type, Union

import ijson  # type: ignore # missing stubs
from pydantic import BaseModel, ValidationError

from .errors import StreamBufferError

# Buffer size constants
MAX_BUFFER_SIZE = 1024 * 1024  # 1MB
BUFFER_CLEANUP_THRESHOLD = MAX_BUFFER_SIZE // 2
LOG_SIZE_THRESHOLD = 100 * 1024  # 100KB

# Error handling constants
MAX_CLEANUP_ATTEMPTS = 3
MAX_PARSE_ERRORS = 5


@dataclass
class StreamConfig:
    """Configuration for stream behavior.

    Attributes:
        max_buffer_size: Maximum buffer size in bytes (default: 1MB)
        cleanup_threshold: Size at which to trigger cleanup (default: 512KB)
        chunk_size: Size of chunks for processing (default: 8KB)
        max_cleanup_attempts: Maximum number of cleanup attempts (default: 3)
        max_parse_errors: Maximum number of parse errors before failing (default: 5)
        log_size_threshold: Size change that triggers logging (default: 100KB)
    """

    max_buffer_size: int = MAX_BUFFER_SIZE  # 1MB max buffer size
    cleanup_threshold: int = BUFFER_CLEANUP_THRESHOLD  # Clean up at 512KB
    chunk_size: int = 8192  # 8KB chunks
    max_cleanup_attempts: int = MAX_CLEANUP_ATTEMPTS
    max_parse_errors: int = MAX_PARSE_ERRORS
    log_size_threshold: int = LOG_SIZE_THRESHOLD


class BufferError(StreamBufferError):
    """Base class for buffer-related errors."""

    pass


class BufferOverflowError(BufferError):
    """Raised when buffer size exceeds limits."""

    pass


class ParseError(BufferError):
    """Raised when JSON parsing fails repeatedly."""

    pass


class StreamBuffer:
    def __init__(
        self,
        config: Optional[StreamConfig] = None,
        schema: Optional[Type[BaseModel]] = None,
    ):
        """Initialize the buffer with optional config and schema."""
        self.config = config or StreamConfig()
        if schema is not None and not issubclass(schema, BaseModel):
            raise ValueError("Schema must be a Pydantic model class")
        self.schema = schema
        self._buffer = StringIO()
        self.total_bytes = 0
        self._cleanup_stats: Dict[str, Union[None, int, str]] = {
            "strategy": None,
            "cleaned_bytes": 0,
        }
        self.parse_errors = 0
        self.cleanup_attempts = 0

    def _log(
        self,
        on_log: Optional[Callable[[int, str, dict[str, Any]], None]],
        level: int,
        message: str,
        data: Optional[dict[str, Any]] = None,
    ) -> None:
        """Log a message with optional callback."""
        if on_log:
            on_log(level, message, data or {})

    def should_log_size(self) -> bool:
        """Check if buffer size should be logged."""
        return self.total_bytes >= self.config.log_size_threshold

    def process_stream_chunk(
        self,
        content: str,
        on_log: Optional[Callable[[int, str, dict[str, Any]], None]] = None,
    ) -> Optional[Any]:
        """Process a stream chunk and return parsed content if complete."""
        try:
            self.write(content)

            if self.should_log_size():
                self._log(
                    on_log,
                    logging.DEBUG,
                    "Buffer size increased significantly",
                    {"size_bytes": self.total_bytes},
                )

            current_content = self.getvalue()
            if self.schema is not None:
                try:
                    result = self.schema.model_validate_json(current_content)
                    self._log(
                        on_log,
                        logging.DEBUG,
                        "Successfully parsed complete object",
                    )
                    self.reset()
                    return result
                except ValidationError as e:
                    self._cleanup_stats.update(
                        {
                            "validation_error": str(e),
                            "content_length": len(current_content),
                        }
                    )
                    if self.total_bytes > self.config.cleanup_threshold:
                        self.cleanup()
                except json.JSONDecodeError as e:
                    self._cleanup_stats.update(
                        {
                            "json_error": str(e),
                            "error_position": int(e.pos),
                            "content_length": len(current_content),
                        }
                    )
                    if self.total_bytes > self.config.cleanup_threshold:
                        self.cleanup()
            return None
        except BufferError as e:
            self._cleanup_stats.update(
                {
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "buffer_size": self.total_bytes,
                }
            )
            self._log(
                on_log,
                logging.ERROR,
                "Critical stream buffer error",
                {"error": str(e), "bytes": self.total_bytes},
            )
            raise

    def write(self, content: str) -> None:
        """Write content to the buffer with size checks and error handling."""
        try:
            content_bytes = len(content.encode())
            if self.total_bytes + content_bytes > self.config.max_buffer_size:
                raise BufferOverflowError(
                    f"Buffer would exceed max size of {self.config.max_buffer_size} bytes"
                )

            self._buffer.write(content)
            self.total_bytes += content_bytes

        except (IOError, OSError) as e:
            raise BufferError(f"Failed to write to buffer: {str(e)}")

    def getvalue(self) -> str:
        """Get the current buffer contents as a string with caching."""
        return self._buffer.getvalue()

    def cleanup(self) -> None:
        """Find and preserve the last complete JSON object using multiple strategies."""
        self.cleanup_attempts += 1
        if self.cleanup_attempts >= self.config.max_cleanup_attempts:
            raise ParseError(
                f"Exceeded maximum cleanup attempts ({self.config.max_cleanup_attempts})"
            )

        content = self.getvalue()
        if not content:
            return

        # Check if content contains JSON-like patterns anywhere
        content_stripped = content.strip()
        is_json_like = "{" in content_stripped or "[" in content_stripped

        original_length = len(content)
        original_bytes = len(content.encode("utf-8"))

        try:
            # Strategy 1: Use ijson for incremental parsing
            try:
                # Convert content to bytes and use BytesIO
                content_bytes = content.encode("utf-8")
                parser = ijson.parse(BytesIO(content_bytes))
                stack = []
                last_complete = 0
                current_pos = 0

                for _, event, _ in parser:
                    # Track nesting level without relying on position
                    if event in ("start_map", "start_array"):
                        stack.append(True)
                    elif event in ("end_map", "end_array"):
                        if stack:
                            stack.pop()
                            if not stack:  # Complete object found
                                # Use the current content up to this point
                                try:
                                    # Validate the content
                                    complete_content = content[:current_pos]
                                    json.loads(
                                        complete_content
                                    )  # Verify it's valid JSON
                                    if self.schema is not None:
                                        self.schema.model_validate_json(
                                            complete_content
                                        )
                                    last_complete = current_pos
                                    break
                                except (json.JSONDecodeError, ValidationError):
                                    continue
                    current_pos += 1

                if last_complete > 0:
                    self._update_buffer(content[last_complete:])
                    self._log_cleanup_stats(
                        original_length=original_length,
                        original_bytes=original_bytes,
                        cleanup_position=last_complete,
                        strategy="ijson_parsing",
                    )
                    return

            except ijson.JSONError as e:
                if is_json_like:
                    self._cleanup_stats.update(
                        {"json_error": str(e), "error_type": type(e).__name__}
                    )
                    self.parse_errors += 1
                    if (
                        self.cleanup_attempts == 1
                    ):  # Only raise on first attempt
                        raise ParseError(f"Failed to parse JSON content: {e}")
                    # On subsequent attempts, let max attempts check handle it

            # Strategy 2: Pattern matching as fallback
            if is_json_like:
                self.parse_errors += 1

                patterns = [
                    "}",  # Object end
                    "]",  # Array end
                    '"',  # String end
                    r"\btrue\b",  # Boolean true
                    r"\bfalse\b",  # Boolean false
                    r"\bnull\b",  # Null
                    r"\d+\.?\d*(?:[eE][+-]?\d+)?\s*(?:,|]|}|$)",  # Numbers
                ]

                # Try each pattern in order of complexity
                last_pos = -1
                for pattern in patterns:
                    matches = list(re.finditer(pattern, content))
                    if matches:
                        pos = matches[-1].end()
                        try:
                            complete_content = content[:pos]
                            # First validate JSON syntax
                            json.loads(complete_content)
                            # Then validate against schema if provided
                            if self.schema is not None:
                                self.schema.model_validate_json(
                                    complete_content
                                )
                            last_pos = pos
                            break
                        except json.JSONDecodeError as e:
                            context = self._get_error_context(content, e.pos)
                            self._cleanup_stats.update(
                                {
                                    "json_error": str(e),
                                    "error_position": int(e.pos),
                                    "error_context": context,
                                    "error_type": type(e).__name__,
                                }
                            )
                            continue
                        except ValidationError as e:
                            error_loc = e.errors()[0].get("loc", ["value"])[-1]
                            context = self._get_error_context(
                                complete_content, error_loc
                            )
                            self._cleanup_stats.update(
                                {
                                    "validation_error": str(e),
                                    "error_context": context,
                                    "error_type": type(e).__name__,
                                }
                            )
                            raise ParseError(
                                f"Schema validation failed: {e}\nContext: {context}"
                            )

                if last_pos > 0:
                    self._update_buffer(content[last_pos:])
                    self._log_cleanup_stats(
                        original_length=original_length,
                        original_bytes=original_bytes,
                        cleanup_position=last_pos,
                        strategy="pattern_matching",
                    )
                    return

                # If both strategies fail for JSON-like content, check max errors
                if self.parse_errors >= self.config.max_parse_errors:
                    context = self._get_error_context(
                        content, 0
                    )  # Show context from start
                    self._cleanup_stats.update(
                        {
                            "json_error": "Failed to find valid JSON content",
                            "error_context": context,
                            "error_type": "ParseError",
                        }
                    )
                    raise ParseError(
                        f"Failed to parse JSON content after {self.parse_errors} attempts.\nContext: {context}"
                    )

        except Exception as e:
            if isinstance(e, ParseError):
                raise e
            if isinstance(e, ValidationError):
                error_loc = e.errors()[0].get("loc", ["value"])[-1]
                context = self._get_error_context(content, error_loc)
                self._cleanup_stats.update(
                    {
                        "validation_error": str(e),
                        "error_context": context,
                        "error_type": type(e).__name__,
                    }
                )
                raise ParseError(
                    f"Schema validation failed: {e}\nContext: {context}"
                )

            self._cleanup_stats.update(
                {
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "error_context": self._get_error_context(content, 0),
                }
            )
            raise ParseError(f"Error during cleanup: {e}")

    def _update_buffer(self, new_content: str) -> None:
        """Update buffer contents efficiently."""
        self._buffer.seek(0)
        self._buffer.truncate()
        self._buffer.write(new_content)
        self.total_bytes = len(new_content.encode("utf-8"))

    def _log_cleanup_stats(
        self,
        original_length: int,
        original_bytes: int,
        cleanup_position: int,
        strategy: str,
    ) -> None:
        """Log statistics about buffer cleanup."""
        new_content = self.getvalue()
        new_length = len(new_content)
        new_bytes = len(new_content.encode("utf-8"))

        self._cleanup_stats.update(
            {
                "attempt": self.cleanup_attempts + 1,
                "strategy": strategy,
                "original_length": original_length,
                "original_bytes": original_bytes,
                "cleanup_position": cleanup_position,
                "chars_removed": original_length - new_length,
                "bytes_removed": original_bytes - new_bytes,
                "remaining_length": new_length,
                "remaining_bytes": new_bytes,
            }
        )

    def reset(self) -> None:
        """Reset the buffer state while preserving configuration."""
        try:
            self._buffer.seek(0)
            self._buffer.truncate()
        except ValueError:  # Buffer is closed
            self._buffer = StringIO()  # Create new buffer
        self.total_bytes = 0
        self._cleanup_stats = {"strategy": None, "cleaned_bytes": 0}
        self.parse_errors = 0
        self.cleanup_attempts = 0

    def close(self) -> None:
        """Close the buffer and clean up resources."""
        if not self._buffer.closed:
            self._buffer.close()
        self.total_bytes = 0
        self._cleanup_stats = {"strategy": None, "cleaned_bytes": 0}
        self.parse_errors = 0
        self.cleanup_attempts = 0

    def _get_error_context(self, content: str, pos: Union[int, str]) -> str:
        """Get context around an error position."""
        if isinstance(pos, str):
            # For validation errors, find the position of the field in the JSON
            try:
                pos = content.find(f'"{pos}"')
            except (TypeError, ValueError):
                pos = 0
        else:
            pos = int(pos)

        start = max(0, pos - 20)
        end = min(len(content), pos + 20)
        return content[start:end]




"""Model version information."""

import re
from dataclasses import dataclass
from datetime import date, datetime
from functools import total_ordering
from typing import Optional


@total_ordering
@dataclass
class ModelVersion:
    """Represents a model version with year, month, and day components."""

    year: int
    month: int
    day: int

    @classmethod
    def from_string(cls, version_str: str) -> "ModelVersion":
        """Create a ModelVersion from a string in YYYY-MM-DD format.

        Args:
            version_str: Version string in YYYY-MM-DD format

        Returns:
            ModelVersion instance

        Raises:
            ValueError: If the version string is invalid or represents an invalid date

        Examples:
            >>> ModelVersion.from_string("2024-01-15")
            ModelVersion(year=2024, month=1, day=15)
        """
        match = re.match(r"(\d{4})-(\d{2})-(\d{2})", version_str)
        if not match:
            raise ValueError(f"Invalid version format: {version_str}")

        year = int(match.group(1))
        month = int(match.group(2))
        day = int(match.group(3))
        try:
            date(year, month, day)
        except ValueError as e:
            raise ValueError(f"Invalid date: {version_str} - {str(e)}")

        return cls(year=year, month=month, day=day)

    def __str__(self) -> str:
        """Return string representation in YYYY-MM-DD format."""
        return f"{self.year}-{self.month:02d}-{self.day:02d}"

    def __lt__(self, other: object) -> bool:
        """Compare versions for less than."""
        if not isinstance(other, ModelVersion):
            return NotImplemented
        return (self.year, self.month, self.day) < (
            other.year,
            other.month,
            other.day,
        )

    def __eq__(self, other: object) -> bool:
        """Compare versions for equality."""
        if not isinstance(other, ModelVersion):
            return NotImplemented
        return (self.year, self.month, self.day) == (
            other.year,
            other.month,
            other.day,
        )


def parse_model_version(model: str) -> Optional[ModelVersion]:
    """
    Parse a model name to extract its version information.

    Args:
        model: The model name to parse (e.g., "gpt-4-0314", "gpt-4-0613", "gpt-4-1106-preview")

    Returns:
        ModelVersion if the model name contains a valid version, None otherwise.

    Examples:
        >>> parse_model_version("gpt-4-0314")
        ModelVersion(year=2023, month=3, day=14)
        >>> parse_model_version("gpt-4-1106-preview")
        ModelVersion(year=2023, month=11, day=6)
        >>> parse_model_version("gpt-4")  # Alias without version
        None
    """
    # Match MMDD pattern in model name
    match = re.search(r"(\d{2})(\d{2})(?:-preview)?$", model)
    if not match:
        return None

    month, day = map(int, match.groups())
    # Assume current year for now - could be made configurable if needed
    year = datetime.now().year
    return ModelVersion(year=year, month=month, day=day)




# src/openai_structured/errors.py
from typing import Optional

from .model_version import ModelVersion


class OpenAIClientError(Exception):
    """Base class for exceptions in the OpenAI client."""

    pass


class CLIError(Exception):
    """Base class for CLI-related exceptions."""

    pass


class VariableError(CLIError):
    """Base class for variable-related errors."""

    pass


class VariableNameError(VariableError):
    """Raised when a variable name is invalid."""

    pass


class VariableValueError(VariableError):
    """Raised when a variable value is invalid."""

    pass


class InvalidJSONError(VariableError):
    """Raised when JSON variable value is invalid."""

    pass


class PathError(CLIError):
    """Base class for path-related errors."""

    pass


class FileNotFoundError(PathError):
    """Raised when a file is not found."""

    pass


class DirectoryNotFoundError(PathError):
    """Raised when a directory is not found."""

    pass


class PathSecurityError(PathError):
    """Raised when a path is outside the allowed directory."""

    pass


class TaskTemplateError(CLIError):
    """Base class for task template errors."""

    pass


class TaskTemplateVariableError(TaskTemplateError):
    """Raised when a template uses undefined variables."""

    pass


class TaskTemplateSyntaxError(TaskTemplateError):
    """Raised when a template has invalid syntax."""

    pass


class SchemaError(CLIError):
    """Base class for schema-related errors."""

    pass


class SchemaValidationError(SchemaError):
    """Raised when schema validation fails."""

    pass


class SchemaFileError(SchemaError):
    """Raised when there are issues with the schema file."""

    pass


class ModelNotSupportedError(OpenAIClientError):
    """Raised when a model is not supported."""

    def __init__(self, message: str, model: Optional[str] = None):
        super().__init__(message)
        self.model = model


class ModelVersionError(ModelNotSupportedError):
    """Raised when the model version is not supported."""

    def __init__(self, model: str, min_version: ModelVersion) -> None:
        super().__init__(
            f"Model '{model}' version is not supported. "
            f"Minimum version required: {min_version}"
        )


class APIResponseError(OpenAIClientError):
    """Raised for errors in the API response."""

    def __init__(
        self,
        message: str,
        response_id: Optional[str] = None,
        content: Optional[str] = None,
    ):
        super().__init__(message)
        self.response_id = response_id
        self.content = content


class InvalidResponseFormatError(APIResponseError):
    """Raised if the API doesn't provide the expected JSON format."""

    pass


class EmptyResponseError(APIResponseError):
    """Raised if the API returns an empty response."""

    pass


class JSONParseError(InvalidResponseFormatError):
    """Raised when JSON parsing fails."""

    pass


class StreamInterruptedError(OpenAIClientError):
    """Raised when a stream is interrupted unexpectedly."""

    def __init__(self, message: str, chunk_index: Optional[int] = None):
        super().__init__(message)
        self.chunk_index = chunk_index


class StreamBufferError(StreamInterruptedError):
    """Base class for stream buffer related errors."""

    pass


class StreamParseError(StreamInterruptedError):
    """Raised when stream content cannot be parsed after multiple attempts."""

    def __init__(self, message: str, attempts: int, last_error: Exception):
        super().__init__(
            f"{message} after {attempts} attempts. Last error: {last_error}"
        )
        self.attempts = attempts
        self.last_error = last_error


class BufferOverflowError(StreamBufferError):
    """Raised when the buffer exceeds size limits."""

    pass




"""FileInfoList implementation providing smart file content access."""

from typing import List, Union

from .file_info import FileInfo


class FileInfoList(List[FileInfo]):
    """List of FileInfo objects with smart content access.

    This class extends List[FileInfo] to provide convenient access to file contents
    and metadata. When the list contains exactly one file from a single file mapping,
    properties like content return the value directly. For multiple files or directory
    mappings, properties return a list of values.

    Examples:
        Single file (--file):
            files = FileInfoList([file_info], from_dir=False)
            content = files.content  # Returns "file contents"

        Multiple files or directory (--files or --dir):
            files = FileInfoList([file1, file2])  # or FileInfoList([file1], from_dir=True)
            content = files.content  # Returns ["contents1", "contents2"] or ["contents1"]

        Backward compatibility:
            content = files[0].content  # Still works

    Properties:
        content: File content(s) - string for single file mapping, list for multiple files or directory
        path: File path(s)
        abs_path: Absolute file path(s)
        size: File size(s) in bytes

    Raises:
        ValueError: When accessing properties on an empty list
    """

    def __init__(self, files: List[FileInfo], from_dir: bool = False) -> None:
        """Initialize FileInfoList.

        Args:
            files: List of FileInfo objects
            from_dir: Whether this list was created from a directory mapping
        """
        super().__init__(files)
        self._from_dir = from_dir

    @property
    def content(self) -> Union[str, List[str]]:
        """Get the content of the file(s).

        Returns:
            Union[str, List[str]]: For a single file from file mapping, returns its content as a string.
                                  For multiple files or directory mapping, returns a list of contents.

        Raises:
            ValueError: If the list is empty
        """
        if not self:
            raise ValueError("No files in FileInfoList")
        if len(self) == 1 and not self._from_dir:
            return self[0].content
        return [f.content for f in self]

    @property
    def path(self) -> Union[str, List[str]]:
        """Get the path of the file(s).

        Returns:
            Union[str, List[str]]: For a single file from file mapping, returns its path as a string.
                                  For multiple files or directory mapping, returns a list of paths.

        Raises:
            ValueError: If the list is empty
        """
        if not self:
            raise ValueError("No files in FileInfoList")
        if len(self) == 1 and not self._from_dir:
            return self[0].path
        return [f.path for f in self]

    @property
    def abs_path(self) -> Union[str, List[str]]:
        """Get the absolute path of the file(s).

        Returns:
            Union[str, List[str]]: For a single file from file mapping, returns its absolute path as a string.
                                  For multiple files or directory mapping, returns a list of absolute paths.

        Raises:
            ValueError: If the list is empty
        """
        if not self:
            raise ValueError("No files in FileInfoList")
        if len(self) == 1 and not self._from_dir:
            return self[0].abs_path
        return [f.abs_path for f in self]

    @property
    def size(self) -> Union[int, List[int]]:
        """Get file size(s) in bytes.

        Returns:
            Union[int, List[int]]: For a single file from file mapping, returns its size in bytes.
                                  For multiple files or directory mapping, returns a list of sizes.

        Raises:
            ValueError: If the list is empty or if any file size is None
        """
        if not self:
            raise ValueError("No files in FileInfoList")

        # For single file not from directory, return its size
        if len(self) == 1 and not self._from_dir:
            size = self[0].size
            if size is None:
                raise ValueError(
                    f"Could not get size for file: {self[0].path}"
                )
            return size

        # For multiple files, collect all sizes
        sizes = []
        for f in self:
            size = f.size
            if size is None:
                raise ValueError(f"Could not get size for file: {f.path}")
            sizes.append(size)
        return sizes

    def __str__(self) -> str:
        """Get string representation of the file list.

        Returns:
            str: String representation in format FileInfoList([paths])
        """
        if not self:
            return "FileInfoList([])"
        if len(self) == 1:
            return f"FileInfoList(['{self[0].path}'])"
        return f"FileInfoList({[f.path for f in self]})"

    def __repr__(self) -> str:
        """Get detailed string representation of the file list.

        Returns:
            str: Same as str() for consistency
        """
        return str(self)




"""Schema validation and proxy objects for template processing.

Provides proxy objects that validate attribute/key access during template validation
by checking against actual data structures passed to the proxies.

Classes:
    - ValidationProxy: Base proxy class for validation
    - FileInfoProxy: Proxy for file information objects with standard attributes
    - DictProxy: Proxy for dictionary objects that validates against actual structure
    - ListProxy: Proxy for list/iterable objects that validates indices and content
    - StdinProxy: Proxy for lazy stdin access
    - LazyValidationProxy: Proxy that delays attribute access until string conversion
    - DotDict: Dictionary wrapper that supports both dot notation and dictionary access

Examples:
    Create validation context with actual data:
    >>> data = {
    ...     'config': {'debug': True, 'settings': {'mode': 'test'}},
    ...     'source_file': FileInfo('test.txt')
    ... }
    >>> context = create_validation_context(data)
    >>> # config will be a DictProxy validating against actual structure
    >>> # source_file will be a FileInfoProxy with standard attributes

    Access validation:
    >>> # Valid access patterns:
    >>> config = context['config']
    >>> debug_value = config.debug  # OK - debug exists in data
    >>> mode = config.settings.mode  # OK - settings.mode exists in data
    >>>
    >>> # Invalid access raises ValueError:
    >>> config.invalid  # Raises ValueError - key doesn't exist
    >>> config.settings.invalid  # Raises ValueError - nested key doesn't exist

    File info validation:
    >>> file = context['source_file']
    >>> name = file.name  # OK - standard attribute
    >>> content = file.content  # OK - standard attribute
    >>> file.invalid  # Raises ValueError - invalid attribute

Notes:
    - DictProxy validates against actual dictionary structure
    - FileInfoProxy validates standard file attributes
    - ListProxy validates indices and returns appropriate proxies
    - All invalid attribute/key access raises ValueError with details
"""

import logging
import sys
from typing import Any, Dict, Iterator, List, Optional, Set, Tuple, Union, cast

from .file_utils import FileInfo

logger = logging.getLogger(__name__)


class ValidationProxy:
    """Proxy object that validates attribute/key access during template validation."""

    def __init__(
        self,
        var_name: str,
        value: Any = None,
        valid_attrs: Optional[Set[str]] = None,
        nested_attrs: Optional[Dict[str, Any]] = None,
        allow_nested: bool = True,
    ) -> None:
        """Initialize the proxy.

        Args:
            var_name: The name of the variable being proxied.
            value: The value being proxied (optional).
            valid_attrs: Set of valid attribute names.
            nested_attrs: Dictionary of nested attributes and their allowed values.
            allow_nested: Whether to allow nested attribute access.
        """
        self._var_name = var_name
        self._value = value
        self._valid_attrs = valid_attrs or set()
        self._nested_attrs = nested_attrs or {}
        self._allow_nested = allow_nested
        self._accessed_attributes: Set[str] = set()
        logger.debug(
            "Created ValidationProxy for %s with valid_attrs=%s, nested_attrs=%s, allow_nested=%s",
            var_name,
            valid_attrs,
            nested_attrs,
            allow_nested,
        )

    def __getattr__(self, name: str) -> Union["ValidationProxy", "DictProxy"]:
        """Validate attribute access during template validation."""
        logger.debug("\n=== ValidationProxy.__getattr__ ===")
        logger.debug("Called for: %s.%s", self._var_name, name)
        logger.debug(
            "State: valid_attrs=%s, nested_attrs=%s, allow_nested=%s",
            self._valid_attrs,
            self._nested_attrs,
            self._allow_nested,
        )

        self._accessed_attributes.add(name)

        # Allow HTML escaping attributes for all variables
        if name in {"__html__", "__html_format__"}:
            logger.debug(
                "Allowing HTML escape attribute %s for %s",
                name,
                self._var_name,
            )
            return ValidationProxy(f"{self._var_name}.{name}", value="")

        # Check nested attributes
        if name in self._nested_attrs:
            nested_value = self._nested_attrs[name]
            if isinstance(nested_value, dict):
                return ValidationProxy(
                    f"{self._var_name}.{name}",
                    nested_attrs=nested_value,
                    allow_nested=True,
                )
            elif isinstance(nested_value, set):
                if (
                    not nested_value
                ):  # Empty set means "any nested keys allowed"
                    return ValidationProxy(
                        f"{self._var_name}.{name}", allow_nested=True
                    )
                return ValidationProxy(
                    f"{self._var_name}.{name}",
                    valid_attrs=nested_value,
                    allow_nested=False,
                )

        # Validate against valid_attrs if present
        if self._valid_attrs:
            if name not in self._valid_attrs:
                raise ValueError(
                    f"Task template uses undefined attribute '{self._var_name}.{name}'"
                )
            return ValidationProxy(
                f"{self._var_name}.{name}", allow_nested=False
            )

        # Check nesting allowance and get actual value
        if not self._allow_nested:
            raise ValueError(
                f"Task template uses undefined attribute '{self._var_name}.{name}'"
            )

        value = None
        if self._value is not None and hasattr(self._value, name):
            value = getattr(self._value, name)

        return ValidationProxy(
            f"{self._var_name}.{name}", value=value, allow_nested=True
        )

    def __getitem__(self, key: Any) -> Union["ValidationProxy", "DictProxy"]:
        """Support item access for validation."""
        key_str = f"['{key}']" if isinstance(key, str) else f"[{key}]"
        return ValidationProxy(
            f"{self._var_name}{key_str}",
            valid_attrs=self._valid_attrs,
            allow_nested=self._allow_nested,
        )

    def __str__(self) -> str:
        """Convert the proxy value to a string."""
        return str(self._value) if self._value is not None else ""

    def __html__(self) -> str:
        """Return HTML representation."""
        return str(self)

    def __html_format__(self, format_spec: str) -> str:
        """Return formatted HTML representation."""
        return str(self)

    def __iter__(self) -> Iterator[Union["ValidationProxy", "DictProxy"]]:
        """Support iteration for validation."""
        yield ValidationProxy(
            f"{self._var_name}[0]", valid_attrs=self._valid_attrs
        )

    def get_accessed_attributes(self) -> Set[str]:
        """Get the set of accessed attributes."""
        return self._accessed_attributes.copy()


class FileInfoProxy:
    """Proxy for FileInfo that provides validation during template rendering.

    This class wraps FileInfo to provide validation during template rendering.
    It ensures that only valid attributes are accessed and returns empty strings
    for content to support filtering in templates.

    Attributes:
        _var_name: Base variable name for error messages
        _value: The wrapped FileInfo object
        _accessed_attrs: Set of attributes that have been accessed
        _valid_attrs: Set of valid attribute names
    """

    def __init__(self, var_name: str, value: FileInfo) -> None:
        """Initialize FileInfoProxy.

        Args:
            var_name: Base variable name for error messages
            value: FileInfo object to validate
        """
        self._var_name = var_name
        self._value = value
        self._accessed_attrs: Set[str] = set()
        self._valid_attrs = {
            "name",
            "path",
            "content",
            "ext",
            "basename",
            "dirname",
            "abs_path",
            "exists",
            "is_file",
            "is_dir",
            "size",
            "mtime",
            "encoding",
            "hash",
            "extension",
            "parent",
            "stem",
            "suffix",
            "__html__",
            "__html_format__",
        }

    def __getattr__(self, name: str) -> str:
        """Get attribute value with validation.

        Args:
            name: Attribute name to get

        Returns:
            Empty string for content, actual value for other attributes

        Raises:
            ValueError: If attribute name is not valid
        """
        if name not in self._valid_attrs:
            raise ValueError(
                f"undefined attribute '{name}' for file {self._var_name}"
            )

        self._accessed_attrs.add(name)

        # Return empty string for content and HTML methods to support filtering
        if name in ("content", "__html__", "__html_format__"):
            return ""

        # Return actual value for all other attributes
        return str(getattr(self._value, name))

    def __str__(self) -> str:
        """Convert to string.

        Returns:
            Empty string to support filtering
        """
        return ""

    def __html__(self) -> str:
        """Convert to HTML-safe string.

        Returns:
            Empty string to support filtering
        """
        return ""


class DictProxy:
    """Proxy for dictionary access during validation.

    Validates all attribute/key access against the actual dictionary structure.
    Provides standard dictionary methods (get, items, keys, values).
    Supports HTML escaping for Jinja2 compatibility.
    """

    def __init__(self, name: str, value: Dict[str, Any]) -> None:
        """Initialize the proxy.

        Args:
            name: The name of the variable being proxied.
            value: The dictionary being proxied.
        """
        self._name = name
        self._value = value

    def __getattr__(self, name: str) -> Union["DictProxy", ValidationProxy]:
        """Validate attribute access against actual dictionary structure."""
        if name in {"get", "items", "keys", "values"}:
            return cast(
                Union["DictProxy", ValidationProxy], getattr(self, f"_{name}")
            )

        if name not in self._value:
            raise ValueError(
                f"Task template uses undefined attribute '{self._name}.{name}'"
            )

        if isinstance(self._value[name], dict):
            return DictProxy(f"{self._name}.{name}", self._value[name])
        return ValidationProxy(f"{self._name}.{name}")

    def __getitem__(self, key: Any) -> Union["DictProxy", ValidationProxy]:
        """Validate dictionary key access."""
        if isinstance(key, int):
            key = str(key)

        if key not in self._value:
            raise ValueError(
                f"Task template uses undefined key '{self._name}['{key}']'"
            )

        if isinstance(self._value[key], dict):
            return DictProxy(f"{self._name}['{key}']", self._value[key])
        return ValidationProxy(f"{self._name}['{key}']")

    def __contains__(self, key: Any) -> bool:
        """Support 'in' operator for validation."""
        if isinstance(key, int):
            key = str(key)
        return key in self._value

    def _get(self, key: str, default: Any = None) -> Any:
        """Implement dict.get() method."""
        try:
            return self[key]
        except ValueError:
            return default

    def _items(
        self,
    ) -> Iterator[Tuple[str, Union["DictProxy", ValidationProxy]]]:
        """Implement dict.items() method."""
        for key, value in self._value.items():
            if isinstance(value, dict):
                yield (key, DictProxy(f"{self._name}['{key}']", value))
            else:
                yield (key, ValidationProxy(f"{self._name}['{key}']"))

    def _keys(self) -> Iterator[str]:
        """Implement dict.keys() method."""
        for key in self._value.keys():
            yield key

    def _values(self) -> Iterator[Union["DictProxy", ValidationProxy]]:
        """Implement dict.values() method."""
        for key, value in self._value.items():
            if isinstance(value, dict):
                yield DictProxy(f"{self._name}['{key}']", value)
            else:
                yield ValidationProxy(f"{self._name}['{key}']")

    def __html__(self) -> str:
        """Support HTML escaping."""
        return ""

    def __html_format__(self, spec: str) -> str:
        """Support HTML formatting."""
        return ""


class ListProxy(ValidationProxy):
    """Proxy for list/iterable objects during validation.

    For file lists (from --files or --dir), validates that only valid file attributes
    are accessed. For other lists, validates indices and returns appropriate proxies
    based on the actual content type.
    """

    def __init__(self, var_name: str, value: List[Any]) -> None:
        """Initialize the proxy.

        Args:
            var_name: The name of the variable being proxied.
            value: The list being proxied.
        """
        super().__init__(var_name)
        self._value = value
        # Determine if this is a list of files
        self._is_file_list = value and all(
            isinstance(item, FileInfo) for item in value
        )
        self._file_attrs = {
            "name",
            "path",
            "abs_path",
            "content",
            "size",
            "extension",
            "exists",
            "mtime",
            "encoding",
            "dir",
            "hash",
            "is_file",
            "is_dir",
            "parent",
            "stem",
            "suffix",
        }

    def __len__(self) -> int:
        """Support len() for validation."""
        return len(self._value)

    def __iter__(self) -> Iterator[Union[ValidationProxy, DictProxy]]:
        """Support iteration, returning appropriate proxies."""
        if self._is_file_list:
            # For file lists, return FileInfoProxy for validation
            for i in range(len(self._value)):
                yield cast(
                    Union[ValidationProxy, DictProxy],
                    FileInfoProxy(f"{self._var_name}[{i}]", self._value[i]),
                )
        else:
            # For other lists, return basic ValidationProxy
            for i in range(len(self._value)):
                if isinstance(self._value[i], dict):
                    yield DictProxy(f"{self._var_name}[{i}]", self._value[i])
                else:
                    yield ValidationProxy(f"{self._var_name}[{i}]")

    def __getitem__(self, key: Any) -> Union[ValidationProxy, DictProxy]:
        """Validate list index access and return appropriate proxy."""
        if isinstance(key, int) and (key < 0 or key >= len(self._value)):
            raise ValueError(
                f"List index {key} out of range for {self._var_name}"
            )

        key_str = f"['{key}']" if isinstance(key, str) else f"[{key}]"

        if self._is_file_list:
            return cast(
                Union[ValidationProxy, DictProxy],
                FileInfoProxy(f"{self._var_name}{key_str}", self._value[key]),
            )
        else:
            value = self._value[key]
            if isinstance(value, dict):
                return DictProxy(f"{self._var_name}{key_str}", value)
            return ValidationProxy(f"{self._var_name}{key_str}")


class StdinProxy:
    """Proxy for lazy stdin access.

    This proxy only reads from stdin when the content is actually accessed.
    This prevents unnecessary stdin reads when the template doesn't use stdin.
    """

    def __init__(self) -> None:
        """Initialize the proxy."""
        self._content: Optional[str] = None

    def __str__(self) -> str:
        """Return stdin content when converted to string."""
        if self._content is None:
            if sys.stdin.isatty():
                raise ValueError("No input available on stdin")
            self._content = sys.stdin.read()
        return self._content or ""

    def __html__(self) -> str:
        """Support HTML escaping."""
        return str(self)

    def __html_format__(self, spec: str) -> str:
        """Support HTML formatting."""
        return str(self)


class DotDict:
    """Dictionary wrapper that supports both dot notation and dictionary access."""

    def __init__(self, data: Dict[str, Any]):
        self._data = data

    def __getattr__(self, name: str) -> Any:
        try:
            value = self._data[name]
            return DotDict(value) if isinstance(value, dict) else value
        except KeyError:
            raise AttributeError(f"'DotDict' object has no attribute '{name}'")

    def __getitem__(self, key: str) -> Any:
        value = self._data[key]
        return DotDict(value) if isinstance(value, dict) else value

    def __contains__(self, key: str) -> bool:
        return key in self._data

    def get(self, key: str, default: Any = None) -> Any:
        value = self._data.get(key, default)
        return DotDict(value) if isinstance(value, dict) else value

    def items(self) -> List[Tuple[str, Any]]:
        return [
            (k, DotDict(v) if isinstance(v, dict) else v)
            for k, v in self._data.items()
        ]

    def keys(self) -> List[str]:
        return list(self._data.keys())

    def values(self) -> List[Any]:
        return [
            DotDict(v) if isinstance(v, dict) else v
            for v in self._data.values()
        ]


def create_validation_context(
    template_context: Dict[str, Any]
) -> Dict[str, Any]:
    """Create validation context with proxy objects.

    Creates appropriate proxy objects based on the actual type and content
    of each value in the mappings. Validates all attribute/key access
    against the actual data structures.

    Args:
        template_context: Original template context with actual values

    Returns:
        Dictionary with proxy objects for validation

    Example:
        >>> data = {'config': {'debug': True}, 'files': [FileInfo('test.txt')]}
        >>> context = create_validation_context(data)
        >>> # context['config'] will be DictProxy validating against {'debug': True}
        >>> # context['files'] will be ListProxy validating file attributes
    """
    validation_context: Dict[str, Any] = {}

    # Add stdin proxy by default - it will only read if accessed
    validation_context["stdin"] = StdinProxy()

    for name, value in template_context.items():
        if isinstance(value, FileInfo):
            validation_context[name] = FileInfoProxy(name, value)
        elif isinstance(value, dict):
            validation_context[name] = DictProxy(name, value)
        elif isinstance(value, list):
            validation_context[name] = ListProxy(name, value)
        else:
            validation_context[name] = ValidationProxy(name, value)

    return validation_context




"""Security type definitions and protocols."""

from pathlib import Path
from typing import List, Protocol


class SecurityManagerProtocol(Protocol):
    """Protocol defining the interface for security managers."""

    @property
    def base_dir(self) -> Path:
        """Get the base directory."""
        ...

    @property
    def allowed_dirs(self) -> List[Path]:
        """Get the list of allowed directories."""
        ...

    def add_allowed_dir(self, directory: str) -> None:
        """Add a directory to the set of allowed directories."""
        ...

    def add_allowed_dirs_from_file(self, file_path: str) -> None:
        """Add allowed directories from a file."""
        ...

    def is_path_allowed(self, path: str) -> bool:
        """Check if a path is allowed."""
        ...

    def validate_path(self, path: str, purpose: str = "access") -> Path:
        """Validate and normalize a path."""
        ...

    def is_allowed_file(self, path: str) -> bool:
        """Check if file access is allowed."""
        ...

    def is_allowed_path(self, path_str: str) -> bool:
        """Check if string path is allowed."""
        ...

    def resolve_path(self, path: str) -> Path:
        """Resolve and validate a path.

        This is an alias for validate_path() for backward compatibility.
        """
        ...




"""File I/O operations for template processing.

This module provides functionality for file operations related to template processing:
1. Reading files with encoding detection and caching
2. Extracting metadata from files and templates
3. Managing file content caching and eviction
4. Progress tracking for file operations

Key Components:
    - read_file: Main function for reading files
    - extract_metadata: Extract metadata from files
    - extract_template_metadata: Extract metadata from templates
    - Cache management for file content

Examples:
    Basic file reading:
    >>> file_info = read_file('example.txt')
    >>> print(file_info.name)  # 'example.txt'
    >>> print(file_info.content)  # File contents
    >>> print(file_info.encoding)  # Detected encoding

    Lazy loading:
    >>> file_info = read_file('large_file.txt', lazy=True)
    >>> # Content not loaded yet
    >>> print(file_info.content)  # Now content is loaded
    >>> print(file_info.size)  # File size in bytes

    Metadata extraction:
    >>> metadata = extract_metadata(file_info)
    >>> print(metadata['size'])  # File size
    >>> print(metadata['encoding'])  # File encoding
    >>> print(metadata['mtime'])  # Last modified time

    Template metadata:
    >>> template = "Hello {{ name }}, files: {% for f in files %}{{ f.name }}{% endfor %}"
    >>> metadata = extract_template_metadata(template)
    >>> print(metadata['variables'])  # ['name', 'files']
    >>> print(metadata['has_loops'])  # True
    >>> print(metadata['filters'])  # []

    Cache management:
    >>> # Files are automatically cached
    >>> file_info1 = read_file('example.txt')
    >>> file_info2 = read_file('example.txt')  # Uses cached content
    >>> # Cache is invalidated if file changes
    >>> # Large files evicted from cache based on size

Notes:
    - Automatically detects file encoding
    - Caches file content for performance
    - Tracks file modifications
    - Provides progress updates for large files
    - Handles various error conditions gracefully
"""

import logging
import os
import threading
from typing import Any, Dict, Optional

from jinja2 import Environment

from .file_utils import FileInfo
from .progress import ProgressContext
from .security import SecurityManager

logger = logging.getLogger(__name__)

# Cache settings
MAX_CACHE_SIZE = 50 * 1024 * 1024  # 50MB total cache size
_cache_lock = threading.Lock()
_file_mtimes: Dict[str, float] = {}
_file_cache: Dict[str, str] = {}
_file_encodings: Dict[str, str] = {}
_file_hashes: Dict[str, str] = {}
_cache_size: int = 0


def read_file(
    file_path: str,
    security_manager: Optional["SecurityManager"] = None,
    encoding: Optional[str] = None,
    progress_enabled: bool = True,
    chunk_size: int = 1024 * 1024,  # 1MB chunks
) -> FileInfo:
    """Read a file and return its contents.

    Args:
        file_path: Path to the file to read
        security_manager: Security manager for path validation. If None, creates one with current directory
        encoding: Optional encoding to use for reading the file
        progress_enabled: Whether to show progress updates
        chunk_size: Size of chunks to read at a time

    Returns:
        FileInfo: Object containing file content and metadata

    Raises:
        ValueError: If file cannot be read or found
    """
    logger = logging.getLogger(__name__)
    logger.debug("\n=== read_file called ===")
    logger.debug("Args: file_path=%s, encoding=%s", file_path, encoding)

    # Create security manager if not provided
    if security_manager is None:
        security_manager = SecurityManager(base_dir=os.getcwd())

    with ProgressContext(
        "Reading file", show_progress=progress_enabled
    ) as progress:
        try:
            if progress:
                progress.update(1)  # Update progress for setup

            # Get absolute path and check file exists
            abs_path = os.path.abspath(file_path)
            logger.debug("Absolute path: %s", abs_path)
            if not os.path.isfile(abs_path):
                raise ValueError(f"File not found: {file_path}")

            # Check if file is in cache and up to date
            mtime = os.path.getmtime(abs_path)
            with _cache_lock:
                logger.debug(
                    "Cache state - mtimes: %s, cache: %s",
                    _file_mtimes,
                    _file_cache,
                )
                if (
                    abs_path in _file_mtimes
                    and _file_mtimes[abs_path] == mtime
                ):
                    logger.debug("Cache hit for %s", abs_path)
                    if progress:
                        progress.update(1)  # Update progress for cache hit
                    # Create FileInfo and update from cache
                    file_info = FileInfo.from_path(
                        path=file_path, security_manager=security_manager
                    )
                    file_info.update_cache(
                        content=_file_cache[abs_path],
                        encoding=_file_encodings.get(abs_path),
                        hash_value=_file_hashes.get(abs_path),
                    )
                    return file_info

            # Create new FileInfo - content will be loaded immediately
            file_info = FileInfo.from_path(
                path=file_path, security_manager=security_manager
            )

            # Update cache with loaded content
            with _cache_lock:
                logger.debug("Updating cache for %s", abs_path)
                _file_mtimes[abs_path] = mtime
                _file_cache[abs_path] = file_info.content
                if file_info.encoding is not None:
                    _file_encodings[abs_path] = file_info.encoding
                if file_info.hash is not None:
                    _file_hashes[abs_path] = file_info.hash

                global _cache_size
                _cache_size = sum(
                    len(content) for content in _file_cache.values()
                )
                logger.debug("Cache updated - size: %d", _cache_size)

                # Remove old entries if cache is too large
                while _cache_size > MAX_CACHE_SIZE:
                    oldest = min(_file_mtimes.items(), key=lambda x: x[1])
                    old_path = oldest[0]
                    if old_path in _file_cache:
                        _cache_size -= len(_file_cache[old_path])
                        del _file_cache[old_path]
                        del _file_encodings[old_path]
                        del _file_hashes[old_path]
                    del _file_mtimes[old_path]
                    logger.debug("Removed old cache entry: %s", old_path)

            if progress:
                progress.update(1)  # Update progress for successful read

            return file_info

        except Exception as e:
            logger.error("Error reading file: %s", str(e))
            if isinstance(e, ValueError):
                raise
            raise ValueError(f"Failed to read file: {str(e)}")


def extract_metadata(file_info: FileInfo) -> Dict[str, Any]:
    """Extract metadata from a FileInfo object.

    This function respects lazy loading - it will not force content loading
    if the content hasn't been loaded yet.
    """
    metadata: Dict[str, Any] = {
        "name": os.path.basename(file_info.path),
        "path": file_info.path,
        "abs_path": os.path.realpath(file_info.path),
        "mtime": file_info.mtime,
    }

    # Only include content-related fields if content has been explicitly accessed
    if (
        hasattr(file_info, "_FileInfo__content")
        and file_info.content is not None
    ):
        metadata["content"] = file_info.content
        metadata["size"] = file_info.size

    return metadata


def extract_template_metadata(
    template_str: str,
    context: Dict[str, Any],
    jinja_env: Optional[Environment] = None,
    progress_enabled: bool = True,
) -> Dict[str, Dict[str, Any]]:
    """Extract metadata about a template string."""
    metadata: Dict[str, Dict[str, Any]] = {
        "template": {"is_file": True, "path": template_str},
        "context": {
            "variables": sorted(context.keys()),
            "dict_vars": [],
            "list_vars": [],
            "file_info_vars": [],
            "other_vars": [],
        },
    }

    with ProgressContext(
        description="Analyzing template", show_progress=progress_enabled
    ) as progress:
        # Categorize variables by type
        for key, value in context.items():
            if isinstance(value, dict):
                metadata["context"]["dict_vars"].append(key)
            elif isinstance(value, list):
                metadata["context"]["list_vars"].append(key)
            elif isinstance(value, FileInfo):
                metadata["context"]["file_info_vars"].append(key)
            else:
                metadata["context"]["other_vars"].append(key)

        # Sort lists for consistent output
        for key in ["dict_vars", "list_vars", "file_info_vars", "other_vars"]:
            metadata["context"][key].sort()

        if progress.enabled:
            progress.current = 1

        return metadata




"""Template validation using Jinja2.

This module provides functionality for validating Jinja2 templates, ensuring that:
1. All variables used in templates exist in the context
2. Nested attribute/key access is valid according to schema
3. Loop variables and filters are used correctly

Key Components:
    - SafeUndefined: Custom undefined type for validation
    - validate_template_placeholders: Main validation function

Examples:
    Basic template validation:
    >>> template = "Hello {{ name }}, your score is {{ results.score }}"
    >>> template_context = {'name': 'value', 'results': {'score': 100}}
    >>> validate_template_placeholders(template, template_context)  # OK
    >>>
    >>> # Missing variable raises ValueError:
    >>> template = "Hello {{ missing }}"
    >>> validate_template_placeholders(template, {'name'})  # Raises ValueError

    Nested attribute validation:
    >>> template = '''
    ... Debug mode: {{ config.debug }}
    ... Settings: {{ config.settings.mode }}
    ... Invalid: {{ config.invalid }}
    ... '''
    >>> validate_template_placeholders(template, {'config'})  # Raises ValueError

    Loop variable validation:
    >>> template = '''
    ... {% for item in items %}
    ...   - {{ item.name }}: {{ item.value }}
    ...   Invalid: {{ item.invalid }}
    ... {% endfor %}
    ... '''
    >>> validate_template_placeholders(template, {'items'})  # Raises ValueError

    Filter validation:
    >>> template = '''
    ... {{ code | format_code }}  {# OK - valid filter #}
    ... {{ data | invalid_filter }}  {# Raises ValueError #}
    ... '''
    >>> validate_template_placeholders(template, {'code', 'data'})

Notes:
    - Uses Jinja2's meta API to find undeclared variables
    - Supports custom filters through safe wrappers
    - Provides detailed error messages for validation failures
"""

import logging
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Optional,
    Set,
    TypeVar,
    Union,
    cast,
)

import jinja2
from jinja2 import Environment, meta
from jinja2.nodes import For, Name, Node

from . import template_filters
from .template_extensions import CommentExtension
from .template_schema import (
    DictProxy,
    FileInfoProxy,
    ValidationProxy,
    create_validation_context,
)
from .template_env import create_jinja_env

T = TypeVar("T")
FilterFunc = Callable[..., Any]
FilterWrapper = Callable[[Any, Any, Any], Optional[Union[Any, str, List[Any]]]]


class SafeUndefined(jinja2.StrictUndefined):
    """A strict Undefined class that allows any attribute access during validation."""

    def __getattr__(self, name: str) -> Any:
        # Allow any attribute access during validation
        return self

    def __getitem__(self, key: Any) -> Any:
        # Allow any key access during validation
        return self


def safe_filter(func: FilterFunc) -> FilterWrapper:
    """Wrap a filter function to handle None and proxy values safely."""

    def wrapper(
        value: Any, *args: Any, **kwargs: Any
    ) -> Optional[Union[Any, str, List[Any]]]:
        if value is None:
            return None
        if isinstance(value, (ValidationProxy, FileInfoProxy, DictProxy)):
            # For validation, just return an empty result of the appropriate type
            if func.__name__ in (
                "extract_field",
                "frequency",
                "aggregate",
                "pivot_table",
                "summarize",
            ):
                return []
            elif func.__name__ in ("dict_to_table", "list_to_table"):
                return ""
            return value
        return func(value, *args, **kwargs)

    return wrapper


def find_loop_vars(nodes: List[Node]) -> Set[str]:
    """Find variables used in loop constructs."""
    loop_vars = set()
    for node in nodes:
        if isinstance(node, For):
            target = node.target
            if isinstance(target, Name):
                loop_vars.add(target.name)
            elif hasattr(target, "items"):
                items = cast(List[Name], target.items)
                for item in items:
                    loop_vars.add(item.name)
        if hasattr(node, "body"):
            loop_vars.update(find_loop_vars(cast(List[Node], node.body)))
        if hasattr(node, "else_"):
            loop_vars.update(find_loop_vars(cast(List[Node], node.else_)))
    return loop_vars


def validate_template_placeholders(
    task_template: str,
    template_context: Dict[str, Any],
    env: Optional[Environment] = None,
) -> None:
    """Validate that a task template only uses available variables.

    This function checks that all variables used in the template exist in the
    template context and that any nested attribute access is valid.

    Args:
        task_template: The template string to validate
        template_context: Dictionary mapping variable names to their values
        env: Optional Jinja2 Environment to use for validation

    Raises:
        ValueError: If the template uses undefined variables or has invalid syntax
    """
    logger = logging.getLogger(__name__)

    logger.debug("=== validate_template_placeholders called ===")
    logger.debug(
        "Args: task_template=%s, template_context=%s",
        task_template,
        {k: type(v).__name__ for k, v in template_context.items()},
    )

    try:
        # 1) Create Jinja2 environment with meta extension and safe undefined
        if env is None:
            env = create_jinja_env(validation_mode=True)

        # Register custom filters with None-safe wrappers
        env.filters.update(
            {
                "format_code": safe_filter(template_filters.format_code),
                "strip_comments": safe_filter(template_filters.strip_comments),
                "extract_field": safe_filter(template_filters.extract_field),
                "frequency": safe_filter(template_filters.frequency),
                "aggregate": safe_filter(template_filters.aggregate),
                "pivot_table": safe_filter(template_filters.pivot_table),
                "summarize": safe_filter(template_filters.summarize),
                "dict_to_table": safe_filter(template_filters.dict_to_table),
                "list_to_table": safe_filter(template_filters.list_to_table),
            }
        )

        # Add built-in Jinja2 functions and filters
        builtin_vars = {
            # Core functions
            "range",
            "dict",
            "lipsum",
            "cycler",
            "joiner",
            "namespace",
            "loop",
            "super",
            "self",
            "varargs",
            "kwargs",
            "items",
            # String filters
            "upper",
            "lower",
            "title",
            "capitalize",
            "trim",
            "strip",
            "lstrip",
            "rstrip",
            "center",
            "ljust",
            "rjust",
            "wordcount",
            "truncate",
            "striptags",
            # List filters
            "first",
            "last",
            "length",
            "max",
            "min",
            "sum",
            "sort",
            "unique",
            "reverse",
            "reject",
            "select",
            "map",
            "join",
            "count",
            # Type conversion
            "abs",
            "round",
            "int",
            "float",
            "string",
            "list",
            "bool",
            "batch",
            "slice",
            "attr",
            # Common filters
            "default",
            "replace",
            "safe",
            "urlencode",
            "indent",
            "format",
            "escape",
            "e",
            "nl2br",
            "urlize",
            # Dictionary operations
            "items",
            "keys",
            "values",
            "dictsort",
            # Math operations
            "round",
            "ceil",
            "floor",
            "abs",
            "max",
            "min",
            # Date/time
            "now",
            "strftime",
            "strptime",
            "datetimeformat",
        }
        builtin_vars.update(env.filters.keys())
        builtin_vars.update(env.globals.keys())

        # 2) Parse template and find variables
        ast = env.parse(task_template)
        available_vars = set(template_context.keys())
        logger.debug("Available variables: %s", available_vars)

        # Find loop variables
        loop_vars = find_loop_vars(ast.body)
        logger.debug("Found loop variables: %s", loop_vars)

        # Find all undeclared variables using jinja2.meta
        found_vars = meta.find_undeclared_variables(ast)
        logger.debug("Found undeclared variables: %s", found_vars)

        # Check for missing variables
        missing = {
            name
            for name in found_vars
            if name not in available_vars
            and name not in builtin_vars
            and name not in loop_vars
            and name
            != "stdin"  # Special case for stdin which may be added dynamically
        }

        if missing:
            raise ValueError(
                f"Task template uses undefined variables: {', '.join(sorted(missing))}. "
                f"Available variables: {', '.join(sorted(available_vars))}"
            )

        logger.debug(
            "Before create_validation_context - available_vars type: %s, value: %s",
            type(available_vars),
            available_vars,
        )
        logger.debug(
            "Before create_validation_context - template_context type: %s, value: %s",
            type(template_context),
            template_context,
        )

        # 3) Create validation context with proxy objects
        logger.debug(
            "Creating validation context with template_context: %s",
            template_context,
        )
        validation_context = create_validation_context(template_context)

        # 4) Try to render with validation context
        try:
            env.from_string(task_template).render(validation_context)
        except jinja2.UndefinedError as e:
            # Convert Jinja2 undefined errors to our own ValueError
            raise ValueError(str(e))
        except Exception as e:
            logger.error(
                "Unexpected error during template validation: %s", str(e)
            )
            raise

    except jinja2.TemplateSyntaxError as e:
        # Convert Jinja2 syntax errors to our own ValueError
        raise ValueError(f"Invalid task template syntax: {str(e)}")
    except Exception as e:
        logger.error("Unexpected error during template validation: %s", str(e))
        raise




"""Template utilities for the CLI.

This module serves as the main entry point for template processing functionality.
It re-exports the public APIs from specialized modules:

- template_schema: Schema validation and proxy objects
- template_validation: Template validation using Jinja2
- template_rendering: Template rendering with Jinja2
- template_io: File I/O operations and metadata extraction
"""

from typing import Any, Dict, List, Optional, Set

import jsonschema
from jinja2 import Environment, meta
from jinja2.nodes import Node

from .errors import (
    CLIError,
    SchemaError,
    SchemaValidationError,
    SystemPromptError,
    TaskTemplateError,
    TaskTemplateSyntaxError,
    TaskTemplateVariableError,
)
from .file_utils import FileInfo
from .template_io import extract_metadata, extract_template_metadata, read_file
from .template_rendering import DotDict, render_template
from .template_schema import (
    DictProxy,
    FileInfoProxy,
    ValidationProxy,
    create_validation_context,
)
from .template_validation import SafeUndefined, validate_template_placeholders


# Custom error classes
class TemplateMetadataError(TaskTemplateError):
    """Raised when there are issues extracting template metadata."""

    pass


def validate_json_schema(schema: Dict[str, Any]) -> None:
    """Validate that a dictionary follows JSON Schema structure.

    This function checks that the provided dictionary is a valid JSON Schema,
    following the JSON Schema specification.

    Args:
        schema: Dictionary to validate as a JSON Schema

    Raises:
        SchemaValidationError: If the schema is invalid
    """
    try:
        # Get the validator class for the schema
        validator_cls = jsonschema.validators.validator_for(schema)

        # Check schema itself is valid
        validator_cls.check_schema(schema)

        # Create validator instance
        validator_cls(schema)
    except jsonschema.exceptions.SchemaError as e:
        raise SchemaValidationError(f"Invalid JSON Schema: {e}")
    except Exception as e:
        raise SchemaValidationError(f"Schema validation error: {e}")


def validate_response(
    response: Dict[str, Any], schema: Dict[str, Any]
) -> None:
    """Validate that a response dictionary matches a JSON Schema.

    This function validates that the response dictionary conforms to the provided
    JSON Schema specification.

    Args:
        response: Dictionary to validate
        schema: JSON Schema to validate against

    Raises:
        ValueError: If the response does not match the schema
    """
    try:
        # Create a validator for the schema
        validator = jsonschema.validators.validator_for(schema)(schema)

        # Validate the response
        validator.validate(response)
    except jsonschema.exceptions.ValidationError as e:
        raise ValueError(f"Response validation failed: {e}")
    except Exception as e:
        raise ValueError(f"Response validation error: {e}")


def find_all_template_variables(
    template: str, env: Optional[Environment] = None
) -> Set[str]:
    """Find all variables used in a template and its dependencies.

    This function recursively parses the template and any included/imported/extended
    templates to find all variables that might be used.

    Args:
        template: The template string to parse
        env: Optional Jinja2 environment. If not provided, a new one will be created.

    Returns:
        Set of all variable names used in the template and its dependencies

    Raises:
        jinja2.TemplateSyntaxError: If the template has invalid syntax
    """
    if env is None:
        env = Environment(undefined=SafeUndefined)

    # Parse template
    ast = env.parse(template)

    # Find all variables in this template
    variables = meta.find_undeclared_variables(ast)

    # Filter out built-in functions and filters
    builtin_vars = {
        # Core functions
        "range",
        "dict",
        "lipsum",
        "cycler",
        "joiner",
        "namespace",
        "loop",
        "super",
        "self",
        "varargs",
        "kwargs",
        "items",
        # String filters
        "upper",
        "lower",
        "title",
        "capitalize",
        "trim",
        "strip",
        "lstrip",
        "rstrip",
        "center",
        "ljust",
        "rjust",
        "wordcount",
        "truncate",
        "striptags",
        # List filters
        "first",
        "last",
        "length",
        "max",
        "min",
        "sum",
        "sort",
        "unique",
        "reverse",
        "reject",
        "select",
        "map",
        "join",
        "count",
        # Type conversion
        "abs",
        "round",
        "int",
        "float",
        "string",
        "list",
        "bool",
        "batch",
        "slice",
        "attr",
        # Common filters
        "default",
        "replace",
        "safe",
        "urlencode",
        "indent",
        "format",
        "escape",
        "e",
        "nl2br",
        "urlize",
        # Dictionary operations
        "items",
        "keys",
        "values",
        "dictsort",
        # Math operations
        "round",
        "ceil",
        "floor",
        "abs",
        "max",
        "min",
        # Date/time
        "now",
        "strftime",
        "strptime",
        "datetimeformat",
    }

    variables = variables - builtin_vars

    # Find template dependencies (include, extends, import)
    def visit_nodes(nodes: List[Node]) -> None:
        """Visit AST nodes recursively to find template dependencies.

        Args:
            nodes: List of AST nodes to visit
        """
        for node in nodes:
            if node.__class__.__name__ in (
                "Include",
                "Extends",
                "Import",
                "FromImport",
            ):
                # Get the template name
                if hasattr(node, "template"):
                    template_name = node.template.value
                    try:
                        # Load and parse the referenced template
                        if env.loader is not None:
                            included_template = env.loader.get_source(
                                env, template_name
                            )[0]
                            # Recursively find variables in the included template
                            variables.update(
                                find_all_template_variables(
                                    included_template, env
                                )
                            )
                    except Exception:
                        # Skip if template can't be loaded - it will be caught during rendering
                        pass

            # Recursively visit child nodes
            if hasattr(node, "body"):
                visit_nodes(node.body)
            if hasattr(node, "else_"):
                visit_nodes(node.else_)

    visit_nodes(ast.body)
    return variables


__all__ = [
    # Schema types and validation
    "ValidationProxy",
    "FileInfoProxy",
    "DictProxy",
    "create_validation_context",
    "validate_json_schema",
    "validate_response",
    # Template validation
    "SafeUndefined",
    "validate_template_placeholders",
    "find_all_template_variables",
    # Template rendering
    "render_template",
    "DotDict",
    # File I/O
    "read_file",
    "extract_metadata",
    "extract_template_metadata",
    # File info
    "FileInfo",
    # Error classes
    "CLIError",
    "TaskTemplateError",
    "TaskTemplateSyntaxError",
    "TaskTemplateVariableError",
    "SchemaError",
    "SchemaValidationError",
    "SystemPromptError",
    "TemplateMetadataError",
]




"""Custom Jinja2 extensions for enhanced template functionality.

This module provides extensions that modify Jinja2's default behavior:
- CommentExtension: Ignores variables inside comment blocks during validation and rendering
"""

import logging
from typing import Any, List, Optional

from jinja2 import nodes
from jinja2.ext import Extension
from jinja2.lexer import Token, TokenStream
from jinja2.parser import Parser


class CommentExtension(Extension):
    """Extension that ignores variables inside comment blocks.
    
    This extension ensures that:
    1. Contents of comment blocks are completely ignored during parsing
    2. Variables inside comments are not validated or processed
    3. Comments are stripped from the output
    """

    tags = {"comment"}

    def parse(self, parser: Parser) -> nodes.Node:
        """Parse a comment block, ignoring its contents.
        
        Args:
            parser: The Jinja2 parser instance
            
        Returns:
            An empty node since comments are ignored
            
        Raises:
            TemplateSyntaxError: If the comment block is not properly closed
        """
        logger = logging.getLogger(__name__)
        
        # Get the line number for error reporting
        lineno = parser.stream.current.lineno
        
        # Skip the opening comment tag
        next(parser.stream)
        
        # Skip until we find {% endcomment %}
        while not parser.stream.current.test('name:endcomment'):
            if parser.stream.current.type == 'eof':
                raise parser.fail('Unclosed comment block', lineno)
            next(parser.stream)
            
        # Skip the endcomment tag
        next(parser.stream)
        
        # Return an empty string node
        return nodes.Output([nodes.TemplateData("")]).set_lineno(lineno) 



"""Security management for file access.

This module provides security checks for file access, including:
- Base directory restrictions
- Allowed directory validation
- Path traversal prevention
- Temporary directory handling
"""

import logging
import os
import tempfile
from pathlib import Path
from typing import List, Optional, Set

from .errors import DirectoryNotFoundError, PathSecurityError
from .security_types import SecurityManagerProtocol


def is_temp_file(path: str) -> bool:
    """Check if a file is in a temporary directory.

    Args:
        path: Path to check (will be converted to absolute path)

    Returns:
        True if the path is in a temporary directory, False otherwise

    Note:
        This function handles platform-specific path normalization, including symlinks
        (e.g., on macOS where /var is symlinked to /private/var).
    """
    # Normalize the input path (resolve symlinks)
    abs_path = os.path.realpath(path)

    # Get all potential temp directories and normalize them
    temp_dirs = set()
    # System temp dir (platform independent)
    temp_dirs.add(os.path.realpath(tempfile.gettempdir()))

    # Common Unix/Linux/macOS temp locations
    unix_temp_dirs = ["/tmp", "/var/tmp", "/var/folders"]
    for temp_dir in unix_temp_dirs:
        if os.path.exists(temp_dir):
            temp_dirs.add(os.path.realpath(temp_dir))

    # Windows temp locations (if on Windows)
    if os.name == "nt":
        if "TEMP" in os.environ:
            temp_dirs.add(os.path.realpath(os.environ["TEMP"]))
        if "TMP" in os.environ:
            temp_dirs.add(os.path.realpath(os.environ["TMP"]))

    # Check if file is in any temp directory using normalized paths
    abs_path_parts = os.path.normpath(abs_path).split(os.sep)
    for temp_dir in temp_dirs:
        temp_dir_parts = os.path.normpath(temp_dir).split(os.sep)
        # Check if the path starts with the temp directory components
        if len(abs_path_parts) >= len(temp_dir_parts) and all(
            a == b
            for a, b in zip(
                abs_path_parts[: len(temp_dir_parts)], temp_dir_parts
            )
        ):
            return True

    return False


class SecurityManager(SecurityManagerProtocol):
    """Manages security for file access.

    Validates all file access against a base directory and optional
    allowed directories. Prevents unauthorized access and directory
    traversal attacks.

    The security model is based on:
    1. A base directory that serves as the root for all file operations
    2. A set of explicitly allowed directories that can be accessed outside the base directory
    3. Special handling for temporary directories that are always allowed

    All paths are normalized using realpath() to handle symlinks consistently across platforms.
    """

    def __init__(
        self,
        base_dir: Optional[str] = None,
        allowed_dirs: Optional[List[str]] = None,
    ):
        """Initialize security manager.

        Args:
            base_dir: Base directory for file access. Defaults to current working directory.
            allowed_dirs: Optional list of additional allowed directories

        All paths are normalized using realpath to handle symlinks
        and relative paths consistently across platforms.
        """
        self._base_dir = Path(os.path.realpath(base_dir or os.getcwd()))
        self._allowed_dirs: Set[Path] = set()
        if allowed_dirs:
            for directory in allowed_dirs:
                self.add_allowed_dir(directory)

    @property
    def base_dir(self) -> Path:
        """Get the base directory."""
        return self._base_dir

    @property
    def allowed_dirs(self) -> List[Path]:
        """Get the list of allowed directories."""
        return sorted(self._allowed_dirs)  # Sort for consistent ordering

    def add_allowed_dir(self, directory: str) -> None:
        """Add a directory to the set of allowed directories.

        Args:
            directory: Directory to allow access to

        Raises:
            DirectoryNotFoundError: If directory does not exist
        """
        real_path = Path(os.path.realpath(directory))
        if not real_path.exists():
            raise DirectoryNotFoundError(f"Directory not found: {directory}")
        if not real_path.is_dir():
            raise DirectoryNotFoundError(
                f"Path is not a directory: {directory}"
            )
        self._allowed_dirs.add(real_path)

    def add_allowed_dirs_from_file(self, file_path: str) -> None:
        """Add allowed directories from a file.

        Args:
            file_path: Path to file containing allowed directories (one per line)

        Raises:
            PathSecurityError: If file_path is outside allowed directories
            FileNotFoundError: If file does not exist
            ValueError: If file contains invalid directories
        """
        real_path = Path(os.path.realpath(file_path))

        # First validate the file path itself
        try:
            self.validate_path(
                str(real_path), purpose="read allowed directories"
            )
        except PathSecurityError:
            raise PathSecurityError.from_expanded_paths(
                original_path=file_path,
                expanded_path=str(real_path),
                error_logged=True,
                base_dir=str(self._base_dir),
                allowed_dirs=[str(d) for d in self._allowed_dirs],
            )

        if not real_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        with open(real_path) as f:
            for line in f:
                directory = line.strip()
                if directory and not directory.startswith("#"):
                    self.add_allowed_dir(directory)

    def is_path_allowed(self, path: str) -> bool:
        """Check if a path is allowed.

        A path is allowed if it is:
        1. Under the normalized base directory
        2. Under any normalized allowed directory

        The path must also exist.

        Args:
            path: Path to check

        Returns:
            bool: True if path exists and is allowed, False otherwise
        """
        try:
            real_path = Path(os.path.realpath(path))
            if not real_path.exists():
                return False
        except (ValueError, OSError):
            return False

        # Check if path is in base directory
        try:
            if real_path.is_relative_to(self._base_dir):
                return True
        except ValueError:
            pass

        # Check if path is in allowed directories
        for allowed_dir in self._allowed_dirs:
            try:
                if real_path.is_relative_to(allowed_dir):
                    return True
            except ValueError:
                continue

        return False

    def validate_path(self, path: str, purpose: str = "access") -> Path:
        """Validate and normalize a path.

        Args:
            path: Path to validate
            purpose: Description of intended access (for error messages)

        Returns:
            Path: Normalized path if valid

        Raises:
            PathSecurityError: If path is not allowed
        """
        try:
            real_path = Path(os.path.realpath(path))
        except (ValueError, OSError) as e:
            logger = logging.getLogger("ostruct")
            logger.error("Invalid path format: %s", e)
            raise PathSecurityError(
                f"Invalid path format: {e}", error_logged=True
            )

        if not self.is_path_allowed(str(real_path)):
            logger = logging.getLogger("ostruct")
            logger.error(
                "Access denied: %s is outside base directory and not in allowed directories",
                path,
            )
            raise PathSecurityError.from_expanded_paths(
                original_path=path,
                expanded_path=str(real_path),
                base_dir=str(self._base_dir),
                allowed_dirs=[str(d) for d in self._allowed_dirs],
                error_logged=True,
            )

        return real_path

    def is_allowed_file(self, path: str) -> bool:
        """Check if file access is allowed.

        Args:
            path: Path to check

        Returns:
            bool: True if file exists and is allowed
        """
        try:
            real_path = Path(os.path.realpath(path))
            return self.is_path_allowed(str(real_path)) and real_path.is_file()
        except (ValueError, OSError):
            return False

    def is_allowed_path(self, path_str: str) -> bool:
        """Check if string path is allowed.

        Args:
            path_str: Path string to check

        Returns:
            bool: True if path is allowed
        """
        try:
            return self.is_path_allowed(path_str)
        except (ValueError, OSError):
            return False

    def resolve_path(self, path: str) -> Path:
        """Resolve and validate a path.

        This is an alias for validate_path() for backward compatibility.

        Args:
            path: Path to resolve and validate

        Returns:
            Path: Normalized path if valid

        Raises:
            PathSecurityError: If path is not allowed
        """
        return self.validate_path(path)




"""Template rendering with Jinja2.

This module provides functionality for rendering Jinja2 templates with support for:
1. Custom filters and functions
2. Dot notation access for dictionaries
3. Error handling and reporting

Key Components:
    - render_template: Main rendering function
    - DotDict: Dictionary wrapper for dot notation access
    - Custom filters for code formatting and data manipulation

Examples:
    Basic template rendering:
    >>> template = "Hello {{ name }}!"
    >>> context = {'name': 'World'}
    >>> result = render_template(template, context)
    >>> print(result)
    Hello World!

    Dictionary access with dot notation:
    >>> template = '''
    ... Debug: {{ config.debug }}
    ... Mode: {{ config.settings.mode }}
    ... '''
    >>> config = {
    ...     'debug': True,
    ...     'settings': {'mode': 'test'}
    ... }
    >>> result = render_template(template, {'config': config})
    >>> print(result)
    Debug: True
    Mode: test

    Using custom filters:
    >>> template = '''
    ... {{ code | format_code('python') }}
    ... {{ data | dict_to_table }}
    ... '''
    >>> context = {
    ...     'code': 'def hello(): print("Hello")',
    ...     'data': {'name': 'test', 'value': 42}
    ... }
    >>> result = render_template(template, context)

    File content rendering:
    >>> template = "Content: {{ file.content }}"
    >>> context = {'file': FileInfo('test.txt')}
    >>> result = render_template(template, context)

Notes:
    - All dictionaries are wrapped in DotDict for dot notation access
    - Custom filters are registered automatically
    - Provides detailed error messages for rendering failures
"""

import datetime
import logging
import os
from typing import Any, Dict, List, Optional, Union

import jinja2
from jinja2 import Environment

from . import template_filters
from .file_utils import FileInfo
from .template_extensions import CommentExtension
from .template_schema import DotDict, StdinProxy
from .template_env import create_jinja_env

__all__ = ["render_template", "DotDict"]

logger = logging.getLogger(__name__)

# Type alias for values that can appear in the template context
TemplateContextValue = Union[
    DotDict,
    StdinProxy,
    FileInfo,
    List[Union[FileInfo, Any]],  # For file lists
    str,
    int,
    float,
    bool,
    None,
]

def render_template(
    template_str: str,
    context: Dict[str, Any],
    jinja_env: Optional[Environment] = None,
    progress_enabled: bool = True,
) -> str:
    """Render a task template with the given context.

    Args:
        template_str: Task template string or path to task template file
        context: Task template variables
        jinja_env: Optional Jinja2 environment to use
        progress_enabled: Whether to show progress indicators

    Returns:
        Rendered task template string

    Raises:
        ValueError: If task template cannot be loaded or rendered. The original error
                  will be chained using `from` for proper error context.
    """
    from .progress import (  # Import here to avoid circular dependency
        ProgressContext,
    )

    with ProgressContext(
        "Rendering task template", show_progress=progress_enabled
    ) as progress:
        try:
            if progress:
                progress.update(1)  # Update progress for setup

            if jinja_env is None:
                jinja_env = create_jinja_env(loader=jinja2.FileSystemLoader("."))

            # Wrap JSON variables in DotDict and handle special cases
            wrapped_context: Dict[str, TemplateContextValue] = {}
            for key, value in context.items():
                if isinstance(value, dict):
                    wrapped_context[key] = DotDict(value)
                else:
                    wrapped_context[key] = value

            # Add stdin only if not already in context
            if "stdin" not in wrapped_context:
                wrapped_context["stdin"] = StdinProxy()

            # Load file content for FileInfo objects
            for key, value in context.items():
                if isinstance(value, FileInfo):
                    # Access content property to trigger loading
                    _ = value.content
                elif (
                    isinstance(value, list)
                    and value
                    and isinstance(value[0], FileInfo)
                ):
                    for file_info in value:
                        # Access content property to trigger loading
                        _ = file_info.content

            if progress:
                progress.update(1)  # Update progress for template creation

            # Create template from string or file
            template: Optional[jinja2.Template] = None
            if template_str.endswith((".j2", ".jinja2", ".md")):
                if not os.path.isfile(template_str):
                    raise ValueError(
                        f"Task template file not found: {template_str}"
                    )
                try:
                    template = jinja_env.get_template(template_str)
                except jinja2.TemplateNotFound as e:
                    raise ValueError(
                        f"Task template file not found: {e.name}"
                    ) from e
            else:
                try:
                    template = jinja_env.from_string(template_str)
                except jinja2.TemplateSyntaxError as e:
                    raise ValueError(
                        f"Task template syntax error: {str(e)}"
                    ) from e

            if template is None:
                raise ValueError("Failed to create task template")
            assert template is not None  # Help mypy understand control flow

            # Add template globals
            template.globals["template_name"] = getattr(
                template, "name", "<string>"
            )
            template.globals["template_path"] = getattr(
                template, "filename", None
            )

            try:
                # Attempt to render the template
                result = template.render(**wrapped_context)
                if progress:
                    progress.update(1)  # Update progress for successful render
                return result
            except (jinja2.TemplateError, Exception) as e:
                # Convert all errors to ValueError with proper context
                raise ValueError(f"Template rendering failed: {str(e)}") from e

        except ValueError as e:
            # Re-raise with original context
            raise e




"""CLI package for structured OpenAI API calls."""

from openai import AsyncOpenAI

from .cli import (
    ExitCode,
    ProgressContext,
    create_dynamic_model,
    estimate_tokens_for_chat,
    get_context_window_limit,
    get_default_token_limit,
    main,
    supports_structured_output,
    validate_template_placeholders,
    validate_token_limits,
)
from .template_utils import read_file, render_template

__all__ = [
    "ExitCode",
    "main",
    "create_dynamic_model",
    "validate_template_placeholders",
    "estimate_tokens_for_chat",
    "get_context_window_limit",
    "get_default_token_limit",
    "validate_token_limits",
    "supports_structured_output",
    "ProgressContext",
    "render_template",
    "AsyncOpenAI",
    "read_file",
]




"""Template filters for Jinja2 environment."""

import itertools
import json
import logging
import re
import textwrap
from collections import Counter
from typing import Any, Dict, List, Optional, Sequence, TypeVar, Union
import datetime

import tiktoken
from pygments import highlight
from pygments.formatters import HtmlFormatter, NullFormatter, TerminalFormatter
from pygments.lexers import TextLexer, get_lexer_by_name, guess_lexer
from pygments.util import ClassNotFound
from jinja2 import Environment

logger = logging.getLogger(__name__)

T = TypeVar("T")


def extract_keywords(text: str) -> List[str]:
    """Extract keywords from text."""
    return text.split()


def word_count(text: str) -> int:
    """Count words in text."""
    return len(text.split())


def char_count(text: str) -> int:
    """Count characters in text."""
    return len(text)


def to_json(obj: Any) -> str:
    """Convert object to JSON string."""
    return json.dumps(obj, indent=2)


def from_json(text: str) -> Any:
    """Parse JSON string to object."""
    return json.loads(text)


def remove_comments(text: str) -> str:
    """Remove comments from text."""
    return re.sub(r"#.*$|//.*$|/\*[\s\S]*?\*/", "", text, flags=re.MULTILINE)


def wrap_text(text: str, width: int = 80) -> str:
    """Wrap text to specified width."""
    return textwrap.fill(text, width)


def indent_text(text: str, width: int = 4) -> str:
    """Indent text by specified width."""
    return textwrap.indent(text, " " * width)


def dedent_text(text: str) -> str:
    """Remove common leading whitespace from text."""
    return textwrap.dedent(text)


def normalize_text(text: str) -> str:
    """Normalize whitespace in text."""
    return " ".join(text.split())


def strip_markdown(text: str) -> str:
    """Remove markdown formatting characters."""
    return re.sub(r"[#*`_~]", "", text)


def format_table(headers: Sequence[Any], rows: Sequence[Sequence[Any]]) -> str:
    """Format data as a markdown table."""
    return (
        f"| {' | '.join(str(h) for h in headers)} |\n"
        f"| {' | '.join('-' * max(len(str(h)), 3) for h in headers)} |\n"
        + "\n".join(
            f"| {' | '.join(str(cell) for cell in row)} |" for row in rows
        )
    )


def align_table(
    headers: Sequence[Any],
    rows: Sequence[Sequence[Any]],
    alignments: Optional[Sequence[str]] = None,
) -> str:
    """Format table with column alignments."""
    alignments_list = alignments or ["left"] * len(headers)
    alignment_markers = []
    for a in alignments_list:
        if a == "center":
            alignment_markers.append(":---:")
        elif a == "left":
            alignment_markers.append(":---")
        elif a == "right":
            alignment_markers.append("---:")
        else:
            alignment_markers.append("---")

    return (
        f"| {' | '.join(str(h) for h in headers)} |\n"
        f"| {' | '.join(alignment_markers)} |\n"
        + "\n".join(
            f"| {' | '.join(str(cell) for cell in row)} |" for row in rows
        )
    )


def dict_to_table(data: Dict[Any, Any]) -> str:
    """Convert dictionary to markdown table."""
    return "| Key | Value |\n| --- | --- |\n" + "\n".join(
        f"| {k} | {v} |" for k, v in data.items()
    )


def list_to_table(
    items: Sequence[Any], headers: Optional[Sequence[str]] = None
) -> str:
    """Convert list to markdown table."""
    if not headers:
        return "| # | Value |\n| --- | --- |\n" + "\n".join(
            f"| {i+1} | {item} |" for i, item in enumerate(items)
        )
    return (
        f"| {' | '.join(headers)} |\n| {' | '.join('-' * len(h) for h in headers)} |\n"
        + "\n".join(
            f"| {' | '.join(str(cell) for cell in row)} |" for row in items
        )
    )


def escape_special(text: str) -> str:
    """Escape special characters in text."""
    return re.sub(r'([{}\[\]"\'\\])', r"\\\1", text)


def debug_print(x: Any) -> None:
    """Print debug information."""
    print(f"DEBUG: {x}")


def type_of(x: Any) -> str:
    """Get type name of object."""
    return type(x).__name__


def dir_of(x: Any) -> List[str]:
    """Get list of attributes."""
    return dir(x)


def len_of(x: Any) -> Optional[int]:
    """Get length of object if available."""
    return len(x) if hasattr(x, "__len__") else None


def validate_json(text: str) -> bool:
    """Check if text is valid JSON."""
    if not text:
        return False
    try:
        json.loads(text)
        return True
    except json.JSONDecodeError:
        return False


def format_error(e: Exception) -> str:
    """Format exception as string."""
    return f"{type(e).__name__}: {str(e)}"


def estimate_tokens(text: str) -> int:
    """Estimate number of tokens in text."""
    try:
        encoding = tiktoken.encoding_for_model("gpt-4")
        return len(encoding.encode(str(text)))
    except Exception as e:
        logger.warning(f"Failed to estimate tokens: {e}")
        return len(str(text).split())


def format_json(obj: Any) -> str:
    """Format JSON with indentation."""
    return json.dumps(obj, indent=2, default=str)


def auto_table(data: Any) -> str:
    """Format data as table based on type."""
    if isinstance(data, dict):
        return dict_to_table(data)
    if isinstance(data, (list, tuple)):
        return list_to_table(data)
    return str(data)


def sort_by(items: Sequence[T], key: str) -> List[T]:
    """Sort items by key."""

    def get_key(x: T) -> Any:
        if isinstance(x, dict):
            return x.get(key, 0)
        return getattr(x, key, 0)

    return sorted(items, key=get_key)


def group_by(items: Sequence[T], key: str) -> Dict[Any, List[T]]:
    """Group items by key."""

    def safe_get_key(x: T) -> Any:
        if isinstance(x, dict):
            return x.get(key)
        return getattr(x, key, None)

    sorted_items = sorted(items, key=safe_get_key)
    return {
        k: list(g)
        for k, g in itertools.groupby(sorted_items, key=safe_get_key)
    }


def filter_by(items: Sequence[T], key: str, value: Any) -> List[T]:
    """Filter items by key-value pair."""
    return [
        x
        for x in items
        if (x.get(key) if isinstance(x, dict) else getattr(x, key, None))
        == value
    ]


def extract_field(items: Sequence[Any], key: str) -> List[Any]:
    """Extract field from each item."""
    return [
        x.get(key) if isinstance(x, dict) else getattr(x, key, None)
        for x in items
    ]


def frequency(items: Sequence[T]) -> Dict[T, int]:
    """Count frequency of items."""
    return dict(Counter(items))


def aggregate(
    items: Sequence[Any], key: Optional[str] = None
) -> Dict[str, Union[int, float]]:
    """Calculate aggregate statistics."""
    if not items:
        return {"count": 0, "sum": 0, "avg": 0, "min": 0, "max": 0}

    def get_value(x: Any) -> float:
        if key is None:
            if isinstance(x, (int, float)):
                return float(x)
            raise ValueError(f"Cannot convert {type(x)} to float")
        val = x.get(key) if isinstance(x, dict) else getattr(x, key, 0)
        if val is None:
            return 0.0
        return float(val)

    values = [get_value(x) for x in items]
    return {
        "count": len(values),
        "sum": sum(values),
        "avg": sum(values) / len(values),
        "min": min(values),
        "max": max(values),
    }


def unique(items: Sequence[Any]) -> List[Any]:
    """Get unique values while preserving order."""
    return list(dict.fromkeys(items))


def pivot_table(
    data: Sequence[Dict[str, Any]],
    index: str,
    value: str,
    aggfunc: str = "sum",
) -> Dict[str, Dict[str, Any]]:
    """Create pivot table from data."""
    if not data:
        logger.debug("Empty data provided to pivot_table")
        return {
            "aggregates": {},
            "metadata": {"total_records": 0, "null_index_count": 0},
        }

    # Validate aggfunc
    valid_aggfuncs = {"sum", "mean", "count"}
    if aggfunc not in valid_aggfuncs:
        raise ValueError(
            f"Invalid aggfunc: {aggfunc}. Must be one of {valid_aggfuncs}"
        )

    # Validate columns exist in first row
    if data and (index not in data[0] or value not in data[0]):
        missing = []
        if index not in data[0]:
            missing.append(f"index column '{index}'")
        if value not in data[0]:
            missing.append(f"value column '{value}'")
        raise ValueError(f"Missing required columns: {', '.join(missing)}")

    # Count records with null index
    null_index_count = sum(1 for row in data if row.get(index) is None)
    if null_index_count:
        logger.warning(f"Found {null_index_count} rows with null index values")

    # Group by index
    groups: Dict[str, List[float]] = {}
    invalid_values = 0
    for row in data:
        idx = str(row.get(index, ""))
        try:
            val = float(row.get(value, 0))
        except (TypeError, ValueError):
            invalid_values += 1
            logger.warning(
                f"Invalid value for {value} in row with index {idx}, using 0"
            )
            val = 0.0

        if idx not in groups:
            groups[idx] = []
        groups[idx].append(val)

    if invalid_values:
        logger.warning(
            f"Found {invalid_values} invalid values in column {value}"
        )

    result: Dict[str, Dict[str, Any]] = {"aggregates": {}, "metadata": {}}
    for idx, values in groups.items():
        if aggfunc == "sum":
            result["aggregates"][idx] = {"value": sum(values)}
        elif aggfunc == "mean":
            result["aggregates"][idx] = {"value": sum(values) / len(values)}
        else:  # count
            result["aggregates"][idx] = {"value": len(values)}

    result["metadata"] = {
        "total_records": len(data),
        "null_index_count": null_index_count,
        "invalid_values": invalid_values,
    }
    return result


def summarize(
    data: Sequence[Any], keys: Optional[Sequence[str]] = None
) -> Dict[str, Any]:
    """Generate summary statistics for data fields."""
    if not data:
        logger.debug("Empty data provided to summarize")
        return {"total_records": 0, "fields": {}}

    # Validate data type
    if not isinstance(data[0], dict) and not hasattr(data[0], "__dict__"):
        raise TypeError("Data items must be dictionaries or objects")

    def get_field_value(item: Any, field: str) -> Any:
        try:
            if isinstance(item, dict):
                return item.get(field)
            return getattr(item, field, None)
        except Exception as e:
            logger.warning(f"Error accessing field {field}: {e}")
            return None

    def get_field_type(values: List[Any]) -> str:
        """Determine field type from non-null values."""
        non_null = [v for v in values if v is not None]
        if not non_null:
            return "NoneType"

        # Check if all values are of the same type
        types = {type(v) for v in non_null}
        if len(types) == 1:
            return next(iter(types)).__name__

        # Handle mixed numeric types
        if all(isinstance(v, (int, float)) for v in non_null):
            return "number"

        # Default to most specific common ancestor type
        return "mixed"

    def analyze_field(field: str) -> Dict[str, Any]:
        logger.debug(f"Analyzing field: {field}")
        values = [get_field_value(x, field) for x in data]
        non_null = [v for v in values if v is not None]

        stats = {
            "type": get_field_type(values),
            "total": len(values),
            "null_count": len(values) - len(non_null),
            "unique": len(set(non_null)),
        }

        # Add numeric statistics if applicable
        if stats["type"] in ("int", "float", "number"):
            try:
                nums = [float(x) for x in non_null]
                stats.update(
                    {
                        "min": min(nums) if nums else None,
                        "max": max(nums) if nums else None,
                        "avg": sum(nums) / len(nums) if nums else None,
                    }
                )
            except (ValueError, TypeError) as e:
                logger.warning(
                    f"Error calculating numeric stats for {field}: {e}"
                )

        # Add most common values
        if non_null:
            try:
                most_common = Counter(non_null).most_common(5)
                stats["most_common"] = [
                    {"value": str(v), "count": c} for v, c in most_common
                ]
            except TypeError as e:
                logger.warning(
                    f"Error calculating most common values for {field}: {e}"
                )

        return stats

    try:
        available_keys = keys or (
            list(data[0].keys())
            if isinstance(data[0], dict)
            else [k for k in dir(data[0]) if not k.startswith("_")]
        )

        if not available_keys:
            raise ValueError("No valid keys found in data")

        logger.debug(
            f"Analyzing {len(data)} records with {len(available_keys)} fields"
        )
        result = {
            "total_records": len(data),
            "fields": {k: analyze_field(k) for k in available_keys},
        }
        logger.debug("Analysis complete")
        return result

    except Exception as e:
        logger.error(f"Failed to analyze data: {e}", exc_info=True)
        raise ValueError(f"Failed to analyze data: {str(e)}")


def strip_comments(text: str, lang: str = "python") -> str:
    """Remove comments from code text based on language.

    Args:
        text: Code text to process
        lang: Programming language

    Returns:
        Text with comments removed if language is supported,
        otherwise returns original text with a warning
    """
    # Define comment patterns for different languages
    single_line_comments = {
        "python": "#",
        "javascript": "//",
        "typescript": "//",
        "java": "//",
        "c": "//",
        "cpp": "//",
        "go": "//",
        "rust": "//",
        "swift": "//",
        "ruby": "#",
        "perl": "#",
        "shell": "#",
        "bash": "#",
        "php": "//",
    }

    multi_line_comments = {
        "javascript": ("/*", "*/"),
        "typescript": ("/*", "*/"),
        "java": ("/*", "*/"),
        "c": ("/*", "*/"),
        "cpp": ("/*", "*/"),
        "go": ("/*", "*/"),
        "rust": ("/*", "*/"),
        "swift": ("/*", "*/"),
        "php": ("/*", "*/"),
    }

    # Return original text if language is not supported
    if lang not in single_line_comments and lang not in multi_line_comments:
        logger.debug(
            f"Language '{lang}' is not supported for comment removal. "
            f"Comments will be preserved in the output."
        )
        return text

    lines = text.splitlines()
    cleaned_lines = []

    # Handle single-line comments
    if lang in single_line_comments:
        comment_char = single_line_comments[lang]
        for line in lines:
            # Remove inline comments
            line = re.sub(f"\\s*{re.escape(comment_char)}.*$", "", line)
            # Keep non-empty lines
            if line.strip():
                cleaned_lines.append(line)
        text = "\n".join(cleaned_lines)

    # Handle multi-line comments
    if lang in multi_line_comments:
        start, end = multi_line_comments[lang]
        # Remove multi-line comments
        text = re.sub(
            f"{re.escape(start)}.*?{re.escape(end)}", "", text, flags=re.DOTALL
        )

    return text


def format_code(
    text: str, output_format: str = "terminal", language: str = "python"
) -> str:
    """Format code with syntax highlighting.

    Args:
        text (str): The code text to format
        output_format (str): The output format ('terminal', 'html', or 'plain')
        language (str): The programming language for syntax highlighting

    Returns:
        str: Formatted code string

    Raises:
        ValueError: If output_format is not one of 'terminal', 'html', or 'plain'
    """
    if not text:
        return ""

    if output_format not in ["terminal", "html", "plain"]:
        raise ValueError(
            "output_format must be one of 'terminal', 'html', or 'plain'"
        )

    try:
        lexer = get_lexer_by_name(language)
    except ClassNotFound:
        try:
            lexer = guess_lexer(text)
        except ClassNotFound:
            lexer = TextLexer()

    try:
        if output_format == "terminal":
            formatter: Union[
                TerminalFormatter[str], HtmlFormatter[str], NullFormatter[str]
            ] = TerminalFormatter[str]()
        elif output_format == "html":
            formatter = HtmlFormatter[str]()
        else:  # plain
            formatter = NullFormatter[str]()

        return highlight(text, lexer, formatter)
    except Exception as e:
        logger.error(f"Error formatting code: {e}")
        return text


def register_template_filters(env: Environment) -> None:
    """Register all template filters with the Jinja2 environment.
    
    Args:
        env: The Jinja2 environment to register filters with.
    """
    filters = {
        # Text processing
        "extract_keywords": extract_keywords,
        "word_count": word_count,
        "char_count": char_count,
        "to_json": to_json,
        "from_json": from_json,
        "remove_comments": remove_comments,
        "wrap": wrap_text,
        "indent": indent_text,
        "dedent": dedent_text,
        "normalize": normalize_text,
        "strip_markdown": strip_markdown,
        
        # Data processing
        "sort_by": sort_by,
        "group_by": group_by,
        "filter_by": filter_by,
        "extract_field": extract_field,
        "unique": unique,
        "frequency": frequency,
        "aggregate": aggregate,
        
        # Table formatting
        "table": format_table,
        "align_table": align_table,
        "dict_to_table": dict_to_table,
        "list_to_table": list_to_table,
        
        # Code processing
        "format_code": format_code,
        "strip_comments": strip_comments,
        
        # Special character handling
        "escape_special": escape_special,
        
        # Table utilities
        "auto_table": auto_table,
    }
    
    env.filters.update(filters)

    # Add template globals
    env.globals.update({
        "estimate_tokens": estimate_tokens,
        "format_json": format_json,
        "now": datetime.datetime.now,
        "debug": debug_print,
        "type_of": type_of,
        "dir_of": dir_of,
        "len_of": len_of,
        "validate_json": validate_json,
        "format_error": format_error,
        # Data analysis globals
        "summarize": summarize,
        "pivot_table": pivot_table,
        # Table utilities
        "auto_table": auto_table,
    })




"""Command-line interface for making structured OpenAI API calls."""

import argparse
import asyncio
import json
import logging
import os
import sys
from enum import IntEnum
from importlib.metadata import version
from pathlib import Path
from typing import (
    Any,
    Dict,
    List,
    Literal,
    Optional,
    Set,
    Tuple,
    Type,
    TypeVar,
    cast,
    overload,
)

import jinja2
import tiktoken
import yaml
from openai import (
    APIConnectionError,
    AsyncOpenAI,
    AuthenticationError,
    BadRequestError,
    InternalServerError,
    RateLimitError,
)
from pydantic import BaseModel, ConfigDict, create_model

from ..client import async_openai_structured_stream, supports_structured_output
from ..errors import (
    APIResponseError,
    EmptyResponseError,
    InvalidResponseFormatError,
    JSONParseError,
    ModelNotSupportedError,
    ModelVersionError,
    OpenAIClientError,
    SchemaFileError,
    SchemaValidationError,
    StreamBufferError,
    StreamInterruptedError,
    StreamParseError,
)
from .errors import (
    DirectoryNotFoundError,
    FileNotFoundError,
    InvalidJSONError,
    PathSecurityError,
    TaskTemplateSyntaxError,
    TaskTemplateVariableError,
    VariableError,
    VariableNameError,
    VariableValueError,
)
from .file_utils import TemplateValue, collect_files
from .path_utils import validate_path_mapping
from .progress import ProgressContext
from .security import SecurityManager
from .template_env import create_jinja_env
from .template_utils import (
    SystemPromptError,
    TemplateMetadataError,
    render_template,
    validate_json_schema,
    validate_template_placeholders,
)

# Set up logging
logger = logging.getLogger("ostruct")

# Constants
DEFAULT_SYSTEM_PROMPT = "You are a helpful assistant."

# Get package version
try:
    __version__ = version("openai-structured")
except Exception:
    __version__ = "unknown"


class ExitCode(IntEnum):
    """Exit codes for the CLI following standard Unix conventions.

    Categories:
    - Success (0-1)
    - User Interruption (2-3)
    - Input/Validation (64-69)
    - I/O and File Access (70-79)
    - API and External Services (80-89)
    - Internal Errors (90-99)
    """

    # Success codes
    SUCCESS = 0

    # User interruption
    INTERRUPTED = 2

    # Input/Validation errors (64-69)
    USAGE_ERROR = 64
    DATA_ERROR = 65
    SCHEMA_ERROR = 66
    VALIDATION_ERROR = 67

    # I/O and File Access errors (70-79)
    IO_ERROR = 70
    FILE_NOT_FOUND = 71
    PERMISSION_ERROR = 72
    SECURITY_ERROR = 73

    # API and External Service errors (80-89)
    API_ERROR = 80
    API_TIMEOUT = 81

    # Internal errors (90-99)
    INTERNAL_ERROR = 90
    UNKNOWN_ERROR = 91


def create_dynamic_model(schema: Dict[str, Any]) -> Type[BaseModel]:
    """Create a Pydantic model from a JSON schema."""
    properties = schema.get("properties", {})
    field_definitions: Dict[str, Any] = {}

    for name, prop in properties.items():
        field_type: Any
        if prop.get("type") == "string":
            field_type = str
        elif prop.get("type") == "integer":
            field_type = int
        elif prop.get("type") == "number":
            field_type = float
        elif prop.get("type") == "boolean":
            field_type = bool
        elif prop.get("type") == "array":
            field_type = List[Any]
        elif prop.get("type") == "object":
            field_type = Dict[str, Any]
        else:
            field_type = Any

        field_definitions[name] = (field_type, ...)

    model_config = ConfigDict(arbitrary_types_allowed=True)
    model: Type[BaseModel] = create_model(
        "DynamicModel", __config__=model_config, **field_definitions
    )
    return model


T = TypeVar("T")
K = TypeVar("K")
V = TypeVar("V")


def estimate_tokens_for_chat(
    messages: List[Dict[str, str]], model: str
) -> int:
    """Estimate the number of tokens in a chat completion."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        # Fall back to cl100k_base for unknown models
        encoding = tiktoken.get_encoding("cl100k_base")

    num_tokens = 0
    for message in messages:
        # Add message overhead
        num_tokens += 4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
        for key, value in message.items():
            num_tokens += len(encoding.encode(str(value)))
            if key == "name":  # if there's a name, the role is omitted
                num_tokens += -1  # role is always required and always 1 token
    num_tokens += 2  # every reply is primed with <im_start>assistant
    return num_tokens


def get_default_token_limit(model: str) -> int:
    """Get the default token limit for a given model.

    Note: These limits are based on current OpenAI model specifications as of 2024 and may
    need to be updated if OpenAI changes the models' capabilities.

    Args:
        model: The model name (e.g., 'gpt-4o', 'gpt-4o-mini', 'o1')

    Returns:
        The default token limit for the model
    """
    if "o1" in model:
        return 100_000  # o1 supports up to 100K output tokens
    elif "gpt-4o" in model:
        return 16_384  # gpt-4o and gpt-4o-mini support up to 16K output tokens
    else:
        return 4_096  # default fallback


def get_context_window_limit(model: str) -> int:
    """Get the total context window limit for a given model.

    Note: These limits are based on current OpenAI model specifications as of 2024 and may
    need to be updated if OpenAI changes the models' capabilities.

    Args:
        model: The model name (e.g., 'gpt-4o', 'gpt-4o-mini', 'o1')

    Returns:
        The context window limit for the model
    """
    if "o1" in model:
        return 200_000  # o1 supports 200K total context window
    elif "gpt-4o" in model:
        return 128_000  # gpt-4o and gpt-4o-mini support 128K context window
    else:
        return 8_192  # default fallback


def validate_token_limits(
    model: str, total_tokens: int, max_token_limit: Optional[int] = None
) -> None:
    """Validate token counts against model limits.

    Args:
        model: The model name
        total_tokens: Total number of tokens in the prompt
        max_token_limit: Optional user-specified token limit

    Raises:
        ValueError: If token limits are exceeded
    """
    context_limit = get_context_window_limit(model)
    output_limit = (
        max_token_limit
        if max_token_limit is not None
        else get_default_token_limit(model)
    )

    # Check if total tokens exceed context window
    if total_tokens >= context_limit:
        raise ValueError(
            f"Total tokens ({total_tokens:,}) exceed model's context window limit "
            f"of {context_limit:,} tokens"
        )

    # Check if there's enough room for output tokens
    remaining_tokens = context_limit - total_tokens
    if remaining_tokens < output_limit:
        raise ValueError(
            f"Only {remaining_tokens:,} tokens remaining in context window, but "
            f"output may require up to {output_limit:,} tokens"
        )


def process_system_prompt(
    task_template: str,
    system_prompt: Optional[str],
    template_context: Dict[str, Any],
    env: jinja2.Environment,
    ignore_task_sysprompt: bool = False,
) -> str:
    """Process system prompt from various sources.

    Args:
        task_template: The task template string
        system_prompt: Optional system prompt string or file path (with @ prefix)
        template_context: Template context for rendering
        env: Jinja2 environment
        ignore_task_sysprompt: Whether to ignore system prompt in task template

    Returns:
        The final system prompt string

    Raises:
        SystemPromptError: If the system prompt cannot be loaded or rendered
        FileNotFoundError: If a prompt file does not exist
        PathSecurityError: If a prompt file path violates security constraints
    """
    # Default system prompt
    default_prompt = "You are a helpful assistant."

    # Try to get system prompt from CLI argument first
    if system_prompt:
        if system_prompt.startswith("@"):
            # Load from file
            path = system_prompt[1:]
            try:
                name, path = validate_path_mapping(f"system_prompt={path}")
                with open(path, "r", encoding="utf-8") as f:
                    system_prompt = f.read().strip()
            except (FileNotFoundError, PathSecurityError) as e:
                raise SystemPromptError(f"Invalid system prompt file: {e}")

        # Render system prompt with template context
        try:
            template = env.from_string(system_prompt)
            return template.render(**template_context).strip()
        except jinja2.TemplateError as e:
            raise SystemPromptError(f"Error rendering system prompt: {e}")

    # If not ignoring task template system prompt, try to extract it
    if not ignore_task_sysprompt:
        try:
            # Extract YAML frontmatter
            if task_template.startswith("---\n"):
                end = task_template.find("\n---\n", 4)
                if end != -1:
                    frontmatter = task_template[4:end]
                    try:
                        metadata = yaml.safe_load(frontmatter)
                        if (
                            isinstance(metadata, dict)
                            and "system_prompt" in metadata
                        ):
                            system_prompt = str(metadata["system_prompt"])
                            # Render system prompt with template context
                            try:
                                template = env.from_string(system_prompt)
                                return template.render(
                                    **template_context
                                ).strip()
                            except jinja2.TemplateError as e:
                                raise SystemPromptError(
                                    f"Error rendering system prompt: {e}"
                                )
                    except yaml.YAMLError as e:
                        raise SystemPromptError(
                            f"Invalid YAML frontmatter: {e}"
                        )

        except Exception as e:
            raise SystemPromptError(
                f"Error extracting system prompt from template: {e}"
            )

    # Fall back to default
    return default_prompt


def validate_variable_mapping(
    mapping: str, is_json: bool = False
) -> tuple[str, Any]:
    """Validate a variable mapping in name=value format."""
    try:
        name, value = mapping.split("=", 1)
        if not name:
            raise VariableNameError(
                f"Empty name in {'JSON ' if is_json else ''}variable mapping"
            )

        if is_json:
            try:
                value = json.loads(value)
            except json.JSONDecodeError as e:
                raise InvalidJSONError(
                    f"Invalid JSON value for variable {name!r}: {value!r}"
                ) from e

        return name, value

    except ValueError as e:
        if "not enough values to unpack" in str(e):
            raise VariableValueError(
                f"Invalid {'JSON ' if is_json else ''}variable mapping "
                f"(expected name=value format): {mapping!r}"
            )
        raise


@overload
def _validate_path_mapping_internal(
    mapping: str,
    is_dir: Literal[True],
    base_dir: Optional[str] = None,
    security_manager: Optional[SecurityManager] = None,
) -> Tuple[str, str]: ...


@overload
def _validate_path_mapping_internal(
    mapping: str,
    is_dir: Literal[False] = False,
    base_dir: Optional[str] = None,
    security_manager: Optional[SecurityManager] = None,
) -> Tuple[str, str]: ...


def _validate_path_mapping_internal(
    mapping: str,
    is_dir: bool = False,
    base_dir: Optional[str] = None,
    security_manager: Optional[SecurityManager] = None,
) -> Tuple[str, str]:
    """Validate a path mapping in the format "name=path".

    Args:
        mapping: The path mapping string (e.g., "myvar=/path/to/file").
        is_dir: Whether the path is expected to be a directory (True) or file (False).
        base_dir: Optional base directory to resolve relative paths against.
        security_manager: Optional security manager to validate paths.

    Returns:
        A (name, path) tuple.

    Raises:
        VariableNameError: If the variable name portion is empty or invalid.
        DirectoryNotFoundError: If is_dir=True and the path is not a directory or doesn't exist.
        FileNotFoundError: If is_dir=False and the path is not a file or doesn't exist.
        PathSecurityError: If the path is inaccessible or outside the allowed directory.
        ValueError: If the format is invalid (missing "=").
        OSError: If there is an underlying OS error (permissions, etc.).
    """
    try:
        if not mapping or "=" not in mapping:
            raise ValueError(
                "Invalid path mapping format. Expected format: name=path"
            )

        name, path = mapping.split("=", 1)
        if not name:
            raise VariableNameError(
                f"Empty name in {'directory' if is_dir else 'file'} mapping"
            )

        if not path:
            raise VariableValueError("Path cannot be empty")

        # Convert to Path object and resolve against base_dir if provided
        path_obj = Path(path)
        if base_dir:
            path_obj = Path(base_dir) / path_obj

        # Resolve the path to catch directory traversal attempts
        try:
            resolved_path = path_obj.resolve()
        except OSError as e:
            raise OSError(f"Failed to resolve path: {e}")

        # Check for directory traversal
        try:
            base_path = (
                Path.cwd() if base_dir is None else Path(base_dir).resolve()
            )
            if not str(resolved_path).startswith(str(base_path)):
                raise PathSecurityError(
                    f"Path {str(path)!r} resolves to {str(resolved_path)!r} which is outside "
                    f"base directory {str(base_path)!r}"
                )
        except OSError as e:
            raise OSError(f"Failed to resolve base path: {e}")

        # Check if path exists
        if not resolved_path.exists():
            if is_dir:
                raise DirectoryNotFoundError(f"Directory not found: {path!r}")
            else:
                raise FileNotFoundError(f"File not found: {path!r}")

        # Check if path is correct type
        if is_dir and not resolved_path.is_dir():
            raise DirectoryNotFoundError(f"Path is not a directory: {path!r}")
        elif not is_dir and not resolved_path.is_file():
            raise FileNotFoundError(f"Path is not a file: {path!r}")

        # Check if path is accessible
        try:
            if is_dir:
                os.listdir(str(resolved_path))
            else:
                with open(str(resolved_path), "r", encoding="utf-8") as f:
                    f.read(1)
        except OSError as e:
            if e.errno == 13:  # Permission denied
                raise PathSecurityError(
                    f"Permission denied accessing path: {path!r}",
                    error_logged=True,
                )
            raise

        if security_manager:
            if not security_manager.is_allowed_file(str(resolved_path)):
                raise PathSecurityError.from_expanded_paths(
                    original_path=str(path),
                    expanded_path=str(resolved_path),
                    base_dir=str(security_manager.base_dir),
                    allowed_dirs=[
                        str(d) for d in security_manager.allowed_dirs
                    ],
                    error_logged=True,
                )

        # Return the original path to maintain relative paths in the output
        return name, path

    except ValueError as e:
        if "not enough values to unpack" in str(e):
            raise VariableValueError(
                f"Invalid {'directory' if is_dir else 'file'} mapping "
                f"(expected name=path format): {mapping!r}"
            )
        raise


def validate_task_template(task: str) -> str:
    """Validate and load a task template.

    Args:
        task: The task template string or path to task template file (with @ prefix)

    Returns:
        The task template string

    Raises:
        TaskTemplateVariableError: If the template file cannot be read or is invalid
        TaskTemplateSyntaxError: If the template has invalid syntax
        FileNotFoundError: If the template file does not exist
        PathSecurityError: If the template file path violates security constraints
    """
    template_content = task

    # Check if task is a file path
    if task.startswith("@"):
        path = task[1:]
        try:
            name, path = validate_path_mapping(f"task={path}")
            with open(path, "r", encoding="utf-8") as f:
                template_content = f.read()
        except (FileNotFoundError, PathSecurityError) as e:
            raise TaskTemplateVariableError(f"Invalid task template file: {e}")

    # Validate template syntax
    try:
        env = jinja2.Environment(undefined=jinja2.StrictUndefined)
        env.parse(template_content)
        return template_content
    except jinja2.TemplateSyntaxError as e:
        raise TaskTemplateSyntaxError(
            f"Invalid task template syntax at line {e.lineno}: {e.message}"
        )


def validate_schema_file(path: str) -> Dict[str, Any]:
    """Validate and load a JSON schema file.

    Args:
        path: Path to the JSON schema file

    Returns:
        The loaded and validated schema

    Raises:
        InvalidJSONError: If the schema file contains invalid JSON
        SchemaValidationError: If the schema is invalid
        FileNotFoundError: If the schema file does not exist
        PathSecurityError: If the schema file is outside base directory
    """
    try:
        # Validate file exists and is readable
        name, path = validate_path_mapping(f"schema={path}")

        # Load and parse JSON
        with open(path, "r", encoding="utf-8") as f:
            try:
                schema = json.load(f)
            except json.JSONDecodeError as e:
                raise InvalidJSONError(f"Invalid JSON: {e.msg}")

        # Validate schema
        validate_json_schema(schema)
        return cast(Dict[str, Any], schema)
    except json.JSONDecodeError as e:
        raise InvalidJSONError(f"Invalid JSON: {e.msg}")


def collect_template_files(
    args: argparse.Namespace,
    security_manager: SecurityManager,
) -> Dict[str, TemplateValue]:
    """Collect files from command line arguments.

    Args:
        args: Parsed command line arguments
        security_manager: Security manager for path validation

    Returns:
        Dictionary mapping variable names to file info objects

    Raises:
        PathSecurityError: If any file paths violate security constraints
        ValueError: If file mappings are invalid or files cannot be accessed
    """
    try:
        result = collect_files(
            file_mappings=args.file,
            pattern_mappings=args.files,
            dir_mappings=args.dir,
            recursive=args.recursive,
            extensions=args.ext.split(",") if args.ext else None,
            security_manager=security_manager,
        )
        return cast(Dict[str, TemplateValue], result)
    except PathSecurityError:
        # Let PathSecurityError propagate without wrapping
        raise
    except (FileNotFoundError, DirectoryNotFoundError) as e:
        # Wrap file-related errors
        raise ValueError(f"File access error: {e}")
    except Exception as e:
        # Check if this is a wrapped security error
        if isinstance(e.__cause__, PathSecurityError):
            raise e.__cause__
        # Wrap unexpected errors
        raise ValueError(f"Error collecting files: {e}")


def collect_simple_variables(args: argparse.Namespace) -> Dict[str, str]:
    """Collect simple string variables from --var arguments.

    Args:
        args: Parsed command line arguments

    Returns:
        Dictionary mapping variable names to string values

    Raises:
        VariableNameError: If a variable name is invalid or duplicate
    """
    variables: Dict[str, str] = {}
    all_names: Set[str] = set()

    if args.var:
        for mapping in args.var:
            try:
                name, value = mapping.split("=", 1)
                if not name.isidentifier():
                    raise VariableNameError(f"Invalid variable name: {name}")
                if name in all_names:
                    raise VariableNameError(f"Duplicate variable name: {name}")
                variables[name] = value
                all_names.add(name)
            except ValueError:
                raise VariableNameError(
                    f"Invalid variable mapping (expected name=value format): {mapping!r}"
                )

    return variables


def collect_json_variables(args: argparse.Namespace) -> Dict[str, Any]:
    """Collect JSON variables from --json-var arguments.

    Args:
        args: Parsed command line arguments

    Returns:
        Dictionary mapping variable names to parsed JSON values

    Raises:
        VariableNameError: If a variable name is invalid or duplicate
        InvalidJSONError: If a JSON value is invalid
    """
    variables: Dict[str, Any] = {}
    all_names: Set[str] = set()

    if args.json_var:
        for mapping in args.json_var:
            try:
                name, json_str = mapping.split("=", 1)
                if not name.isidentifier():
                    raise VariableNameError(f"Invalid variable name: {name}")
                if name in all_names:
                    raise VariableNameError(f"Duplicate variable name: {name}")
                try:
                    value = json.loads(json_str)
                    variables[name] = value
                    all_names.add(name)
                except json.JSONDecodeError as e:
                    raise InvalidJSONError(
                        f"Invalid JSON value for {name}: {str(e)}"
                    )
            except ValueError:
                raise VariableNameError(
                    f"Invalid JSON variable mapping format: {mapping}. Expected name=json"
                )

    return variables


def create_template_context(
    args: argparse.Namespace,
    security_manager: SecurityManager,
) -> Dict[str, Any]:
    """Create template context from command line arguments.

    Args:
        args: Parsed command line arguments
        security_manager: Security manager for path validation

    Returns:
        Template context dictionary with files accessible as:
            doc.content  # For single files
            doc[0].content  # Traditional access (still works)
            doc.content  # Returns list for multiple files

    Raises:
        PathSecurityError: If any file paths violate security constraints
        VariableError: If variable mappings are invalid
        ValueError: If file mappings are invalid or files cannot be accessed
    """
    try:
        context: Dict[str, Any] = {}

        # Only collect files if there are file mappings
        if any([args.file, args.files, args.dir]):
            files = collect_files(
                file_mappings=args.file,
                pattern_mappings=args.files,
                dir_mappings=args.dir,
                recursive=args.recursive,
                extensions=args.ext.split(",") if args.ext else None,
                security_manager=security_manager,
            )
            context.update(files)

        # Add simple variables
        try:
            variables = collect_simple_variables(args)
            context.update(variables)
        except VariableNameError as e:
            raise VariableError(str(e))

        # Add JSON variables
        if args.json_var:
            for mapping in args.json_var:
                try:
                    name, value = mapping.split("=", 1)
                    if not name.isidentifier():
                        raise VariableNameError(
                            f"Invalid variable name: {name}"
                        )
                    try:
                        json_value = json.loads(value)
                    except json.JSONDecodeError as e:
                        raise InvalidJSONError(
                            f"Invalid JSON value for {name} ({value!r}): {str(e)}"
                        )
                    if name in context:
                        raise VariableNameError(
                            f"Duplicate variable name: {name}"
                        )
                    context[name] = json_value
                except ValueError:
                    raise VariableNameError(
                        f"Invalid JSON variable mapping format: {mapping}. Expected name=json"
                    )

        # Add stdin if available and readable
        try:
            if not sys.stdin.isatty():
                context["stdin"] = sys.stdin.read()
        except (OSError, IOError):
            # Skip stdin if it can't be read (e.g. in pytest environment)
            pass

        return context

    except PathSecurityError:
        # Let PathSecurityError propagate without wrapping
        raise
    except (FileNotFoundError, DirectoryNotFoundError) as e:
        # Wrap file-related errors
        raise ValueError(f"File access error: {e}")
    except Exception as e:
        # Check if this is a wrapped security error
        if isinstance(e.__cause__, PathSecurityError):
            raise e.__cause__
        # Wrap unexpected errors
        raise ValueError(f"Error collecting files: {e}")


def validate_security_manager(
    base_dir: Optional[str] = None,
    allowed_dirs: Optional[List[str]] = None,
    allowed_dirs_file: Optional[str] = None,
) -> SecurityManager:
    """Create and validate a security manager.

    Args:
        base_dir: Optional base directory to resolve paths against
        allowed_dirs: Optional list of allowed directory paths
        allowed_dirs_file: Optional path to file containing allowed directories

    Returns:
        Configured SecurityManager instance

    Raises:
        FileNotFoundError: If allowed_dirs_file does not exist
        PathSecurityError: If any paths are outside base directory
    """
    # Convert base_dir to string if it's a Path
    base_dir_str = str(base_dir) if base_dir else None
    security_manager = SecurityManager(base_dir_str)

    if allowed_dirs_file:
        security_manager.add_allowed_dirs_from_file(str(allowed_dirs_file))

    if allowed_dirs:
        for allowed_dir in allowed_dirs:
            security_manager.add_allowed_dir(str(allowed_dir))

    return security_manager


def parse_var(var_str: str) -> Tuple[str, str]:
    """Parse a variable string in the format 'name=value'.

    Args:
        var_str: Variable string in format 'name=value'

    Returns:
        Tuple of (name, value)

    Raises:
        VariableNameError: If variable name is empty or invalid
        VariableValueError: If variable format is invalid
    """
    try:
        name, value = var_str.split("=", 1)
        if not name:
            raise VariableNameError("Empty name in variable mapping")
        if not name.isidentifier():
            raise VariableNameError(
                f"Invalid variable name: {name}. Must be a valid Python identifier"
            )
        return name, value

    except ValueError as e:
        if "not enough values to unpack" in str(e):
            raise VariableValueError(
                f"Invalid variable mapping (expected name=value format): {var_str!r}"
            )
        raise


def parse_json_var(var_str: str) -> Tuple[str, Any]:
    """Parse a JSON variable string in the format 'name=json_value'.

    Args:
        var_str: Variable string in format 'name=json_value'

    Returns:
        Tuple of (name, parsed_value)

    Raises:
        VariableNameError: If variable name is empty or invalid
        VariableValueError: If variable format is invalid
        InvalidJSONError: If JSON value is invalid
    """
    try:
        name, json_str = var_str.split("=", 1)
        if not name:
            raise VariableNameError("Empty name in JSON variable mapping")
        if not name.isidentifier():
            raise VariableNameError(
                f"Invalid variable name: {name}. Must be a valid Python identifier"
            )

        try:
            value = json.loads(json_str)
        except json.JSONDecodeError as e:
            raise InvalidJSONError(
                f"Invalid JSON value for variable {name!r}: {json_str!r}"
            ) from e

        return name, value

    except ValueError as e:
        if "not enough values to unpack" in str(e):
            raise VariableValueError(
                f"Invalid JSON variable mapping (expected name=json format): {var_str!r}"
            )
        raise


def create_argument_parser() -> argparse.ArgumentParser:
    """Create and configure the argument parser for the CLI.

    Returns:
        The configured argument parser.
    """
    parser = argparse.ArgumentParser(
        description="Make structured OpenAI API calls from the command line.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    # Required arguments
    parser.add_argument(
        "--task",
        required=True,
        help="Task template string or @file",
    )

    # File access arguments
    parser.add_argument(
        "--file",
        action="append",
        default=[],
        help="Map file to variable (name=path)",
        metavar="NAME=PATH",
    )
    parser.add_argument(
        "--files",
        action="append",
        default=[],
        help="Map file pattern to variable (name=pattern)",
        metavar="NAME=PATTERN",
    )
    parser.add_argument(
        "--dir",
        action="append",
        default=[],
        help="Map directory to variable (name=path)",
        metavar="NAME=PATH",
    )
    parser.add_argument(
        "--allowed-dir",
        action="append",
        default=[],
        help="Additional allowed directory or @file",
        metavar="PATH",
    )
    parser.add_argument(
        "--recursive",
        action="store_true",
        help="Process directories recursively",
    )
    parser.add_argument(
        "--ext",
        help="Comma-separated list of file extensions to include",
    )

    # Variable arguments
    parser.add_argument(
        "--var",
        action="append",
        default=[],
        help="Pass simple variables (name=value)",
        metavar="NAME=VALUE",
    )
    parser.add_argument(
        "--json-var",
        action="append",
        default=[],
        help="Pass JSON variables (name=json)",
        metavar="NAME=JSON",
    )

    # System prompt options
    parser.add_argument(
        "--system-prompt",
        help=(
            "System prompt for the model (use @file to load from file, "
            "can also be specified in task template YAML frontmatter)"
        ),
        default=DEFAULT_SYSTEM_PROMPT,
    )
    parser.add_argument(
        "--ignore-task-sysprompt",
        action="store_true",
        help="Ignore system prompt from task template YAML frontmatter",
    )

    # Schema validation
    parser.add_argument(
        "--schema",
        dest="schema_file",
        required=True,
        help="JSON schema file for response validation",
    )
    parser.add_argument(
        "--validate-schema",
        action="store_true",
        help="Validate schema and response",
    )

    # Model configuration
    parser.add_argument(
        "--model",
        default="gpt-4o-2024-08-06",
        help="Model to use",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.0,
        help="Temperature (0.0-2.0)",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        help="Maximum tokens to generate",
    )
    parser.add_argument(
        "--top-p",
        type=float,
        default=1.0,
        help="Top-p sampling (0.0-1.0)",
    )
    parser.add_argument(
        "--frequency-penalty",
        type=float,
        default=0.0,
        help="Frequency penalty (-2.0-2.0)",
    )
    parser.add_argument(
        "--presence-penalty",
        type=float,
        default=0.0,
        help="Presence penalty (-2.0-2.0)",
    )
    parser.add_argument(
        "--timeout",
        type=float,
        default=60.0,
        help="API timeout in seconds",
    )

    # Output options
    parser.add_argument(
        "--output-file",
        help="Write JSON output to file",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate API call without making request",
    )
    parser.add_argument(
        "--no-progress",
        action="store_true",
        help="Disable progress indicators",
    )

    # Other options
    parser.add_argument(
        "--api-key",
        help="OpenAI API key (overrides env var)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable detailed logging",
    )
    parser.add_argument(
        "--version",
        action="version",
        version=f"%(prog)s {__version__}",
    )

    return parser


async def _main() -> ExitCode:
    """Main entry point for the CLI.

    Returns:
        Exit code indicating success or type of failure
    """
    # Parse command line arguments
    parser = create_argument_parser()
    args = parser.parse_args()

    # Configure logging
    log_dir = os.path.expanduser("~/.ostruct/logs")
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, "ostruct.log")

    # Configure root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)

    # Remove any existing handlers
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # Create file handler
    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(
        logging.Formatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        )
    )
    root_logger.addHandler(file_handler)

    # Create console handler
    console_handler = logging.StreamHandler(sys.stderr)
    console_handler.setLevel(logging.DEBUG if args.verbose else logging.INFO)
    console_handler.setFormatter(
        logging.Formatter("%(levelname)s:%(name)s:%(funcName)s: %(message)s")
    )
    root_logger.addHandler(console_handler)

    logger = logging.getLogger("ostruct")
    logger.debug("Starting ostruct CLI with log file: %s", log_file)

    # Initialize security manager with current directory as base
    security_manager = SecurityManager(str(Path.cwd()))
    logger.debug("Initialized security manager with base dir: %s", Path.cwd())

    # Process allowed directories
    if args.allowed_dir:
        for allowed_dir in args.allowed_dir:
            if allowed_dir.startswith("@"):
                # Read allowed directories from file
                allowed_file = allowed_dir[1:]
                try:
                    security_manager.add_allowed_dirs_from_file(allowed_file)
                except PathSecurityError as e:
                    if not e.has_been_logged:
                        logger.error(str(e))
                    return ExitCode.SECURITY_ERROR
                except OSError as e:
                    logger.error(
                        f"Could not read allowed directories file: {e}"
                    )
                    return ExitCode.IO_ERROR
            else:
                # Add single allowed directory
                try:
                    security_manager.add_allowed_dir(allowed_dir)
                except OSError as e:
                    logger.error(f"Invalid allowed directory path: {e}")
                    return ExitCode.IO_ERROR

    # Create template context from arguments with security checks
    try:
        logger.debug("[_main] Creating template context from arguments")
        template_context = create_template_context(args, security_manager)
    except PathSecurityError as e:
        logger.debug(
            "[_main] Caught PathSecurityError: %s (logged=%s)",
            str(e),
            getattr(e, "has_been_logged", False),
        )
        if not getattr(e, "has_been_logged", False):
            logger.error(str(e))
        return ExitCode.SECURITY_ERROR
    except VariableError as e:
        logger.debug("[_main] Caught VariableError: %s", str(e))
        logger.error(str(e))
        return ExitCode.DATA_ERROR
    except OSError as e:
        logger.debug("[_main] Caught OSError: %s", str(e))
        logger.error(f"File access error: {e}")
        return ExitCode.IO_ERROR
    except ValueError as e:
        # Check if this is a wrapped security error
        if isinstance(e.__cause__, PathSecurityError):
            logger.debug(
                "[_main] Caught wrapped PathSecurityError in ValueError: %s (logged=%s)",
                str(e.__cause__),
                getattr(e.__cause__, "has_been_logged", False),
            )
            if not getattr(e.__cause__, "has_been_logged", False):
                logger.error(str(e.__cause__))
            return ExitCode.SECURITY_ERROR
        # Check if this is a wrapped security error in the error message
        if "Access denied:" in str(e):
            logger.debug(
                "[_main] Detected security error in ValueError message: %s",
                str(e),
            )
            logger.error(f"Invalid input: {e}")
            return ExitCode.SECURITY_ERROR
        logger.debug("[_main] Caught ValueError: %s", str(e))
        logger.error(f"Invalid input: {e}")
        return ExitCode.DATA_ERROR

    # Create Jinja2 environment
    env = create_jinja_env(
        loader=jinja2.FileSystemLoader("."),
    )

    # Load and validate task template
    try:
        task_template = validate_task_template(args.task)
    except TaskTemplateSyntaxError as e:
        logger.error(f"Template syntax error: {e}")
        return ExitCode.VALIDATION_ERROR
    except PathSecurityError as e:
        if not e.has_been_logged:
            logger.error(str(e))
        return ExitCode.SECURITY_ERROR
    except OSError as e:
        logger.error(f"Could not read template file: {e}")
        return ExitCode.IO_ERROR

    # Validate template placeholders
    try:
        validate_template_placeholders(task_template, template_context, env)
    except ValueError as e:
        logger.error(f"Template validation error: {e}")
        return ExitCode.VALIDATION_ERROR

    # Load and validate schema
    try:
        schema = validate_schema_file(args.schema_file)
    except SchemaValidationError as e:
        logger.error(f"Schema validation error: {e}")
        return ExitCode.SCHEMA_ERROR
    except PathSecurityError as e:
        if not e.has_been_logged:
            logger.error(str(e))
        return ExitCode.SECURITY_ERROR
    except OSError as e:
        logger.error(f"Could not read schema file: {e}")
        return ExitCode.IO_ERROR

    # Process system prompt
    try:
        system_prompt = process_system_prompt(
            task_template,
            args.system_prompt,
            template_context,
            env,
            args.ignore_task_sysprompt,
        )
    except (TemplateMetadataError, SystemPromptError) as e:
        logger.error(str(e))
        return ExitCode.VALIDATION_ERROR

    # Render task template
    try:
        user_message = render_template(task_template, template_context, env)
    except jinja2.TemplateSyntaxError as e:
        logger.error(f"Template syntax error: {e}")
        return ExitCode.VALIDATION_ERROR
    except jinja2.UndefinedError as e:
        logger.error(f"Template variable error: {e}")
        return ExitCode.VALIDATION_ERROR
    except Exception as e:
        logger.error(f"Error rendering template: {e}")
        return ExitCode.VALIDATION_ERROR

    # Create messages for chat completion
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message},
    ]

    # Estimate tokens
    try:
        total_tokens = estimate_tokens_for_chat(messages, args.model)
        validate_token_limits(args.model, total_tokens, args.max_tokens)
        logger.debug(f"Total tokens in prompt: {total_tokens:,}")
    except (ValueError, OpenAIClientError) as e:
        logger.error(str(e))
        return ExitCode.VALIDATION_ERROR

    # Handle dry run mode
    if args.dry_run:
        logger.info("*** DRY RUN MODE - No API call will be made ***\n")
        logger.info("System Prompt:\n%s\n", system_prompt)
        logger.info("User Prompt:\n%s\n", user_message)
        logger.info("Estimated Tokens: %s", total_tokens)
        logger.info("Model: %s", args.model)
        logger.info("Temperature: %s", args.temperature)
        if args.max_tokens is not None:
            logger.info("Max Tokens: %s", args.max_tokens)
        logger.info("Top P: %s", args.top_p)
        logger.info("Frequency Penalty: %s", args.frequency_penalty)
        logger.info("Presence Penalty: %s", args.presence_penalty)
        if args.validate_schema:
            logger.info("Schema: Valid")
        if args.output_file:
            logger.info("Output would be written to: %s", args.output_file)
        return ExitCode.SUCCESS

    # Get API key
    api_key = args.api_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.error(
            "No OpenAI API key provided (--api-key or OPENAI_API_KEY env var)"
        )
        return ExitCode.USAGE_ERROR

    # Create OpenAI client
    client = AsyncOpenAI(api_key=api_key, timeout=args.timeout)

    # Make API request
    try:
        if not supports_structured_output(args.model):
            logger.error(
                f"Model '{args.model}' does not support structured output"
            )
            return ExitCode.API_ERROR

        # Create output schema model
        output_model = (
            create_dynamic_model(schema)
            if args.validate_schema
            else create_model(
                "DynamicModel",
                __config__=ConfigDict(arbitrary_types_allowed=True),
                result=(Any, ...),
            )
        )

        # Initialize response
        response = None
        async for chunk in async_openai_structured_stream(
            client=client,
            model=args.model,
            output_schema=output_model,
            system_prompt=system_prompt,
            user_prompt=user_message,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            top_p=args.top_p,
            frequency_penalty=args.frequency_penalty,
            presence_penalty=args.presence_penalty,
        ):
            # Each chunk is already a structured response
            logger.debug("Received chunk: %s", chunk)
            response = chunk
            # Print each chunk as it arrives
            if not args.output_file:
                # Convert Pydantic model to dict before JSON serialization
                chunk_dict = (
                    chunk.model_dump()
                    if hasattr(chunk, "model_dump")
                    else chunk
                )
                print(json.dumps(chunk_dict, indent=2))

        if response is None:
            raise EmptyResponseError("No response received from API")

        # Write response to file if requested
        if args.output_file:
            try:
                # Convert Pydantic model to dict before JSON serialization
                response_dict = (
                    response.model_dump()
                    if hasattr(response, "model_dump")
                    else response
                )
                with open(args.output_file, "w") as f:
                    json.dump(response_dict, f, indent=2)
            except OSError:
                raise ValueError("Could not write to output file")

        return ExitCode.SUCCESS

    except KeyboardInterrupt:
        return ExitCode.INTERRUPTED

    except (
        APIConnectionError,
        AuthenticationError,
        BadRequestError,
        RateLimitError,
        InternalServerError,
    ) as e:
        logger.error(f"API error: {e}")
        return ExitCode.API_ERROR

    except (
        APIResponseError,
        EmptyResponseError,
        InvalidResponseFormatError,
        JSONParseError,
        ModelNotSupportedError,
        ModelVersionError,
        StreamBufferError,
        StreamInterruptedError,
        StreamParseError,
    ) as e:
        logger.error(f"Stream error: {e}")
        return ExitCode.API_ERROR

    except KeyboardInterrupt:
        return ExitCode.INTERRUPTED

    except (
        FileNotFoundError,
        DirectoryNotFoundError,
        PathSecurityError,
        SchemaFileError,
    ) as e:
        logger.error(str(e))
        return ExitCode.IO_ERROR

    except (
        InvalidJSONError,
        SchemaValidationError,
        TaskTemplateSyntaxError,
        TaskTemplateVariableError,
        VariableNameError,
        VariableValueError,
    ) as e:
        logger.error(str(e))
        return ExitCode.VALIDATION_ERROR

    except OpenAIClientError as e:
        logger.error(str(e))
        return ExitCode.API_ERROR

    except Exception:
        logger.error("Unexpected error occurred")
        if args.verbose:
            import traceback

            traceback.print_exc()
        return ExitCode.INTERNAL_ERROR


def main() -> None:
    """CLI entry point that handles all errors."""
    try:
        logger.debug("[main] Starting main execution")
        exit_code = asyncio.run(_main())
        sys.exit(exit_code.value)
    except KeyboardInterrupt:
        logger.error("Operation cancelled by user")
        sys.exit(ExitCode.INTERRUPTED.value)
    except PathSecurityError as e:
        # Only log security errors if they haven't been logged already
        logger.debug(
            "[main] Caught PathSecurityError: %s (logged=%s)",
            str(e),
            getattr(e, "has_been_logged", False),
        )
        if not getattr(e, "has_been_logged", False):
            logger.error(str(e))
        sys.exit(ExitCode.SECURITY_ERROR.value)
    except ValueError as e:
        # Get the original cause of the error
        cause = e.__cause__ or e.__context__
        if isinstance(cause, PathSecurityError):
            logger.debug(
                "[main] Caught wrapped PathSecurityError in ValueError: %s (logged=%s)",
                str(cause),
                getattr(cause, "has_been_logged", False),
            )
            # Only log security errors if they haven't been logged already
            if not getattr(cause, "has_been_logged", False):
                logger.error(str(cause))
            sys.exit(ExitCode.SECURITY_ERROR.value)
        else:
            logger.debug("[main] Caught ValueError: %s", str(e))
            logger.error(f"Invalid input: {e}")
            sys.exit(ExitCode.DATA_ERROR.value)
    except Exception as e:
        # Check if this is a wrapped security error
        if isinstance(e.__cause__, PathSecurityError):
            logger.debug(
                "[main] Caught wrapped PathSecurityError in Exception: %s (logged=%s)",
                str(e.__cause__),
                getattr(e.__cause__, "has_been_logged", False),
            )
            # Only log security errors if they haven't been logged already
            if not getattr(e.__cause__, "has_been_logged", False):
                logger.error(str(e.__cause__))
            sys.exit(ExitCode.SECURITY_ERROR.value)
        logger.debug("[main] Caught unexpected error: %s", str(e))
        logger.error(f"Invalid input: {e}")
        sys.exit(ExitCode.DATA_ERROR.value)


if __name__ == "__main__":
    main()

# Export public API
__all__ = [
    "create_dynamic_model",
    "validate_template_placeholders",
    "estimate_tokens_for_chat",
    "get_context_window_limit",
    "get_default_token_limit",
    "validate_token_limits",
    "supports_structured_output",
    "ProgressContext",
    "validate_path_mapping",
]




"""Common utilities for the CLI package."""

from typing import Tuple

from .errors import VariableNameError, VariableValueError


def parse_mapping(mapping: str) -> Tuple[str, str]:
    """Parse a mapping string in the format 'name=value'.

    Args:
        mapping: Mapping string in format 'name=value'

    Returns:
        Tuple of (name, value)

    Raises:
        ValueError: If mapping format is invalid
        VariableNameError: If name part is empty
        VariableValueError: If value part is empty
    """
    if not mapping or "=" not in mapping:
        raise ValueError("Invalid mapping format")

    name, value = mapping.split("=", 1)
    if not name:
        raise VariableNameError("Empty name in mapping")
    if not value:
        raise VariableValueError("Empty value in mapping")

    return name, value




"""File utilities for the CLI.

This module provides utilities for file operations with security controls:

1. File Information:
   - FileInfo class for safe file access and metadata
   - Support for file content caching
   - Automatic encoding detection

2. Path Handling:
   - Supports ~ expansion for home directory
   - Supports environment variable expansion
   - Security checks for file access
   - Requires explicit allowed directories for access outside CWD

3. Security Features:
   - Directory traversal prevention
   - Explicit allowed directory configuration
   - Temporary file access controls
   - Path validation and normalization

Usage Examples:
    Basic file access (from current directory):
    >>> info = FileInfo.from_path("var_name", "local_file.txt")
    >>> content = info.content

    Access home directory files (requires --allowed-dir):
    >>> info = FileInfo.from_path("var_name", "~/file.txt", allowed_dirs=["~/"])
    >>> content = info.content

    Multiple file collection:
    >>> files = collect_files(
    ...     file_args=["var=path.txt"],
    ...     allowed_dirs=["/allowed/path"],
    ...     recursive=True
    ... )

Security Notes:
    - Files must be in current directory or explicitly allowed directories
    - Use --allowed-dir to access files outside current directory
    - Home directory (~) is not automatically allowed
    - Environment variables are expanded in paths
"""

import codecs
import glob
import logging
import os
from typing import Any, Dict, List, Optional, Type, Union

import chardet

from .errors import (
    DirectoryNotFoundError,
    FileNotFoundError,
    PathSecurityError,
)
from .file_info import FileInfo
from .file_list import FileInfoList
from .security import SecurityManager
from .security_types import SecurityManagerProtocol

__all__ = [
    "FileInfo",  # Re-exported from file_info
    "SecurityManager",  # Re-exported from security
    "FileInfoList",  # Re-exported from file_list
    "collect_files",
    "collect_files_from_pattern",
    "collect_files_from_directory",
    "detect_encoding",
    "expand_path",
    "read_allowed_dirs_from_file",
]

logger = logging.getLogger(__name__)

# Type for values in template context
TemplateValue = Union[str, List[str], Dict[str, str]]


def _get_security_manager() -> Type[SecurityManagerProtocol]:
    """Get the SecurityManager class.

    Returns:
        The SecurityManager class type
    """
    return SecurityManager


def expand_path(path: str, force_absolute: bool = False) -> str:
    """Expand user home directory and environment variables in path.

    Args:
        path: Path that may contain ~ or environment variables
        force_absolute: Whether to force conversion to absolute path

    Returns:
        Expanded path, maintaining relative paths unless force_absolute=True
        or the path contains ~ or environment variables
    """
    # First expand user and environment variables
    expanded = os.path.expanduser(os.path.expandvars(path))

    # If the path hasn't changed and we're not forcing absolute, keep it relative
    if expanded == path and not force_absolute:
        return path

    # Otherwise return absolute path
    return os.path.abspath(expanded)


def collect_files_from_pattern(
    pattern: str,
    security_manager: SecurityManager,
) -> List[FileInfo]:
    """Collect files matching a glob pattern.

    Args:
        pattern: Glob pattern to match files
        security_manager: Security manager for path validation

    Returns:
        List of FileInfo objects for matched files

    Raises:
        PathSecurityError: If any matched file is outside base directory
    """
    # Expand pattern
    matched_paths = glob.glob(pattern, recursive=True)
    if not matched_paths:
        logger.debug("No files matched pattern: %s", pattern)
        return []

    # Create FileInfo objects
    files = []
    for path in matched_paths:
        try:
            file_info = FileInfo.from_path(path, security_manager)
            files.append(file_info)
        except PathSecurityError:
            # Let security errors propagate
            raise
        except Exception:
            logger.warning("Could not process file %s", path)

    return files


def collect_files_from_directory(
    directory: str,
    security_manager: SecurityManager,
    recursive: bool = False,
    allowed_extensions: Optional[List[str]] = None,
    **kwargs: Any,
) -> List[FileInfo]:
    """Collect files from directory.

    Args:
        directory: Directory to collect files from
        security_manager: Security manager for path validation
        recursive: Whether to collect files recursively
        allowed_extensions: List of allowed file extensions without dots
        **kwargs: Additional arguments passed to FileInfo.from_path

    Returns:
        List of FileInfo instances

    Raises:
        DirectoryNotFoundError: If directory does not exist
        PathSecurityError: If directory is not allowed
    """
    # Validate directory exists and is allowed
    try:
        abs_dir = str(security_manager.resolve_path(directory))
    except PathSecurityError:
        # Let the original error propagate
        raise

    if not os.path.exists(abs_dir):
        raise DirectoryNotFoundError(f"Directory not found: {directory}")
    if not os.path.isdir(abs_dir):
        raise DirectoryNotFoundError(f"Path is not a directory: {directory}")

    # Collect files
    files = []
    for root, _, filenames in os.walk(abs_dir):
        if not recursive and root != abs_dir:
            continue

        for filename in filenames:
            # Get relative path from base directory
            abs_path = os.path.join(root, filename)
            try:
                rel_path = os.path.relpath(abs_path, security_manager.base_dir)
            except ValueError:
                # Skip files that can't be made relative
                continue

            # Check extension if filter is specified
            if allowed_extensions is not None:
                ext = os.path.splitext(filename)[1].lstrip(".")
                if ext not in allowed_extensions:
                    continue

            try:
                file_info = FileInfo.from_path(
                    rel_path, security_manager=security_manager, **kwargs
                )
                files.append(file_info)
            except (FileNotFoundError, PathSecurityError):
                # Skip files that can't be accessed
                continue

    return files


def _validate_and_split_mapping(
    mapping: str, mapping_type: str
) -> tuple[str, str]:
    """Validate and split a name=value mapping.

    Args:
        mapping: The mapping string to validate (e.g. "name=value")
        mapping_type: Type of mapping for error messages ("file", "pattern", or "directory")

    Returns:
        Tuple of (name, value)

    Raises:
        ValueError: If mapping format is invalid
    """
    try:
        name, value = mapping.split("=", 1)
    except ValueError:
        raise ValueError(
            f"Invalid {mapping_type} mapping format: {mapping!r} (missing '=' separator)"
        )

    if not name:
        raise ValueError(f"Empty name in {mapping_type} mapping: {mapping!r}")
    if not value:
        raise ValueError(f"Empty value in {mapping_type} mapping: {mapping!r}")

    return name, value


def collect_files(
    file_mappings: Optional[List[str]] = None,
    pattern_mappings: Optional[List[str]] = None,
    dir_mappings: Optional[List[str]] = None,
    recursive: bool = False,
    extensions: Optional[List[str]] = None,
    security_manager: Optional[SecurityManager] = None,
    **kwargs: Any,
) -> Dict[str, FileInfoList]:
    """Collect files from various mappings.

    Args:
        file_mappings: List of name=path mappings for single files
        pattern_mappings: List of name=pattern mappings for multiple files
        dir_mappings: List of name=dir mappings for directories
        recursive: Whether to collect files recursively from directories
        extensions: Optional list of allowed file extensions (with or without dots)
        security_manager: Security manager for path validation
        **kwargs: Additional arguments passed to FileInfo.from_path

    Returns:
        Dict mapping names to FileInfoList objects

    Raises:
        ValueError: If no files are found or if mapping format is invalid
        FileNotFoundError: If a file does not exist
        PathSecurityError: If a path is not allowed
    """
    if not any([file_mappings, pattern_mappings, dir_mappings]):
        raise ValueError("No file mappings provided")

    if security_manager is None:
        security_manager = SecurityManager(base_dir=os.getcwd())

    # Normalize extensions by removing leading dots
    if extensions:
        extensions = [ext.lstrip(".") for ext in extensions]

    files: Dict[str, FileInfoList] = {}

    # Process file mappings
    if file_mappings:
        for mapping in file_mappings:
            name, path = _validate_and_split_mapping(mapping, "file")
            if name in files:
                raise ValueError(f"Duplicate file mapping: {name}")

            file_info = FileInfo.from_path(
                path, security_manager=security_manager, **kwargs
            )
            files[name] = FileInfoList([file_info], from_dir=False)

    # Process pattern mappings
    if pattern_mappings:
        for mapping in pattern_mappings:
            name, pattern = _validate_and_split_mapping(mapping, "pattern")
            if name in files:
                raise ValueError(f"Duplicate pattern mapping: {name}")

            try:
                matched_files = collect_files_from_pattern(
                    pattern, security_manager=security_manager, **kwargs
                )
            except PathSecurityError as e:
                raise PathSecurityError(
                    "Pattern mapping error: Access denied: "
                    f"{pattern} is outside base directory and not in allowed directories"
                ) from e

            if not matched_files:
                logger.warning("No files matched pattern: %s", pattern)
                continue

            files[name] = FileInfoList(matched_files, from_dir=False)

    # Process directory mappings
    if dir_mappings:
        for mapping in dir_mappings:
            name, directory = _validate_and_split_mapping(mapping, "directory")
            if name in files:
                raise ValueError(f"Duplicate directory mapping: {name}")

            try:
                dir_files = collect_files_from_directory(
                    directory=directory,
                    security_manager=security_manager,
                    recursive=recursive,
                    allowed_extensions=extensions,
                    **kwargs,
                )
            except PathSecurityError as e:
                raise PathSecurityError(
                    "Directory mapping error: Access denied: "
                    f"{directory} is outside base directory and not in allowed directories"
                ) from e
            except DirectoryNotFoundError:
                raise DirectoryNotFoundError(
                    f"Directory not found: {directory}"
                )

            if not dir_files:
                logger.warning("No files found in directory: %s", directory)
                files[name] = FileInfoList([], from_dir=True)
            else:
                files[name] = FileInfoList(dir_files, from_dir=True)

    if not files:
        raise ValueError("No files found")

    return files


def detect_encoding(file_path: str) -> str:
    """Detect the encoding of a file.

    Args:
        file_path: Path to the file to check

    Returns:
        str: The detected encoding (e.g. 'utf-8', 'utf-16', etc.)

    Raises:
        OSError: If there is an error reading the file
        ValueError: If the encoding cannot be detected
    """
    logger = logging.getLogger(__name__)
    logger.debug("Detecting encoding for file: %s", file_path)

    try:
        with open(file_path, "rb") as f:
            # Check for BOM markers first
            raw_data = f.read(4)
            if not raw_data:
                logger.debug("Empty file")
                return "utf-8"

            # Check for common BOMs
            if raw_data.startswith(codecs.BOM_UTF8):
                logger.debug("UTF-8 BOM detected")
                return "utf-8"
            elif raw_data.startswith(codecs.BOM_UTF16_LE):
                logger.debug("UTF-16 LE BOM detected")
                return "utf-16-le"
            elif raw_data.startswith(codecs.BOM_UTF16_BE):
                logger.debug("UTF-16 BE BOM detected")
                return "utf-16-be"
            elif raw_data.startswith(codecs.BOM_UTF32_LE):
                logger.debug("UTF-32 LE BOM detected")
                return "utf-32-le"
            elif raw_data.startswith(codecs.BOM_UTF32_BE):
                logger.debug("UTF-32 BE BOM detected")
                return "utf-32-be"

            # Read more data for chardet (up to 1MB)
            f.seek(0)
            raw_data = f.read(
                1024 * 1024
            )  # Read up to 1MB for better detection

            # Try chardet detection
            result = chardet.detect(raw_data)
            logger.debug("Chardet detection result: %s", result)

            if result and isinstance(result, dict) and result.get("encoding"):
                detected = str(result["encoding"]).lower()
                confidence = float(result.get("confidence", 0.0))

                # Handle ASCII detection
                if detected == "ascii":
                    logger.debug(
                        "ASCII detected, converting to UTF-8 (confidence: %f)",
                        confidence,
                    )
                    return "utf-8"

                # High confidence detection
                if confidence > 0.9:
                    logger.debug(
                        "High confidence encoding detected: %s (confidence: %f)",
                        detected,
                        confidence,
                    )
                    return detected

                # Medium confidence - validate with UTF-8 attempt
                if confidence > 0.6:
                    logger.debug(
                        "Medium confidence for %s (confidence: %f), validating",
                        detected,
                        confidence,
                    )
                    try:
                        raw_data.decode("utf-8")
                        logger.debug("Successfully validated as UTF-8")
                        return "utf-8"
                    except UnicodeDecodeError:
                        logger.debug(
                            "UTF-8 validation failed, using detected encoding: %s",
                            detected,
                        )
                        return detected

            # Low confidence or no detection - try UTF-8
            try:
                raw_data.decode("utf-8")
                logger.debug(
                    "No confident detection, but UTF-8 decode successful"
                )
                return "utf-8"
            except UnicodeDecodeError:
                if (
                    result
                    and isinstance(result, dict)
                    and result.get("encoding")
                ):
                    detected_encoding = str(result["encoding"]).lower()
                    logger.debug(
                        "Falling back to detected encoding with low confidence: %s",
                        detected_encoding,
                    )
                    return detected_encoding

                logger.warning(
                    "Could not confidently detect encoding for %s, defaulting to UTF-8",
                    file_path,
                )
                return "utf-8"

    except OSError as e:
        logger.error("Error reading file %s: %s", file_path, e)
        raise
    except Exception as e:
        logger.error(
            "Unexpected error detecting encoding for %s: %s",
            file_path,
            e,
        )
        raise ValueError(f"Failed to detect encoding: {e}")


def read_allowed_dirs_from_file(filepath: str) -> List[str]:
    """Reads a list of allowed directories from a file.

    Args:
        filepath: The path to the file.

    Returns:
        A list of allowed directories as absolute paths.

    Raises:
        FileNotFoundError: If the file does not exist.
        ValueError: If the file contains invalid data.
    """
    try:
        with open(filepath, "r") as f:
            lines = f.readlines()
    except OSError as e:
        raise FileNotFoundError(
            f"Error reading allowed directories from file: {filepath}: {e}"
        )

    allowed_dirs = []
    for line in lines:
        line = line.strip()
        if line and not line.startswith(
            "#"
        ):  # Ignore empty lines and comments
            abs_path = os.path.abspath(line)
            if not os.path.isdir(abs_path):
                raise ValueError(
                    f"Invalid directory in allowed directories file '{filepath}': "
                    f"'{line}' is not a directory or does not exist."
                )
            allowed_dirs.append(abs_path)
    return allowed_dirs




"""Custom error classes for CLI error handling."""

from pathlib import Path
from typing import List, Optional


class CLIError(Exception):
    """Base class for all CLI errors."""

    pass


class VariableError(CLIError):
    """Base class for variable-related errors."""

    pass


class VariableNameError(VariableError):
    """Raised when a variable name is invalid or empty."""

    pass


class VariableValueError(VariableError):
    """Raised when a variable value is invalid or missing."""

    pass


class InvalidJSONError(VariableError):
    """Raised when JSON parsing fails for a variable value."""

    pass


class PathError(CLIError):
    """Base class for path-related errors."""

    pass


class FileNotFoundError(PathError):
    """Raised when a specified file does not exist."""

    pass


class DirectoryNotFoundError(PathError):
    """Raised when a specified directory does not exist."""

    pass


class PathSecurityError(Exception):
    """Exception raised when file access is denied due to security constraints.

    Attributes:
        message: The error message with full context
        error_logged: Whether this error has already been logged
        wrapped: Whether this error has been wrapped by another error
    """

    def __init__(
        self, message: str, error_logged: bool = False, wrapped: bool = False
    ) -> None:
        """Initialize PathSecurityError.

        Args:
            message: Detailed error message with context
            error_logged: Whether this error has already been logged
            wrapped: Whether this error has been wrapped by another error
        """
        super().__init__(message)
        self.error_logged = error_logged
        self.message = message
        self.wrapped = wrapped

    @property
    def has_been_logged(self) -> bool:
        """Check if this error has been logged, more readable than accessing error_logged directly."""
        return self.error_logged

    def __str__(self) -> str:
        """Get string representation of the error."""
        return self.message

    @classmethod
    def access_denied(
        cls,
        path: Path,
        reason: Optional[str] = None,
        error_logged: bool = False,
    ) -> "PathSecurityError":
        """Create access denied error.

        Args:
            path: Path that was denied
            reason: Optional reason for denial
            error_logged: Whether this error has already been logged

        Returns:
            PathSecurityError with standardized message
        """
        msg = f"Access denied: {path}"
        if reason:
            msg += f" - {reason}"
        return cls(msg, error_logged=error_logged)

    @classmethod
    def outside_allowed(
        cls,
        path: Path,
        base_dir: Optional[Path] = None,
        error_logged: bool = False,
    ) -> "PathSecurityError":
        """Create error for path outside allowed directories.

        Args:
            path: Path that was outside
            base_dir: Optional base directory for context
            error_logged: Whether this error has already been logged

        Returns:
            PathSecurityError with standardized message
        """
        parts = [
            f"Access denied: {path} is outside base directory and not in allowed directories"
        ]
        if base_dir:
            parts.append(f"Base directory: {base_dir}")
        parts.append(
            "Use --allowed-dir to specify additional allowed directories"
        )
        return cls("\n".join(parts), error_logged=error_logged)

    @classmethod
    def traversal_attempt(
        cls, path: Path, error_logged: bool = False
    ) -> "PathSecurityError":
        """Create error for directory traversal attempt.

        Args:
            path: Path that attempted traversal
            error_logged: Whether this error has already been logged

        Returns:
            PathSecurityError with standardized message
        """
        return cls(
            f"Access denied: {path} - directory traversal not allowed",
            error_logged=error_logged,
        )

    @classmethod
    def from_expanded_paths(
        cls,
        original_path: str,
        expanded_path: str,
        base_dir: Optional[str] = None,
        allowed_dirs: Optional[List[str]] = None,
        error_logged: bool = False,
    ) -> "PathSecurityError":
        """Create error with expanded path context.

        Args:
            original_path: Original path as provided by user
            expanded_path: Expanded absolute path
            base_dir: Optional base directory
            allowed_dirs: Optional list of allowed directories
            error_logged: Whether this error has already been logged

        Returns:
            PathSecurityError with detailed path context
        """
        parts = [
            f"Access denied: {original_path} is outside base directory and not in allowed directories",
            f"File absolute path: {expanded_path}",
        ]
        if base_dir:
            parts.append(f"Base directory: {base_dir}")
        if allowed_dirs:
            parts.append(f"Allowed directories: {allowed_dirs}")
        parts.append(
            "Use --allowed-dir to specify additional allowed directories"
        )
        return cls("\n".join(parts), error_logged=error_logged)

    def format_with_context(
        self,
        original_path: Optional[str] = None,
        expanded_path: Optional[str] = None,
        base_dir: Optional[str] = None,
        allowed_dirs: Optional[List[str]] = None,
    ) -> str:
        """Format error message with additional context.

        Args:
            original_path: Optional original path as provided by user
            expanded_path: Optional expanded absolute path
            base_dir: Optional base directory
            allowed_dirs: Optional list of allowed directories

        Returns:
            Formatted error message with context
        """
        parts = [self.message]
        if original_path and expanded_path and original_path != expanded_path:
            parts.append(f"Original path: {original_path}")
            parts.append(f"Expanded path: {expanded_path}")
        if base_dir:
            parts.append(f"Base directory: {base_dir}")
        if allowed_dirs:
            parts.append(f"Allowed directories: {allowed_dirs}")
        if not any(
            p.endswith(
                "Use --allowed-dir to specify additional allowed directories"
            )
            for p in parts
        ):
            parts.append(
                "Use --allowed-dir to specify additional allowed directories"
            )
        return "\n".join(parts)

    @classmethod
    def wrap_error(
        cls, context: str, original: "PathSecurityError"
    ) -> "PathSecurityError":
        """Wrap an error with additional context while preserving attributes.

        Args:
            context: Additional context to add to the error message
            original: The original PathSecurityError to wrap

        Returns:
            PathSecurityError with additional context and preserved attributes
        """
        base_message = str(original)
        message = f"{context}: {base_message}"
        return cls(message, error_logged=original.error_logged, wrapped=True)


class TaskTemplateError(CLIError):
    """Base class for task template-related errors."""

    pass


class TaskTemplateSyntaxError(TaskTemplateError):
    """Raised when a task template has invalid syntax."""

    pass


class TaskTemplateVariableError(TaskTemplateError):
    """Raised when a task template uses undefined variables."""

    pass


class SystemPromptError(TaskTemplateError):
    """Raised when there are issues with system prompt loading or processing."""

    pass


class SchemaError(CLIError):
    """Base class for schema-related errors."""

    pass


class SchemaFileError(SchemaError):
    """Raised when a schema file is invalid or inaccessible."""

    pass


class SchemaValidationError(SchemaError):
    """Raised when a schema fails validation."""

    pass




"""Progress reporting utilities for CLI operations."""

import sys
from contextlib import contextmanager
from types import TracebackType
from typing import Iterator, Optional, Type

from rich.console import Console
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TaskID,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)

try:
    # Just check if rich is available
    import rich  # noqa: F401

    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


class ProgressContext:
    """Context manager for showing progress indicators.

    Attributes:
        description: Description of the operation
        total: Total number of items (optional)
        enabled: Whether progress reporting is enabled
        current: Current progress count
    """

    def __init__(
        self,
        description: str = "Processing",
        total: Optional[int] = None,
        show_progress: bool = True,
        output_file: Optional[str] = None,
    ):
        self.description = description
        self.total = total
        self.enabled = show_progress and sys.stdout.isatty() and RICH_AVAILABLE
        self.current = 0
        self._progress: Optional[Progress] = None
        self._task_id: Optional[TaskID] = None
        self._output_file = output_file

    def __enter__(self) -> "ProgressContext":
        """Start progress reporting."""
        if self.enabled:
            self._progress = Progress(
                SpinnerColumn(),
                TextColumn("[bold blue]{task.description}"),
                BarColumn(complete_style="green", finished_style="green"),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn(""),
                TimeElapsedColumn(),
                TextColumn(""),
                TimeRemainingColumn(),
                console=Console(force_terminal=True),
                transient=True,
                expand=True,
            )
            self._progress.start()
            self._task_id = self._progress.add_task(
                self.description,
                total=self.total if self.total is not None else None,
            )
        return self

    def __exit__(
        self,
        exc_type: Optional[Type[BaseException]],
        exc_val: Optional[BaseException],
        exc_tb: Optional[TracebackType],
    ) -> None:
        """Stop progress reporting."""
        if self.enabled and self._progress:
            if exc_type is None:
                # Complete the progress bar
                if self._task_id is not None:
                    self._progress.update(
                        self._task_id, completed=self.total or self.current
                    )
            self._progress.stop()

    def update(self, amount: int = 1) -> None:
        """Update progress by the specified amount."""
        if self.enabled and self._progress and self._task_id is not None:
            self.current += amount
            self._progress.update(self._task_id, advance=amount)

    @contextmanager
    def step(self, description: Optional[str] = None) -> Iterator[None]:
        """Context manager for a single step in the progress.

        Args:
            description: Optional description for this step
        """
        if (
            description
            and self.enabled
            and self._progress
            and self._task_id is not None
        ):
            old_description = self._progress.tasks[self._task_id].description
            self._progress.update(
                self._task_id, description=f"{old_description}  {description}"
            )
        try:
            yield
        finally:
            if self.enabled and self._progress and self._task_id is not None:
                self.update()
                if description:
                    self._progress.update(
                        self._task_id, description=self.description
                    )




"""Jinja2 environment factory and configuration.

This module provides a centralized factory for creating consistently configured Jinja2 environments.
"""

import jinja2
from jinja2 import Environment
from typing import Optional, Type

from .template_extensions import CommentExtension
from .template_filters import register_template_filters


def create_jinja_env(
    *,
    undefined: Optional[Type[jinja2.Undefined]] = None,
    loader: Optional[jinja2.BaseLoader] = None,
    validation_mode: bool = False,
) -> Environment:
    """Create a consistently configured Jinja2 environment.
    
    Args:
        undefined: Custom undefined class to use. Defaults to StrictUndefined.
        loader: Template loader to use. Defaults to None.
        validation_mode: Whether to configure the environment for validation (uses SafeUndefined).
        
    Returns:
        A configured Jinja2 environment.
    """
    if validation_mode:
        from .template_validation import SafeUndefined
        undefined = SafeUndefined
    elif undefined is None:
        undefined = jinja2.StrictUndefined

    env = Environment(
        loader=loader,
        undefined=undefined,
        autoescape=False,  # Disable HTML escaping by default
        trim_blocks=True,
        lstrip_blocks=True,
        keep_trailing_newline=True,
        line_statement_prefix="#",
        line_comment_prefix="##",
        extensions=[
            "jinja2.ext.do",
            "jinja2.ext.loopcontrols",
            CommentExtension,
        ],
    )

    # Register all template filters
    register_template_filters(env)

    return env 



"""Path validation utilities for the CLI."""

import os
from pathlib import Path
from typing import Optional, Tuple

from .errors import (
    DirectoryNotFoundError,
    FileNotFoundError,
    PathSecurityError,
    VariableNameError,
    VariableValueError,
)
from .security import SecurityManager


def validate_path_mapping(
    mapping: str,
    is_dir: bool = False,
    base_dir: Optional[str] = None,
    security_manager: Optional[SecurityManager] = None,
) -> Tuple[str, str]:
    """Validate a path mapping in the format "name=path".

    Args:
        mapping: The path mapping string (e.g., "myvar=/path/to/file").
        is_dir: Whether the path is expected to be a directory (True) or file (False).
        base_dir: Optional base directory to resolve relative paths against.
        security_manager: Optional security manager to validate paths.

    Returns:
        A (name, path) tuple.

    Raises:
        VariableNameError: If the variable name portion is empty or invalid.
        DirectoryNotFoundError: If is_dir=True and the path is not a directory or doesn't exist.
        FileNotFoundError: If is_dir=False and the path is not a file or doesn't exist.
        PathSecurityError: If the path is inaccessible or outside the allowed directory.
        ValueError: If the format is invalid (missing "=").
        OSError: If there is an underlying OS error (permissions, etc.).

    Example:
        >>> validate_path_mapping("config=settings.txt")  # Validates file
        ('config', 'settings.txt')
        >>> validate_path_mapping("data=config/", is_dir=True)  # Validates directory
        ('data', 'config/')
    """
    try:
        if not mapping or "=" not in mapping:
            raise ValueError("Invalid mapping format")

        name, path = mapping.split("=", 1)
        if not name:
            raise VariableNameError(
                f"Empty name in {'directory' if is_dir else 'file'} mapping"
            )

        if not path:
            raise VariableValueError("Path cannot be empty")

        # Expand user home directory and environment variables
        path = os.path.expanduser(os.path.expandvars(path))

        # Convert to Path object and resolve against base_dir if provided
        path_obj = Path(path)
        if base_dir:
            path_obj = Path(base_dir) / path_obj

        # Resolve the path to catch directory traversal attempts
        try:
            resolved_path = path_obj.resolve()
        except OSError as e:
            raise OSError(f"Failed to resolve path: {e}")

        # Check if path exists
        if not resolved_path.exists():
            if is_dir:
                raise DirectoryNotFoundError(f"Directory not found: {path!r}")
            else:
                raise FileNotFoundError(f"File not found: {path!r}")

        # Check if path is correct type
        if is_dir and not resolved_path.is_dir():
            raise DirectoryNotFoundError(f"Path is not a directory: {path!r}")
        elif not is_dir and not resolved_path.is_file():
            raise FileNotFoundError(f"Path is not a file: {path!r}")

        # Check if path is accessible
        try:
            if is_dir:
                os.listdir(str(resolved_path))
            else:
                with open(str(resolved_path), "r", encoding="utf-8") as f:
                    f.read(1)
        except OSError as e:
            if e.errno == 13:  # Permission denied
                raise PathSecurityError(
                    f"Permission denied accessing path: {path!r}"
                )
            raise

        # Check security constraints
        if security_manager:
            if not security_manager.is_path_allowed(str(resolved_path)):
                raise PathSecurityError.from_expanded_paths(
                    original_path=str(path),
                    expanded_path=str(resolved_path),
                    base_dir=str(security_manager.base_dir),
                    allowed_dirs=[
                        str(d) for d in security_manager.allowed_dirs
                    ],
                )

        # Return the original path to maintain relative paths in the output
        return name, path

    except ValueError as e:
        if "not enough values to unpack" in str(e):
            raise VariableValueError(
                f"Invalid {'directory' if is_dir else 'file'} mapping "
                f"(expected name=path format): {mapping!r}"
            )
        raise




"""FileInfo class for representing file metadata and content."""

import hashlib
import logging
import os
from typing import Any, Optional

from .errors import FileNotFoundError, PathSecurityError
from .security import SecurityManager

logger = logging.getLogger(__name__)


class FileInfo:
    """Represents a file with metadata and content.

    This class provides access to file metadata (path, size, etc.) and content,
    with caching support for efficient access.

    Args:
        path: Path to the file
        security_manager: Security manager to use for path validation
        content: Optional cached content
        encoding: Optional cached encoding
        hash_value: Optional cached hash value
    """

    def __init__(
        self,
        path: str,
        security_manager: SecurityManager,
        content: Optional[str] = None,
        encoding: Optional[str] = None,
        hash_value: Optional[str] = None,
    ) -> None:
        """Initialize FileInfo instance.

        Args:
            path: Path to the file
            security_manager: Security manager to use for path validation
            content: Optional cached content
            encoding: Optional cached encoding
            hash_value: Optional cached hash value

        Raises:
            FileNotFoundError: If the file does not exist
            PathSecurityError: If the path is not allowed
        """
        # Validate path
        if not path:
            raise ValueError("Path cannot be empty")

        # Initialize private attributes
        self.__path = os.path.expanduser(os.path.expandvars(path))
        self.__security_manager = security_manager
        self.__content = content
        self.__encoding = encoding
        self.__hash = hash_value
        self.__size: Optional[int] = None
        self.__mtime: Optional[float] = None

        # First check if file exists
        abs_path = os.path.abspath(self.__path)
        if not os.path.exists(abs_path):
            raise FileNotFoundError(f"File not found: {path}")
        if not os.path.isfile(abs_path):
            raise FileNotFoundError(f"Path is not a file: {path}")

        # Then validate security
        try:
            # This will raise PathSecurityError if path is not allowed
            self.abs_path
        except PathSecurityError:
            raise
        except Exception as e:
            raise FileNotFoundError(f"Invalid file path: {e}")

        # If content/encoding weren't provided, read them now
        if self.__content is None or self.__encoding is None:
            self._read_file()

    @property
    def path(self) -> str:
        """Get the relative path of the file."""
        # If original path was relative, keep it relative
        if not os.path.isabs(self.__path):
            try:
                base_dir = self.__security_manager.base_dir
                abs_path = self.abs_path
                return os.path.relpath(abs_path, base_dir)
            except ValueError:
                pass
        return self.__path

    @path.setter
    def path(self, value: str) -> None:
        """Prevent setting path directly."""
        raise AttributeError("Cannot modify path directly")

    @property
    def abs_path(self) -> str:
        """Get the absolute path of the file.

        Returns:
            str: The absolute path of the file.

        Raises:
            PathSecurityError: If the path is not allowed.
        """
        # Get the resolved absolute path through security manager
        resolved = self.__security_manager.resolve_path(self.__path)

        # Always return absolute path
        return str(resolved)

    @property
    def extension(self) -> str:
        """Get file extension without dot."""
        return os.path.splitext(self.__path)[1].lstrip(".")

    @property
    def name(self) -> str:
        """Get the filename without directory path."""
        return os.path.basename(self.__path)

    @name.setter
    def name(self, value: str) -> None:
        """Prevent setting name directly."""
        raise AttributeError("Cannot modify name directly")

    @property
    def size(self) -> Optional[int]:
        """Get file size in bytes."""
        if self.__size is None:
            try:
                self.__size = os.path.getsize(self.abs_path)
            except OSError:
                logger.warning("Could not get size for %s", self.__path)
                return None
        return self.__size

    @size.setter
    def size(self, value: int) -> None:
        """Prevent setting size directly."""
        raise AttributeError("Cannot modify size directly")

    @property
    def mtime(self) -> Optional[float]:
        """Get file modification time as Unix timestamp."""
        if self.__mtime is None:
            try:
                self.__mtime = os.path.getmtime(self.abs_path)
            except OSError:
                logger.warning("Could not get mtime for %s", self.__path)
                return None
        return self.__mtime

    @mtime.setter
    def mtime(self, value: float) -> None:
        """Prevent setting mtime directly."""
        raise AttributeError("Cannot modify mtime directly")

    @property
    def content(self) -> str:
        """Get the content of the file."""
        assert (
            self.__content is not None
        ), "Content should be initialized in constructor"
        return self.__content

    @content.setter
    def content(self, value: str) -> None:
        """Prevent setting content directly."""
        raise AttributeError("Cannot modify content directly")

    @property
    def encoding(self) -> str:
        """Get the encoding of the file."""
        assert (
            self.__encoding is not None
        ), "Encoding should be initialized in constructor"
        return self.__encoding

    @encoding.setter
    def encoding(self, value: str) -> None:
        """Prevent setting encoding directly."""
        raise AttributeError("Cannot modify encoding directly")

    @property
    def hash(self) -> Optional[str]:
        """Get SHA-256 hash of file content."""
        if self.__hash is None and self.__content is not None:
            self.__hash = hashlib.sha256(
                self.__content.encode("utf-8")
            ).hexdigest()
        return self.__hash

    @hash.setter
    def hash(self, value: str) -> None:
        """Prevent setting hash directly."""
        raise AttributeError("Cannot modify hash directly")

    def _read_file(self) -> None:
        """Read file content and encoding from disk."""
        try:
            with open(self.abs_path, "rb") as f:
                raw_content = f.read()
        except FileNotFoundError as e:
            raise FileNotFoundError(f"File not found: {self.__path}") from e
        except OSError as e:
            raise FileNotFoundError(
                f"Could not read file {self.__path}: {e}"
            ) from e

        # Try UTF-8 first
        try:
            self.__content = raw_content.decode("utf-8")
            self.__encoding = "utf-8"
            return
        except UnicodeDecodeError:
            pass

        # Fall back to system default encoding
        try:
            self.__content = raw_content.decode()
            self.__encoding = "system"
            return
        except UnicodeDecodeError as e:
            raise ValueError(
                f"Could not decode file {self.__path}: {e}"
            ) from e

    def update_cache(
        self,
        content: Optional[str] = None,
        encoding: Optional[str] = None,
        hash_value: Optional[str] = None,
    ) -> None:
        """Update cached values.

        Args:
            content: New content to cache
            encoding: New encoding to cache
            hash_value: New hash value to cache
        """
        if content is not None:
            self.__content = content
        if encoding is not None:
            self.__encoding = encoding
        if hash_value is not None:
            self.__hash = hash_value

    @classmethod
    def from_path(
        cls, path: str, security_manager: SecurityManager
    ) -> "FileInfo":
        """Create FileInfo instance from path.

        Args:
            path: Path to file
            security_manager: Security manager for path validation

        Returns:
            FileInfo instance

        Raises:
            FileNotFoundError: If file does not exist
            PathSecurityError: If path is not allowed
        """
        return cls(path, security_manager)

    def __str__(self) -> str:
        """String representation showing path."""
        return f"FileInfo({self.__path})"

    def __repr__(self) -> str:
        """Detailed representation."""
        return (
            f"FileInfo(path={self.__path!r}, "
            f"size={self.size!r}, "
            f"encoding={self.encoding!r}, "
            f"hash={self.hash!r})"
        )

    def __setattr__(self, name: str, value: Any) -> None:
        """Control attribute modification.

        Internal methods can modify private attributes, but external access is prevented.
        """
        # Allow setting private attributes from internal methods
        if name.startswith("_FileInfo__") and self._is_internal_call():
            object.__setattr__(self, name, value)
            return

        # Prevent setting other attributes
        raise AttributeError(f"Cannot modify {name} directly")

    def _is_internal_call(self) -> bool:
        """Check if the call is from an internal method."""
        import inspect

        frame = inspect.currentframe()
        try:
            # Get the caller's frame (2 frames up: _is_internal_call -> __setattr__ -> caller)
            caller = frame.f_back.f_back if frame and frame.f_back else None
            if not caller:
                return False

            # Check if the caller is a method of this class
            return (
                caller.f_code.co_name in type(self).__dict__
                and "self" in caller.f_locals
                and caller.f_locals["self"] is self
            )
        finally:
            del frame  # Avoid reference cycles





INFO:ostruct:_main: Estimated Tokens: 52405
INFO:ostruct:_main: Model: gpt-4o-2024-08-06
INFO:ostruct:_main: Temperature: 0.0
INFO:ostruct:_main: Top P: 1.0
INFO:ostruct:_main: Frequency Penalty: 0.0
INFO:ostruct:_main: Presence Penalty: 0.0
